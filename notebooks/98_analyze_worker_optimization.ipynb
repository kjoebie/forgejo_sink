{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Worker Optimization Analysis\n",
    "\n",
    "Analyze historical runs to understand optimal worker counts per source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "SOURCE_NAME = \"anva_meeus\"  # Change to your source name\n",
    "LOOKBACK_RUNS = 10  # Number of recent runs to analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module fabric.bootstrap\n",
    "# ---------------------\n",
    "# This cell enables a flexible module loading strategy:\n",
    "#\n",
    "# PRODUCTION (default): The `Files/code` directory is empty. This function does nothing,\n",
    "# and Python imports all modules from the stable, versioned Wheel in the Environment.\n",
    "#\n",
    "# DEVELOPMENT / HOTFIX: To bypass the 15-20 minute Fabric publish cycle for urgent fixes,\n",
    "# upload individual .py files to `Files/code` in the Lakehouse. This function prepends\n",
    "# that path to sys.path, so Python finds the override files first. All other modules\n",
    "# continue to load from the Wheel - only the uploaded files are replaced.\n",
    "#\n",
    "# Usage: Keep `Files/code` empty for production stability. Use it only for rapid\n",
    "# iteration during development or emergency hotfixes.\n",
    "\n",
    "from modules.fabric_bootstrap import ensure_module_path\n",
    "ensure_module_path()  # Now Python can find the rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.spark_session import get_or_create_spark_session\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "spark = get_or_create_spark_session(app_name=\"Worker_Optimization_Analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Historical Runs Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get recent runs for this source\n",
    "df_history = (\n",
    "    spark.table(\"logs.bronze_run_summary\")\n",
    "    .filter(F.col(\"source\") == SOURCE_NAME)\n",
    "    .orderBy(F.col(\"run_start\").desc())\n",
    "    .limit(LOOKBACK_RUNS)\n",
    ")\n",
    "\n",
    "# Calculate throughput\n",
    "df_analysis = df_history.select(\n",
    "    F.col(\"run_id\"),\n",
    "    F.col(\"run_start\").cast(\"timestamp\").alias(\"run_time\"),\n",
    "    F.col(\"workers\"),\n",
    "    F.col(\"total_rows\"),\n",
    "    F.col(\"duration_seconds\"),\n",
    "    F.col(\"efficiency_pct\"),\n",
    "    (F.col(\"total_rows\") / F.col(\"duration_seconds\")).alias(\"throughput_rows_per_sec\"),\n",
    "    F.round((F.col(\"total_rows\") / 1000000.0), 2).alias(\"total_rows_millions\")\n",
    ").orderBy(F.col(\"run_time\").desc())\n",
    "\n",
    "print(f\"\\nüìä Last {LOOKBACK_RUNS} runs for source: {SOURCE_NAME}\\n\")\n",
    "df_analysis.show(LOOKBACK_RUNS, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Average Performance per Worker Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate metrics per worker count\n",
    "df_by_workers = (\n",
    "    df_analysis\n",
    "    .groupBy(\"workers\")\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"num_runs\"),\n",
    "        F.round(F.avg(\"throughput_rows_per_sec\"), 0).alias(\"avg_throughput\"),\n",
    "        F.round(F.avg(\"efficiency_pct\"), 1).alias(\"avg_efficiency_pct\"),\n",
    "        F.round(F.avg(\"duration_seconds\"), 0).alias(\"avg_duration_sec\"),\n",
    "        F.round(F.avg(\"total_rows_millions\"), 2).alias(\"avg_rows_millions\")\n",
    "    )\n",
    "    .orderBy(\"workers\")\n",
    ")\n",
    "\n",
    "print(\"\\n‚ö° Performance comparison by worker count:\\n\")\n",
    "df_by_workers.show(truncate=False)\n",
    "\n",
    "# Find best configuration\n",
    "best_throughput = df_by_workers.orderBy(F.col(\"avg_throughput\").desc()).first()\n",
    "best_efficiency = df_by_workers.orderBy(F.col(\"avg_efficiency_pct\").desc()).first()\n",
    "\n",
    "print(f\"\\nüèÜ Best throughput: {best_throughput['workers']} workers ‚Üí {best_throughput['avg_throughput']:.0f} rows/sec\")\n",
    "print(f\"üèÜ Best efficiency: {best_efficiency['workers']} workers ‚Üí {best_efficiency['avg_efficiency_pct']:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Throughput vs Efficiency Trade-off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate speedup relative to 1 worker\n",
    "baseline = df_by_workers.filter(F.col(\"workers\") == 1).first()\n",
    "\n",
    "if baseline:\n",
    "    baseline_throughput = baseline[\"avg_throughput\"]\n",
    "    \n",
    "    df_speedup = (\n",
    "        df_by_workers\n",
    "        .withColumn(\n",
    "            \"speedup_vs_1worker\",\n",
    "            F.round(F.col(\"avg_throughput\") / baseline_throughput, 2)\n",
    "        )\n",
    "        .select(\n",
    "            \"workers\",\n",
    "            \"num_runs\",\n",
    "            \"avg_throughput\",\n",
    "            \"speedup_vs_1worker\",\n",
    "            \"avg_efficiency_pct\",\n",
    "            \"avg_duration_sec\"\n",
    "        )\n",
    "        .orderBy(\"workers\")\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nüìà Speedup analysis (baseline: 1 worker = {baseline_throughput:.0f} rows/sec):\\n\")\n",
    "    df_speedup.show(truncate=False)\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  No baseline (1 worker) data available for speedup calculation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Worker Optimizer Recommendation\n",
    "\n",
    "Test what the optimizer would recommend based on current history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.worker_utils import choose_worker_profile_from_history\n",
    "\n",
    "print(\"\\nü§ñ Worker Optimizer Recommendations:\\n\")\n",
    "\n",
    "# Throughput optimization (Fabric/serverless)\n",
    "rec_throughput = choose_worker_profile_from_history(\n",
    "    spark=spark,\n",
    "    source_name=SOURCE_NAME,\n",
    "    summary_table=\"logs.bronze_run_summary\",\n",
    "    default_workers=8,\n",
    "    min_workers=2,\n",
    "    max_workers_cap=12,\n",
    "    lookback_runs=3,\n",
    "    optimize_for=\"throughput\",\n",
    "    debug=True\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úì Throughput mode (Fabric): {rec_throughput} workers\\n\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Efficiency optimization (on-prem cluster)\n",
    "rec_efficiency = choose_worker_profile_from_history(\n",
    "    spark=spark,\n",
    "    source_name=SOURCE_NAME,\n",
    "    optimize_for=\"efficiency\",\n",
    "    debug=True\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úì Efficiency mode (Cluster): {rec_efficiency} workers\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Volume-based Analysis\n",
    "\n",
    "Understand how data volume affects optimal worker count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify runs by volume\n",
    "df_volume_analysis = (\n",
    "    df_analysis\n",
    "    .withColumn(\n",
    "        \"volume_category\",\n",
    "        F.when(F.col(\"total_rows\") < 100000, \"Tiny (<100k)\")\n",
    "        .when(F.col(\"total_rows\") < 1000000, \"Small (<1M)\")\n",
    "        .when(F.col(\"total_rows\") < 10000000, \"Medium (<10M)\")\n",
    "        .otherwise(\"Large (‚â•10M)\")\n",
    "    )\n",
    "    .groupBy(\"volume_category\", \"workers\")\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"runs\"),\n",
    "        F.round(F.avg(\"throughput_rows_per_sec\"), 0).alias(\"avg_throughput\"),\n",
    "        F.round(F.avg(\"efficiency_pct\"), 1).alias(\"avg_efficiency\")\n",
    "    )\n",
    "    .orderBy(\"volume_category\", \"workers\")\n",
    ")\n",
    "\n",
    "print(\"\\nüì¶ Performance by data volume and worker count:\\n\")\n",
    "df_volume_analysis.show(50, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Detailed Run Timeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timeline view\n",
    "df_timeline = (\n",
    "    df_analysis\n",
    "    .select(\n",
    "        F.date_format(\"run_time\", \"yyyy-MM-dd HH:mm\").alias(\"run_date\"),\n",
    "        \"workers\",\n",
    "        \"total_rows_millions\",\n",
    "        \"duration_seconds\",\n",
    "        F.round(\"throughput_rows_per_sec\", 0).alias(\"throughput\"),\n",
    "        F.round(\"efficiency_pct\", 1).alias(\"efficiency\")\n",
    "    )\n",
    "    .orderBy(F.col(\"run_date\").desc())\n",
    ")\n",
    "\n",
    "print(f\"\\nüìÖ Timeline of last {LOOKBACK_RUNS} runs:\\n\")\n",
    "df_timeline.show(LOOKBACK_RUNS, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Recommendations Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üìã OPTIMIZATION SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Get latest run info\n",
    "latest = df_analysis.first()\n",
    "\n",
    "print(f\"\\nSource: {SOURCE_NAME}\")\n",
    "print(f\"Data analyzed: Last {df_analysis.count()} runs\")\n",
    "print(f\"\\nLatest run:\")\n",
    "print(f\"  - Workers: {latest['workers']}\")\n",
    "print(f\"  - Rows: {latest['total_rows_millions']:.2f}M\")\n",
    "print(f\"  - Duration: {latest['duration_seconds']:.0f}s\")\n",
    "print(f\"  - Throughput: {latest['throughput_rows_per_sec']:.0f} rows/s\")\n",
    "print(f\"  - Efficiency: {latest['efficiency_pct']:.1f}%\")\n",
    "\n",
    "print(f\"\\nüéØ Optimizer Recommendations:\")\n",
    "print(f\"  - Fabric (throughput):  {rec_throughput} workers\")\n",
    "print(f\"  - Cluster (efficiency): {rec_efficiency} workers\")\n",
    "\n",
    "print(f\"\\nüìä Historical Best:\")\n",
    "print(f\"  - Best throughput: {best_throughput['workers']} workers ({best_throughput['avg_throughput']:.0f} rows/s)\")\n",
    "print(f\"  - Best efficiency: {best_efficiency['workers']} workers ({best_efficiency['avg_efficiency_pct']:.1f}%)\")\n",
    "\n",
    "# Calculate expected improvement\n",
    "if latest['workers'] != rec_throughput:\n",
    "    current_throughput = latest['throughput_rows_per_sec']\n",
    "    target_row = df_by_workers.filter(F.col(\"workers\") == rec_throughput).first()\n",
    "    \n",
    "    if target_row:\n",
    "        expected_throughput = target_row['avg_throughput']\n",
    "        improvement = ((expected_throughput - current_throughput) / current_throughput) * 100\n",
    "        \n",
    "        print(f\"\\nüí° Potential Improvement:\")\n",
    "        print(f\"   Switching from {latest['workers']} to {rec_throughput} workers:\")\n",
    "        print(f\"   Expected throughput gain: {improvement:+.1f}%\")\n",
    "        print(f\"   ({current_throughput:.0f} ‚Üí {expected_throughput:.0f} rows/s)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dwh-spark-processing (3.11.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
