{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77771788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Orchestrator Notebook - Test mssparkutils mock\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Voeg project root toe aan path\n",
    "project_root = Path.cwd().parent if Path.cwd().name == 'notebooks' else Path.cwd()\n",
    "sys.path.append(str(project_root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e293c8b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ mssparkutils mock imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import mssparkutils mock\n",
    "from modules.notebook_utils import mssparkutils\n",
    "import json\n",
    "\n",
    "print(\"‚úÖ mssparkutils mock imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "edcf217b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TESTING MSSPARKUTILS.NOTEBOOK.RUN()\n",
      "======================================================================\n",
      "üìì Executing notebook: notebooks\\01_process_data.ipynb\n",
      "‚öôÔ∏è  Arguments: {'source': 'sales_data', 'run_ts': '2025-11-25T01:16:39.864914', 'process_type': 'bronze'}\n",
      "üíæ Output: notebook_outputs\\01_process_data_20251125_011639.ipynb\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f39ea5c71634c13a95b1febca04365f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Executing:   0%|          | 0/7 [00:00<?, ?cell/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------\n",
      "‚ùå Notebook executie mislukt!\n",
      "Error: \n",
      "---------------------------------------------------------------------------\n",
      "Exception encountered at \"In [4]\":\n",
      "---------------------------------------------------------------------------\n",
      "PySparkRuntimeError                       Traceback (most recent call last)\n",
      "Cell In[4], line 4\n",
      "      1 # Initialiseer Spark session\n",
      "      2 from config.spark_config import create_spark_session\n",
      "----> 4 spark = create_spark_session(\n",
      "      5     app_name=f\"Process_{source}_{process_type}\",\n",
      "      6     master=\"local[2]\"  # Voor lokaal testen\n",
      "      7 )\n",
      "      9 print(f\"‚úÖ Spark session created: {spark.sparkContext.appName}\")\n",
      "     10 print(f\"üìä Spark version: {spark.version}\")\n",
      "\n",
      "File c:\\Users\\albert\\source\\repos\\QBIDS\\dwh_spark_processing\\config\\spark_config.py:45, in create_spark_session(app_name, master, executor_memory, executor_cores, max_cores)\n",
      "     33     master = os.getenv(\"SPARK_MASTER_URL\", \"local[*]\")\n",
      "     35 builder = SparkSession.builder \\\n",
      "     36     .appName(app_name) \\\n",
      "     37     .master(master) \\\n",
      "   (...)     42     .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
      "     43     .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
      "---> 45 return builder.getOrCreate()\n",
      "\n",
      "File c:\\Users\\albert\\source\\repos\\QBIDS\\dwh_spark_processing\\.venv\\Lib\\site-packages\\pyspark\\sql\\session.py:497, in SparkSession.Builder.getOrCreate(self)\n",
      "    495     sparkConf.set(key, value)\n",
      "    496 # This SparkContext may be an existing one.\n",
      "--> 497 sc = SparkContext.getOrCreate(sparkConf)\n",
      "    498 # Do not update `SparkConf` for existing `SparkContext`, as it's shared\n",
      "    499 # by all sessions.\n",
      "    500 session = SparkSession(sc, options=self._options)\n",
      "\n",
      "File c:\\Users\\albert\\source\\repos\\QBIDS\\dwh_spark_processing\\.venv\\Lib\\site-packages\\pyspark\\context.py:515, in SparkContext.getOrCreate(cls, conf)\n",
      "    513 with SparkContext._lock:\n",
      "    514     if SparkContext._active_spark_context is None:\n",
      "--> 515         SparkContext(conf=conf or SparkConf())\n",
      "    516     assert SparkContext._active_spark_context is not None\n",
      "    517     return SparkContext._active_spark_context\n",
      "\n",
      "File c:\\Users\\albert\\source\\repos\\QBIDS\\dwh_spark_processing\\.venv\\Lib\\site-packages\\pyspark\\context.py:201, in SparkContext.__init__(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\n",
      "    195 if gateway is not None and gateway.gateway_parameters.auth_token is None:\n",
      "    196     raise ValueError(\n",
      "    197         \"You are trying to pass an insecure Py4j gateway to Spark. This\"\n",
      "    198         \" is not allowed as it is a security risk.\"\n",
      "    199     )\n",
      "--> 201 SparkContext._ensure_initialized(self, gateway=gateway, conf=conf)\n",
      "    202 try:\n",
      "    203     self._do_init(\n",
      "    204         master,\n",
      "    205         appName,\n",
      "   (...)    215         memory_profiler_cls,\n",
      "    216     )\n",
      "\n",
      "File c:\\Users\\albert\\source\\repos\\QBIDS\\dwh_spark_processing\\.venv\\Lib\\site-packages\\pyspark\\context.py:436, in SparkContext._ensure_initialized(cls, instance, gateway, conf)\n",
      "    434 with SparkContext._lock:\n",
      "    435     if not SparkContext._gateway:\n",
      "--> 436         SparkContext._gateway = gateway or launch_gateway(conf)\n",
      "    437         SparkContext._jvm = SparkContext._gateway.jvm\n",
      "    439     if instance:\n",
      "\n",
      "File c:\\Users\\albert\\source\\repos\\QBIDS\\dwh_spark_processing\\.venv\\Lib\\site-packages\\pyspark\\java_gateway.py:107, in launch_gateway(conf, popen_kwargs)\n",
      "    104     time.sleep(0.1)\n",
      "    106 if not os.path.isfile(conn_info_file):\n",
      "--> 107     raise PySparkRuntimeError(\n",
      "    108         error_class=\"JAVA_GATEWAY_EXITED\",\n",
      "    109         message_parameters={},\n",
      "    110     )\n",
      "    112 with open(conn_info_file, \"rb\") as info:\n",
      "    113     gateway_port = read_int(info)\n",
      "\n",
      "PySparkRuntimeError: [JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number.\n",
      "\n",
      "\n",
      "======================================================================\n",
      "RESULT\n",
      "======================================================================\n",
      "{\n",
      "  \"status\": \"failed\",\n",
      "  \"error\": \"\\n---------------------------------------------------------------------------\\nException encountered at \\\"In [4]\\\":\\n---------------------------------------------------------------------------\\nPySparkRuntimeError                       Traceback (most recent call last)\\nCell In[4], line 4\\n      1 # Initialiseer Spark session\\n      2 from config.spark_config import create_spark_session\\n----> 4 spark = create_spark_session(\\n      5     app_name=f\\\"Process_{source}_{process_type}\\\",\\n      6     master=\\\"local[2]\\\"  # Voor lokaal testen\\n      7 )\\n      9 print(f\\\"\\u2705 Spark session created: {spark.sparkContext.appName}\\\")\\n     10 print(f\\\"\\ud83d\\udcca Spark version: {spark.version}\\\")\\n\\nFile c:\\\\Users\\\\albert\\\\source\\\\repos\\\\QBIDS\\\\dwh_spark_processing\\\\config\\\\spark_config.py:45, in create_spark_session(app_name, master, executor_memory, executor_cores, max_cores)\\n     33     master = os.getenv(\\\"SPARK_MASTER_URL\\\", \\\"local[*]\\\")\\n     35 builder = SparkSession.builder \\\\\\n     36     .appName(app_name) \\\\\\n     37     .master(master) \\\\\\n   (...)     42     .config(\\\"spark.sql.adaptive.coalescePartitions.enabled\\\", \\\"true\\\") \\\\\\n     43     .config(\\\"spark.serializer\\\", \\\"org.apache.spark.serializer.KryoSerializer\\\")\\n---> 45 return builder.getOrCreate()\\n\\nFile c:\\\\Users\\\\albert\\\\source\\\\repos\\\\QBIDS\\\\dwh_spark_processing\\\\.venv\\\\Lib\\\\site-packages\\\\pyspark\\\\sql\\\\session.py:497, in SparkSession.Builder.getOrCreate(self)\\n    495     sparkConf.set(key, value)\\n    496 # This SparkContext may be an existing one.\\n--> 497 sc = SparkContext.getOrCreate(sparkConf)\\n    498 # Do not update `SparkConf` for existing `SparkContext`, as it's shared\\n    499 # by all sessions.\\n    500 session = SparkSession(sc, options=self._options)\\n\\nFile c:\\\\Users\\\\albert\\\\source\\\\repos\\\\QBIDS\\\\dwh_spark_processing\\\\.venv\\\\Lib\\\\site-packages\\\\pyspark\\\\context.py:515, in SparkContext.getOrCreate(cls, conf)\\n    513 with SparkContext._lock:\\n    514     if SparkContext._active_spark_context is None:\\n--> 515         SparkContext(conf=conf or SparkConf())\\n    516     assert SparkContext._active_spark_context is not None\\n    517     return SparkContext._active_spark_context\\n\\nFile c:\\\\Users\\\\albert\\\\source\\\\repos\\\\QBIDS\\\\dwh_spark_processing\\\\.venv\\\\Lib\\\\site-packages\\\\pyspark\\\\context.py:201, in SparkContext.__init__(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\\n    195 if gateway is not None and gateway.gateway_parameters.auth_token is None:\\n    196     raise ValueError(\\n    197         \\\"You are trying to pass an insecure Py4j gateway to Spark. This\\\"\\n    198         \\\" is not allowed as it is a security risk.\\\"\\n    199     )\\n--> 201 SparkContext._ensure_initialized(self, gateway=gateway, conf=conf)\\n    202 try:\\n    203     self._do_init(\\n    204         master,\\n    205         appName,\\n   (...)    215         memory_profiler_cls,\\n    216     )\\n\\nFile c:\\\\Users\\\\albert\\\\source\\\\repos\\\\QBIDS\\\\dwh_spark_processing\\\\.venv\\\\Lib\\\\site-packages\\\\pyspark\\\\context.py:436, in SparkContext._ensure_initialized(cls, instance, gateway, conf)\\n    434 with SparkContext._lock:\\n    435     if not SparkContext._gateway:\\n--> 436         SparkContext._gateway = gateway or launch_gateway(conf)\\n    437         SparkContext._jvm = SparkContext._gateway.jvm\\n    439     if instance:\\n\\nFile c:\\\\Users\\\\albert\\\\source\\\\repos\\\\QBIDS\\\\dwh_spark_processing\\\\.venv\\\\Lib\\\\site-packages\\\\pyspark\\\\java_gateway.py:107, in launch_gateway(conf, popen_kwargs)\\n    104     time.sleep(0.1)\\n    106 if not os.path.isfile(conn_info_file):\\n--> 107     raise PySparkRuntimeError(\\n    108         error_class=\\\"JAVA_GATEWAY_EXITED\\\",\\n    109         message_parameters={},\\n    110     )\\n    112 with open(conn_info_file, \\\"rb\\\") as info:\\n    113     gateway_port = read_int(info)\\n\\nPySparkRuntimeError: [JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number.\\n\",\n",
      "  \"output_notebook\": \"notebook_outputs\\\\01_process_data_20251125_011639.ipynb\"\n",
      "}\n",
      "\n",
      "‚ùå Test FAILED!\n"
     ]
    }
   ],
   "source": [
    "# Test: Roep worker notebook aan\n",
    "print(\"=\" * 70)\n",
    "print(\"TESTING MSSPARKUTILS.NOTEBOOK.RUN()\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "result = mssparkutils.notebook.run(\n",
    "    \"01_process_data\",\n",
    "    timeout_seconds=300,\n",
    "    arguments={\n",
    "        \"source\": \"sales_data\",\n",
    "        \"run_ts\": datetime.now().isoformat(),\n",
    "        \"process_type\": \"bronze\"\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"RESULT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "result_dict = json.loads(result)\n",
    "print(json.dumps(result_dict, indent=2))\n",
    "\n",
    "if result_dict[\"status\"] == \"success\":\n",
    "    print(\"\\n‚úÖ Test PASSED!\")\n",
    "else:\n",
    "    print(\"\\n‚ùå Test FAILED!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dwh-spark-processing (3.11.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
