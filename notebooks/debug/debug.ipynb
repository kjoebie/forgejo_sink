{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# 00 — Master Orchestrator: Bronze → Silver Processing\n",
    "\n",
    "Main orchestration notebook for processing parquet files through Bronze and Silver layers.\n",
    "\n",
    "## Architecture Overview\n",
    "\n",
    "```\n",
    "Parquet Files (Files/{source}/{run_ts}/)\n",
    "    ↓\n",
    "Bronze Layer (append with run_ts for CDC)\n",
    "    ↓\n",
    "Silver Layer (CDC merge: INSERT/UPDATE/DELETE)\n",
    "    ↓\n",
    "Watermark Update (incremental tables only)\n",
    "```\n",
    "\n",
    "## Process Flow\n",
    "\n",
    "1. **Load Configuration** (DAG, enabled tables, retry filter)\n",
    "2. **Check Incremental** → Run watermark merge if needed\n",
    "3. **Bronze Processing** → Parallel table loading (10 workers)\n",
    "4. **Bronze Logging** → Batch log all results\n",
    "5. **Silver Processing** → Parallel CDC merge (tables with business_keys)\n",
    "6. **Silver Logging** → Batch log all results\n",
    "7. **Summary Statistics** → Performance metrics, efficiency\n",
    "\n",
    "## Key Features\n",
    "\n",
    "- **Parallel Processing**: ThreadPoolExecutor for 5-10x speedup\n",
    "- **Idempotency**: Check logs before reprocessing\n",
    "- **Retry Support**: Process only specific tables\n",
    "- **Error Resilience**: Continue on failure, comprehensive logging\n",
    "- **Performance Tracking**: Efficiency metrics (theoretical vs actual time)\n",
    "\n",
    "## Parameters\n",
    "\n",
    "- `source`: Source system name (e.g., \"vizier\")\n",
    "- `run_ts`: Run timestamp (e.g., \"20251105T142752505\")\n",
    "- `dag_path`: Path to DAG configuration JSON\n",
    "- `retry_tables`: Optional list of tables to retry\n",
    "- `force_reload`: Ignore log and reload all\n",
    "- `max_workers`: Parallel workers (default: 10)\n",
    "- `debug`: Enable debug output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters (Papermill compatible)\n",
    "source = \"anva_meeus\"                               # Source system name\n",
    "run_ts = \"20251001T183103260\"                       # Run timestamp\n",
    "dag_path = \"config/dag_anva_meeus_week.json\"        # DAG configuration path\n",
    "retry_tables = None                                 # Optional: list of table names to retry\n",
    "force_reload = True                                 # If True, ignore logs and reload all\n",
    "max_workers = 10                                    # Parallel workers for table processing\n",
    "debug = True                                        # Enable debug output\n",
    "log_to_console = True                               # Also stream logs to stdout/stderr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## [1] Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timezone\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from typing import List, Dict, Any\n",
    "import json\n",
    "from uuid import uuid4\n",
    "from modules.logging_utils import configure_logging\n",
    "import logging\n",
    "\n",
    "log_file = configure_logging(run_name=\"master_orchestrator\", enable_console_logging=log_to_console)\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.info(\"Logfile: %s\", log_file)\n",
    "\n",
    "logger.info(\"=\"*80)\n",
    "logger.info(\"MASTER ORCHESTRATOR STARTING\")\n",
    "logger.info(\"=\"*80)\n",
    "logger.info(f\"Source: {source}\")\n",
    "logger.info(f\"Run TS: {run_ts}\")\n",
    "logger.info(f\"DAG: {dag_path}\")\n",
    "logger.info(f\"Retry tables: {retry_tables}\")\n",
    "logger.info(f\"Force reload: {force_reload}\")\n",
    "logger.info(f\"Max workers: {max_workers}\")\n",
    "logger.info(f\"Debug: {debug}\")\n",
    "logger.info(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## [2] Load Utility Notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "# Load logging utilities\n",
    "# Load config utilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## [1.5] Initialize Spark Session\n",
    "\n",
    "# Check if Spark session exists (Fabric/Databricks has it by default)\n",
    "# For local environments, create it\n",
    "try:\n",
    "    spark\n",
    "    logger.info(\"✓ Spark session already available\")\n",
    "except NameError:\n",
    "    logger.info(\"Creating Spark session for local environment...\")\n",
    "    from pyspark.sql import SparkSession\n",
    "    \n",
    "    #todo: use spark_config.py and override spark defaults. Enable Hive support is needed.\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"DWH_Bronze_Silver_Processing\") \\\n",
    "        .enableHiveSupport() \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    logger.info(\"✓ Spark session created\")\n",
    "\n",
    "spark.conf.set(\"spark.sql.parquet.datetimeRebaseModeInRead\", \"CORRECTED\")\n",
    "spark.conf.set(\"spark.sql.parquet.int96RebaseModeInRead\", \"CORRECTED\")\n",
    "spark.conf.set(\"spark.sql.parquet.datetimeRebaseModeInWrite\", \"CORRECTED\")\n",
    "spark.conf.set(\"spark.sql.parquet.int96RebaseModeInWrite\", \"CORRECTED\")\n",
    "\n",
    "# Verify Spark session\n",
    "logger.info(f\"  Spark version: {spark.version}\")\n",
    "logger.info(f\"  Application ID: {spark.sparkContext.applicationId}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.table(\"anva_meeus.Dim_Tekenjaar\") \\\n",
    "     .show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "(\n",
    "    spark.table(\"anva_meeus.Dim_DekkingVariabel\") \n",
    "        .groupBy(\"Index\") \n",
    "        .count() \n",
    "        .filter(F.col(\"count\") > 1) \n",
    "        #.select(\"Index\")\n",
    "        .orderBy(F.desc(\"count\"))\n",
    "        .show(20, truncate=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.table(\"anva_meeus.Dim_DekkingVariabel\").createOrReplaceTempView(\"mytable\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    SELECT Index\n",
    "    FROM mytable\n",
    "    GROUP BY Index\n",
    "    HAVING COUNT(1) > 1\n",
    "\"\"\").show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over all dimension tables and check for duplicates\n",
    "tables = spark.catalog.listTables(\"anva_meeus\")\n",
    "\n",
    "dim_tables = [t.name for t in tables if t.name.lower().startswith(\"dim_\")]\n",
    "\n",
    "for table_name in dim_tables:\n",
    "    logger.info(f\"Processing table: {table_name}\")\n",
    "    df = spark.table(f\"anva_meeus.{table_name}\")\n",
    "    total_count = df.count()\n",
    "    logger.info(f\"  Total records: {total_count}\")\n",
    "    duplicate_counts = (\n",
    "        df.groupBy(df.columns)\n",
    "        .count()\n",
    "        .filter(F.col(\"count\") > 1)\n",
    "    )\n",
    "    dup_count = duplicate_counts.count()\n",
    "    if dup_count > 0:\n",
    "        logger.warning(f\"  Found {dup_count} duplicate records in {table_name}\")\n",
    "        duplicate_counts.show(20, truncate=False)\n",
    "    else:\n",
    "        logger.info(f\"  No duplicate records found in {table_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over all dimension tables and check for duplicates on columns that end with _id\n",
    "tables = spark.catalog.listTables(\"anva_meeus\") \n",
    "fact_tables = [t.name for t in tables if t.name.lower().startswith(\"fact_\") and t.name.lower() == \"fact_agenda\"]\n",
    "\n",
    "for table_name in fact_tables:\n",
    "    logger.info(f\"Processing table {table_name} for _id duplicates..\")\n",
    "    df = spark.table(f\"anva_meeus.{table_name}\")\n",
    "    id_columns = [col for col in df.columns if col.lower().endswith(\"_id\") or col.lower() == \"schadenummer\" or col.lower() == \"taak_datum\" or col.lower() == \"aanmaak_datum\"]\n",
    "    if not id_columns:\n",
    "        logger.info(f\"  No _id columns found in {table_name}, skipping.\")\n",
    "        continue\n",
    "    duplicate_counts = (\n",
    "        df.groupBy(id_columns)\n",
    "        .count()\n",
    "        .filter(F.col(\"count\") > 1)\n",
    "    )\n",
    "    dup_count = duplicate_counts.count()\n",
    "    if dup_count > 0:\n",
    "        logger.warning(f\"  Found {dup_count} duplicate records based on _id columns in {table_name}\")\n",
    "        duplicate_counts.show(20, truncate=False)\n",
    "    else:\n",
    "        logger.info(f\"  No duplicate records found based on _id columns in {table_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "spark.table(\"anva_meeus.Fact_Agenda\")\n",
    "    #.where(\"Medewerker_Id == 'A0779275'\" and \"Relatie_Id == '1658625'\" and \"Polis_Id == '1658625020032003'\" and \"Schadenummer == '3034776'\")\n",
    "    .where(\"Medewerker_Id == 'REGGSOE'\" and \"Relatie_Id == '3066025'\" and \"Polis_Id == '3066025020012001'\" and \"Schadenummer == '2743553'\")\n",
    "    .show(20, truncate=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.table(\"logs.bronze_processing_log\").show(20, truncate=False)\n",
    "#spark.sql(\"DESCRIBE EXTENDED logs.bronze_processing_log\").show(200, truncate=False)\n",
    "#spark.sql(\"DESCRIBE EXTENDED logs.bronze_run_summary\").show(200, truncate=False)\n",
    "#spark.catalog.refreshTable(\"logs.bronze_processing_log\")\n",
    "\n",
    "# detail = spark.sql(\"DESCRIBE DETAIL logs.bronze_processing_log\")\n",
    "# detail.show(truncate=False)\n",
    "\n",
    "\n",
    "#spark.sql(\"create table logs.bronze_processing_log_copy uSING DELTA AS SELECT * FROM logs.bronze_processing_log;\")\n",
    "\n",
    "#spark.table(\"logs.bronze_processing_log_copy\").show(20, truncate=False)\n",
    "# loc = (spark.sql(\"DESCRIBE DETAIL logs.bronze_processing_log\")\n",
    "#          .select(\"location\")\n",
    "#          .collect()[0][0])\n",
    "\n",
    "# spark.sql(\"DROP TABLE IF EXISTS logs.bronze_processing_log\")\n",
    "\n",
    "# spark.sql(f\"\"\"\n",
    "# CREATE TABLE logs.bronze_processing_log\n",
    "# USING DELTA\n",
    "# LOCATION '{loc}'\n",
    "# \"\"\")\n",
    "\n",
    "# spark.sql(f\"\"\"\n",
    "# INSERT INTO logs.bronze_processing_log\n",
    "# SELECT * FROM logs.bronze_processing_log_copy\n",
    "# \"\"\")\n",
    "\n",
    "#spark.table(\"logs.bronze_processing_log\").show(truncate=False)\n",
    "\n",
    "#spark.sql(\"DESCRIBE EXTENDED logs.bronze_processing_log\").show(200, truncate=False)\n",
    "\n",
    "#spark.sql(\"DESCRIBE EXTENDED anva_meeus.dim_agent\").show(200,truncate=False)\n",
    "#spark.catalog.listColumns(\"logs.bronze_processing_log\")\n",
    "\n",
    "# spark.sql(\"\"\"\n",
    "# CREATE OR REPLACE VIEW logs.bronze_processing_log_pbi AS\n",
    "# SELECT\n",
    "#     log_id,\n",
    "#     run_log_id,\n",
    "#     run_id,\n",
    "#     run_date,\n",
    "#     run_ts,\n",
    "#     source,\n",
    "#     table_name,\n",
    "#     partition_key,\n",
    "#     load_mode,\n",
    "#     status,\n",
    "#     rows_processed,\n",
    "#     start_time,\n",
    "#     end_time,\n",
    "#     duration_seconds,\n",
    "#     error_message\n",
    "# FROM logs.bronze_processing_log\n",
    "# \"\"\")\n",
    "\n",
    "# spark.sql(\"\"\"\n",
    "# CREATE OR REPLACE VIEW logs.bronze_run_summary_pbi AS\n",
    "# SELECT\n",
    "#     log_id\n",
    "#     ,run_id\n",
    "#     ,run_date\n",
    "#     ,run_ts\n",
    "#     ,source\n",
    "#     ,status\n",
    "#     ,run_start\n",
    "#     ,run_end\n",
    "#     ,duration_seconds\n",
    "#     ,total_tables\n",
    "#     ,tables_success\n",
    "#     ,tables_empty\n",
    "#     ,tables_failed\n",
    "#     ,tables_skipped\n",
    "#     ,total_rows\n",
    "#     ,workers\n",
    "#     ,sum_task_seconds\n",
    "#     ,theoretical_min_sec\n",
    "#     ,actual_time_sec\n",
    "#     ,efficiency_pct\n",
    "#     ,failed_tables\n",
    "#     ,error_message\n",
    "# FROM logs.bronze_run_summary\n",
    "# \"\"\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dwh-spark-processing (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
