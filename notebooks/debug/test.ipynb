{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Metadata JSON generator\n",
    "\n",
    "This notebook builds pipeline metadata JSON based on SQL Server metadata stored in Parquet and configuration DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Parameter setup\n",
    "Define the Fabric parameter used to filter metadata for a specific source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "# This cell is tagged so Fabric pipelines can override the default value.\n",
    "source = \"anva_meeus\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Imports and helper utilities\n",
    "Load dependencies and helper functions for identifier sanitization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "\n",
    "from pyspark.sql import DataFrame, functions as F, types as T\n",
    "\n",
    "try:\n",
    "    from notebookutils import mssparkutils  # Fabric\n",
    "except ImportError:  # Local Spark\n",
    "    mssparkutils = None\n",
    "\n",
    "\n",
    "def make_safe_identifier(name: str) -> str:\n",
    "    \"Return a Spark/Delta safe identifier.\"\n",
    "    cleaned = re.sub(r\"[^0-9A-Za-z_]+\", \"_\", name.strip())\n",
    "    cleaned = re.sub(r\"^[^A-Za-z_]+\", \"\", cleaned)\n",
    "    cleaned = cleaned or \"col\"\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "make_safe_identifier_udf = F.udf(make_safe_identifier, T.StringType())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Define configuration DataFrames\n",
    "Create in-notebook DataFrames for source mapping, disabled tables, size classes, load modes, window settings, and exclusions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1) Bron/server/database mapping\n",
    "sources_schema = T.StructType([\n",
    "    T.StructField(\"Bron\", T.StringType(), False),\n",
    "    T.StructField(\"Server\", T.StringType(), False),\n",
    "    T.StructField(\"Database\", T.StringType(), False),\n",
    "])\n",
    "\n",
    "df_sources = spark.createDataFrame(\n",
    "    [\n",
    "        (\"ccs_level\", \"vmdwhidpweu01\", \"InsuranceData_CCS_DWH\"),\n",
    "        (\"anva_meeus\", \"vmdwhidpweu01\\MEEUS\", \"InsuranceData_ANVA_DWH\"),\n",
    "        (\"vizier\", \"viz-sql01-mi-p.1d57ac4f4d63.database.windows.net\", \"CRM_DWH\"),\n",
    "        (\"ods_reports\", \"vmdwhodsanvpweu\", \"OG_ODS_Reports\"),\n",
    "        (\"anva_concern\", \"vmdwhidpweu01\", \"InsuranceData_ANVA_DWH\"),\n",
    "        (\"insurance_data_im\", \"vmdwhidpweu01\", \"InsuranceData_OpGroen_DWH\"),\n",
    "    ],\n",
    "    schema=sources_schema,\n",
    ")\n",
    "\n",
    "# 2) Disabled tables\n",
    "_disabled_schema = T.StructType([\n",
    "    T.StructField(\"Bron\", T.StringType(), False),\n",
    "    T.StructField(\"schema_name\", T.StringType(), False),\n",
    "    T.StructField(\"obj_name\", T.StringType(), False),\n",
    "])\n",
    "\n",
    "df_disabled_tables = spark.createDataFrame(\n",
    "    [\n",
    "        (\"anva_concern\", \"dbo\", \"Jobmonitor\"),\n",
    "        (\"anva_concern\", \"dbo\", \"LaatsteVerversing\"),\n",
    "        (\"anva_concern\", \"dbo\", \"Metadata\"),\n",
    "        (\"anva_concern\", \"dbo\", \"VrijeLabels\"),\n",
    "        (\"anva_concern\", \"pbi\", \"Nulmeting_Clausules\"),\n",
    "        (\"anva_concern\", \"pbi\", \"Nulmeting_CodesDekking\"),\n",
    "        (\"anva_concern\", \"pbi\", \"Nulmeting_CodesNAW\"),\n",
    "        (\"anva_concern\", \"pbi\", \"Nulmeting_CodesPolis\"),\n",
    "        (\"anva_concern\", \"pbi\", \"Nulmeting_LabelDekking\"),\n",
    "        (\"anva_concern\", \"pbi\", \"Nulmeting_LabelNAW\"),\n",
    "        (\"anva_concern\", \"pbi\", \"Nulmeting_LabelPolis\"),\n",
    "        (\"anva_concern\", \"pbi\", \"Nulmeting_NAWDetails\"),\n",
    "        (\"anva_concern\", \"pbi\", \"Nulmeting_NAWLabels\"),\n",
    "        (\"anva_concern\", \"pbi\", \"Nulmeting_PolisDetails\"),\n",
    "        (\"anva_concern\", \"pbi\", \"Nulmeting_PolisLabels\"),\n",
    "        (\"anva_concern\", \"pbi\", \"Nulmeting_Voorwaarden\"),\n",
    "        (\"anva_meeus\", \"dbo\", \"Jobmonitor\"),\n",
    "        (\"anva_meeus\", \"dbo\", \"LaatsteVerversing\"),\n",
    "        (\"anva_meeus\", \"dbo\", \"Metadata\"),\n",
    "        (\"anva_meeus\", \"dbo\", \"VrijeLabels\"),\n",
    "        (\"anva_meeus\", \"pbi\", \"Nulmeting_Clausules\"),\n",
    "        (\"anva_meeus\", \"pbi\", \"Nulmeting_CodesDekking\"),\n",
    "        (\"anva_meeus\", \"pbi\", \"Nulmeting_CodesNAW\"),\n",
    "        (\"anva_meeus\", \"pbi\", \"Nulmeting_CodesPolis\"),\n",
    "        (\"anva_meeus\", \"pbi\", \"Nulmeting_LabelDekking\"),\n",
    "        (\"anva_meeus\", \"pbi\", \"Nulmeting_LabelNAW\"),\n",
    "        (\"anva_meeus\", \"pbi\", \"Nulmeting_LabelPolis\"),\n",
    "        (\"anva_meeus\", \"pbi\", \"Nulmeting_NAWDetails\"),\n",
    "        (\"anva_meeus\", \"pbi\", \"Nulmeting_NAWLabels\"),\n",
    "        (\"anva_meeus\", \"pbi\", \"Nulmeting_PolisDetails\"),\n",
    "        (\"anva_meeus\", \"pbi\", \"Nulmeting_PolisLabels\"),\n",
    "        (\"anva_meeus\", \"pbi\", \"Nulmeting_Voorwaarden\"),\n",
    "    ],\n",
    "    schema=_disabled_schema,\n",
    ")\n",
    "\n",
    "# 3) Size class\n",
    "size_schema = T.StructType([\n",
    "    T.StructField(\"Bron\", T.StringType(), False),\n",
    "    T.StructField(\"schema_name\", T.StringType(), False),\n",
    "    T.StructField(\"obj_name\", T.StringType(), False),\n",
    "    T.StructField(\"size_class\", T.StringType(), False),\n",
    "])\n",
    "\n",
    "df_size_class = spark.createDataFrame(\n",
    "    [\n",
    "        (\"anva_concern\", \"pbi\", \"Fact_PremieFacturen\", \"L\"),\n",
    "        (\"ccs_level\", \"pbi\", \"Fact_PremieBoekingen\", \"L\"),\n",
    "        (\"geintegreerd_model\", \"pbi\", \"Fact_PremieFacturen\", \"L\"),\n",
    "        (\"anva_meeus\", \"pbi\", \"Fact_PremieFacturen\", \"L\"),\n",
    "    ],\n",
    "    schema=size_schema,\n",
    ")\n",
    "\n",
    "# 4) Load mode\n",
    "load_schema = T.StructType([\n",
    "    T.StructField(\"Bron\", T.StringType(), False),\n",
    "    T.StructField(\"schema_name\", T.StringType(), False),\n",
    "    T.StructField(\"obj_name\", T.StringType(), False),\n",
    "    T.StructField(\"load_mode\", T.StringType(), False),\n",
    "    T.StructField(\"filter_column\", T.StringType(), True),\n",
    "    T.StructField(\"kind\", T.StringType(), True),\n",
    "])\n",
    "\n",
    "df_load_mode = spark.createDataFrame(\n",
    "    [\n",
    "        (\"anva_concern\", \"pbi\", \"Fact_PremieFacturen\", \"window\", \"Boek_Datum\", \"datetime\"),\n",
    "        (\"ccs_level\", \"pbi\", \"Fact_PremieBoekingen\", \"window\", \"Boek_Datum\", \"datetime\"),\n",
    "        (\"geintegreerd_model\", \"pbi\", \"Fact_PremieFacturen\", \"window\", \"Boek_Datum\", \"datetime\"),\n",
    "        (\"anva_meeus\", \"pbi\", \"Fact_PremieFacturen\", \"window\", \"Boek_Datum\", \"datetime\"),\n",
    "        (\"vizier\", \"dbo\", \"Relaties\", \"incremental\", \"Updatedatum\", \"stamp17\"),\n",
    "        (\"vizier\", \"dbo\", \"Contactpersonen\", \"incremental\", \"Upd_dt\", \"stamp17\"),\n",
    "        (\"vizier\", \"dbo\", \"Sleutels\", \"incremental\", \"upd\", \"stamp17\"),\n",
    "        (\"vizier\", \"dbo\", \"Polissen\", \"incremental\", \"upd_dt\", \"stamp17\"),\n",
    "        (\"vizier\", \"dbo\", \"Schades\", \"incremental\", \"upd_dt\", \"stamp17\"),\n",
    "        (\"vizier\", \"dbo\", \"DnB\", \"incremental\", \"upd_dt\", \"stamp17\"),\n",
    "        (\"vizier\", \"dbo\", \"Contactmomenten\", \"incremental\", \"Upd\", \"stamp17\"),\n",
    "        (\"vizier\", \"dbo\", \"Taken\", \"incremental\", \"upd\", \"stamp17\"),\n",
    "        (\"vizier\", \"dbo\", \"Sales\", \"incremental\", \"UPD_DT\", \"stamp17\"),\n",
    "        (\"vizier\", \"dbo\", \"Retenties\", \"incremental\", \"UPD_DT\", \"stamp17\"),\n",
    "        (\"vizier\", \"dbo\", \"Adresbeeld\", \"incremental\", \"UPD_DT\", \"stamp17\"),\n",
    "        (\"vizier\", \"dbo\", \"UBO_Onderzoeken\", \"incremental\", \"UPD_DT\", \"stamp17\"),\n",
    "        (\"vizier\", \"dbo\", \"Producten\", \"incremental\", \"upd\", \"stamp17\"),\n",
    "        (\"vizier\", \"dbo\", \"Medewerkers\", \"incremental\", \"id_upd\", \"stamp17\"),\n",
    "        (\"vizier\", \"dbo\", \"Klachten\", \"incremental\", \"upd_dt\", \"stamp17\"),\n",
    "        (\"vizier\", \"dbo\", \"Verkoopkansen\", \"incremental\", \"upd\", \"stamp17\"),\n",
    "        (\"vizier\", \"dbo\", \"Interesses\", \"incremental\", \"UPD_DT\", \"stamp17\"),\n",
    "    ],\n",
    "    schema=load_schema,\n",
    ")\n",
    "\n",
    "# 5) Window config\n",
    "window_schema = T.StructType([\n",
    "    T.StructField(\"Bron\", T.StringType(), False),\n",
    "    T.StructField(\"schema_name\", T.StringType(), False),\n",
    "    T.StructField(\"obj_name\", T.StringType(), False),\n",
    "    T.StructField(\"partition_column\", T.StringType(), False),\n",
    "    T.StructField(\"granularity\", T.StringType(), False),\n",
    "    T.StructField(\"lookback_months\", T.IntegerType(), False),\n",
    "])\n",
    "\n",
    "df_window_config = spark.createDataFrame(\n",
    "    [\n",
    "        (\"anva_concern\", \"pbi\", \"Fact_PremieFacturen\", \"Boek_Datum\", \"month\", 12),\n",
    "        (\"geintegreerd_model\", \"pbi\", \"Fact_PremieFacturen\", \"Boek_Datum\", \"month\", 12),\n",
    "        (\"anva_meeus\", \"pbi\", \"Fact_PremieFacturen\", \"Boek_Datum\", \"month\", 12),\n",
    "        (\"ccs_level\", \"pbi\", \"Fact_PremieBoekingen\", \"Boek_Datum\", \"month\", 12),\n",
    "    ],\n",
    "    schema=window_schema,\n",
    ")\n",
    "\n",
    "# 6) Excluded tables\n",
    "excluded_schema = T.StructType([\n",
    "    T.StructField(\"Bron\", T.StringType(), False),\n",
    "    T.StructField(\"schema_name\", T.StringType(), False),\n",
    "    T.StructField(\"obj_name\", T.StringType(), False),\n",
    "    T.StructField(\"excluded\", T.IntegerType(), False),\n",
    "])\n",
    "\n",
    "df_excluded_tables = spark.createDataFrame(\n",
    "    [\n",
    "        (\"vizier\", \"dbo\", \"BO_sleutels_Wim_Verheijen\", 1),\n",
    "        (\"vizier\", \"dbo\", \"UMG_Historie\", 1),\n",
    "    ],\n",
    "    schema=excluded_schema,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Load SQL metadata from Parquet\n",
    "Resolve the source mapping and read the sqlmetadata Parquet file for the chosen source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Resolve source mapping and load dbo.sqlmetadata from Parquet\n",
    "source_row = df_sources.filter(F.col(\"Bron\") == source).limit(1).collect()\n",
    "if not source_row:\n",
    "    raise ValueError(f\"Unknown source '{source}' - check df_sources\")\n",
    "\n",
    "server = source_row[0][\"Server\"]\n",
    "database = source_row[0][\"Database\"]\n",
    "\n",
    "metadata_path = \"Files/metadata/sqlmetadata.parquet\"\n",
    "sql_metadata = (\n",
    "    spark.read.format(\"parquet\")\n",
    "    .load(metadata_path)\n",
    "    .filter(F.col(\"server_name\") == server)\n",
    "    .filter(F.col(\"db_name\") == database)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Build base queries with type mapping\n",
    "Construct SELECT projections with explicit type mappings and sanitized column aliases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Build base_query strings with explicit type mappings\n",
    "\n",
    "def build_projection(row: T.Row) -> str:\n",
    "    col = row[\"column_name\"]\n",
    "    dtype = (row[\"data_type\"] or \"\").lower()\n",
    "    precision = row[\"numeric_precision\"]\n",
    "    scale = row[\"numeric_scale\"]\n",
    "    safe = make_safe_identifier(col)\n",
    "\n",
    "    if dtype in {\"decimal\", \"numeric\"} and precision is not None and scale is not None:\n",
    "        expr = f\"CAST([{col}] AS decimal({precision},{scale}))\"\n",
    "    elif dtype == \"money\":\n",
    "        expr = f\"CAST([{col}] AS decimal(19,4))\"\n",
    "    elif dtype == \"smallmoney\":\n",
    "        expr = f\"CAST([{col}] AS decimal(10,4))\"\n",
    "    elif dtype == \"tinyint\":\n",
    "        expr = f\"CAST([{col}] AS smallint)\"\n",
    "    elif dtype in {\"smallint\", \"int\", \"bigint\", \"bit\", \"float\", \"real\"}:\n",
    "        expr = f\"CAST([{col}] AS {dtype})\"\n",
    "    elif dtype == \"date\":\n",
    "        expr = f\"CAST([{col}] AS date)\"\n",
    "    elif dtype == \"datetime\":\n",
    "        expr = f\"CAST([{col}] AS datetime2(3))\"\n",
    "    elif dtype == \"smalldatetime\":\n",
    "        expr = f\"CAST([{col}] AS datetime2(0))\"\n",
    "    elif dtype == \"datetime2\":\n",
    "        expr = f\"CAST([{col}] AS datetime2(6))\"\n",
    "    elif dtype == \"time\":\n",
    "        expr = f\"CONVERT(varchar(8), [{col}], 108)\"\n",
    "    elif dtype == \"datetimeoffset\":\n",
    "        expr = f\"CAST(SWITCHOFFSET([{col}], '+00:00') AS datetime2(6))\"\n",
    "    elif dtype in {\"char\", \"varchar\", \"nchar\", \"nvarchar\"}:\n",
    "        expr = f\"[{col}]\"\n",
    "    elif dtype == \"text\":\n",
    "        expr = f\"CONVERT(varchar(max), [{col}])\"\n",
    "    elif dtype == \"ntext\":\n",
    "        expr = f\"CONVERT(nvarchar(max), [{col}])\"\n",
    "    elif dtype in {\"binary\", \"varbinary\"}:\n",
    "        expr = f\"[{col}]\"\n",
    "    elif dtype == \"image\":\n",
    "        expr = f\"CONVERT(varbinary(max), [{col}])\"\n",
    "    elif dtype in {\"rowversion\", \"timestamp\"}:\n",
    "        expr = f\"CAST([{col}] AS varbinary(8))\"\n",
    "    elif dtype == \"uniqueidentifier\":\n",
    "        expr = f\"CONVERT(varchar(36), [{col}])\"\n",
    "    elif dtype == \"xml\":\n",
    "        expr = f\"CONVERT(nvarchar(max), [{col}])\"\n",
    "    elif dtype == \"hierarchyid\":\n",
    "        expr = f\"{col}.ToString()\"\n",
    "    elif dtype in {\"geometry\", \"geography\"}:\n",
    "        expr = f\"{col}.STAsBinary()\"\n",
    "    elif dtype == \"sql_variant\":\n",
    "        expr = f\"CONVERT(nvarchar(max), [{col}])\"\n",
    "    else:\n",
    "        expr = f\"[{col}]\"\n",
    "\n",
    "    return f\"{expr} AS [{safe}]\"\n",
    "\n",
    "\n",
    "metadata_filtered = sql_metadata.select(\n",
    "    \"schema_name\",\n",
    "    \"obj_name\",\n",
    "    \"column_name\",\n",
    "    \"data_type\",\n",
    "    \"numeric_precision\",\n",
    "    \"numeric_scale\",\n",
    "    \"ordinal_position\",\n",
    ").orderBy(\"schema_name\", \"obj_name\", \"ordinal_position\")\n",
    "\n",
    "base_query_rows: Dict[str, Dict[str, List[str]]] = {}\n",
    "for row in metadata_filtered.collect():\n",
    "    schema = row[\"schema_name\"]\n",
    "    table = row[\"obj_name\"]\n",
    "    projection = build_projection(row)\n",
    "    base_query_rows.setdefault(schema, {}).setdefault(table, []).append(projection)\n",
    "\n",
    "base_query_records = []\n",
    "for schema, tables in base_query_rows.items():\n",
    "    for table, columns in tables.items():\n",
    "        select_list = \", \".join(columns)\n",
    "        base_query = f\"SELECT {select_list} FROM [{schema}].[{table}]\"\n",
    "        base_query_records.append((schema, table, source, base_query))\n",
    "\n",
    "base_query_df = spark.createDataFrame(\n",
    "    base_query_records,\n",
    "    schema=T.StructType(\n",
    "        [\n",
    "            T.StructField(\"schema_name\", T.StringType(), False),\n",
    "            T.StructField(\"obj_name\", T.StringType(), False),\n",
    "            T.StructField(\"Bron\", T.StringType(), False),\n",
    "            T.StructField(\"base_query\", T.StringType(), False),\n",
    "        ]\n",
    "    ),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Merge configuration for all tables\n",
    "Combine base queries with table-level configuration to prepare the metadata records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Merge configuration\n",
    "\n",
    "tables = (\n",
    "    base_query_df.alias(\"bq\")\n",
    "    .join(df_disabled_tables.alias(\"dis\"), [\"Bron\", \"schema_name\", \"obj_name\"], \"left\")\n",
    "    .join(df_size_class.alias(\"sz\"), [\"Bron\", \"schema_name\", \"obj_name\"], \"left\")\n",
    "    .join(df_load_mode.alias(\"lm\"), [\"Bron\", \"schema_name\", \"obj_name\"], \"left\")\n",
    "    .join(df_window_config.alias(\"wnd\"), [\"Bron\", \"schema_name\", \"obj_name\"], \"left\")\n",
    "    .join(df_excluded_tables.alias(\"ex\"), [\"Bron\", \"schema_name\", \"obj_name\"], \"left\")\n",
    "    .withColumn(\"enabled\", F.when(F.col(\"dis.obj_name\").isNull(), F.lit(True)).otherwise(F.lit(False)))\n",
    "    .withColumn(\"size_class\", F.when(F.col(\"sz.size_class\").isNull(), F.lit(\"S\")).otherwise(F.col(\"sz.size_class\")))\n",
    "    .withColumn(\"load_mode\", F.when(F.col(\"lm.load_mode\").isNull(), F.lit(\"snapshot\")).otherwise(F.col(\"lm.load_mode\")))\n",
    "    .withColumn(\"excluded\", F.when(F.col(\"ex.excluded\").isNull(), F.lit(0)).otherwise(F.col(\"ex.excluded\")))\n",
    "    .withColumnRenamed(\"bq.base_query\", \"base_query\")\n",
    "    .select(\n",
    "        F.col(\"bq.obj_name\").alias(\"name\"),\n",
    "        \"schema_name\",\n",
    "        \"Bron\",\n",
    "        \"enabled\",\n",
    "        \"size_class\",\n",
    "        \"load_mode\",\n",
    "        \"base_query\",\n",
    "        F.col(\"lm.filter_column\"),\n",
    "        F.col(\"lm.kind\"),\n",
    "        F.col(\"wnd.partition_column\"),\n",
    "        F.col(\"wnd.granularity\"),\n",
    "        F.col(\"wnd.lookback_months\"),\n",
    "        \"excluded\",\n",
    "    )\n",
    "    .filter(F.col(\"excluded\") == 0)\n",
    "    .orderBy(\"name\")\n",
    ")\n",
    "\n",
    "tables.cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Assemble and persist metadata JSON\n",
    "Build the final JSON payload and write it to the Files/config directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assemble JSON payload\n",
    "\n",
    "def table_record(row: T.Row) -> Dict:\n",
    "    record = {\n",
    "        \"name\": row[\"name\"],\n",
    "        \"enabled\": bool(row[\"enabled\"]),\n",
    "        \"size_class\": row[\"size_class\"],\n",
    "        \"load_mode\": row[\"load_mode\"],\n",
    "        \"delta_schema\": row[\"Bron\"],\n",
    "        \"delta_table\": row[\"name\"],\n",
    "        \"base_query\": row[\"base_query\"],\n",
    "    }\n",
    "    if row[\"load_mode\"] == \"window\" and row[\"partition_column\"]:\n",
    "        record[\"window\"] = {\n",
    "            \"partition_column\": row[\"partition_column\"],\n",
    "            \"granularity\": row[\"granularity\"],\n",
    "            \"lookback_months\": int(row[\"lookback_months\"]),\n",
    "        }\n",
    "    if row[\"load_mode\"] == \"incremental\" and row[\"filter_column\"]:\n",
    "        record[\"incremental_column\"] = {\n",
    "            \"name\": row[\"filter_column\"],\n",
    "            \"kind\": row[\"kind\"],\n",
    "        }\n",
    "    return record\n",
    "\n",
    "\n",
    "payload = {\n",
    "    \"source\": source,\n",
    "    \"run_date_utc\": None,\n",
    "    \"watermarks_path\": \"config/watermarks.json\",\n",
    "    \"base_files\": \"greenhouse_sources\",\n",
    "    \"connection_name\": f\"connection_{source}_prod\",\n",
    "    \"defaults\": {\n",
    "        \"concurrency_large\": 2,\n",
    "        \"concurrency_small\": 8,\n",
    "        \"max_rows_per_file_large\": 15000000,\n",
    "        \"max_rows_per_file_small\": 1000000,\n",
    "    },\n",
    "    \"tables\": [table_record(row) for row in tables.collect()],\n",
    "}\n",
    "\n",
    "payload_path = Path(f\"Files/config/{source}_metadata.json\")\n",
    "payload_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "json_text = json.dumps(payload, ensure_ascii=False, indent=4)\n",
    "\n",
    "if mssparkutils is not None:\n",
    "    mssparkutils.fs.put(str(payload_path), json_text, overwrite=True)\n",
    "else:\n",
    "    with open(payload_path, \"w\", encoding=\"utf-8\") as fp:\n",
    "        fp.write(json_text)\n",
    "\n",
    "print(f\"Written metadata to {payload_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dwh-spark-processing (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
