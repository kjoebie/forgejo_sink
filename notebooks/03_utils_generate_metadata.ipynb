{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "faa6f3dd",
   "metadata": {},
   "source": [
    "# PySpark metadata generator\n",
    "\n",
    "Genereer een JSON configuratiebestand voor datapipeline metadata op basis van SQL Server metadata."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9e53c0",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "Stel de Fabric parameters in voor de gewenste bron en het pad naar het parquetbestand met SQL metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ae9f80",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "source = \"anva_concern\"\n",
    "#source = \"anva_meeus\"\n",
    "#source = \"ccs_level\"\n",
    "#source = \"insurance_data_im\"\n",
    "#source = \"ods_reports\"\n",
    "#source = \"vizier\"\n",
    "\n",
    "#metadata_path = \"Files/metadata/connection_anva_concern_prod_metadata.parquet\"\n",
    "metadata_path = f\"Files/metadata/connection_{source}_prod_metadata.parquet\"\n",
    "\n",
    "#metadata_path = \"Files/metadata/connection_anva_meeus_prod_metadata.parquet\"\n",
    "#metadata_path = \"Files/metadata/sqlmetadata.parquet\"\n",
    "\n",
    "# Toggle: gebruik load_mode configuratie of alles snapshot\n",
    "use_load_mode_config = False  # True = gebruik df_load_mode, False = alles snapshot\n",
    "\n",
    "log_to_console = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29dadd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module fabric.bootstrap\n",
    "# ---------------------\n",
    "# This cell enables a flexible module loading strategy:\n",
    "#\n",
    "# PRODUCTION (default): The `Files/code` directory is empty. This function does nothing,\n",
    "# and Python imports all modules from the stable, versioned Wheel in the Environment.\n",
    "#\n",
    "# DEVELOPMENT / HOTFIX: To bypass the 15-20 minute Fabric publish cycle for urgent fixes,\n",
    "# upload individual .py files to `Files/code` in the Lakehouse. This function prepends\n",
    "# that path to sys.path, so Python finds the override files first. All other modules\n",
    "# continue to load from the Wheel - only the uploaded files are replaced.\n",
    "#\n",
    "# Usage: Keep `Files/code` empty for production stability. Use it only for rapid\n",
    "# iteration during development or emergency hotfixes.\n",
    "\n",
    "from modules.fabric_bootstrap import ensure_module_path\n",
    "ensure_module_path()  # Now Python can find the rest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe74b7af",
   "metadata": {},
   "source": [
    "## Imports en helper functies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062ef2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable, Dict\n",
    "from modules.logging_utils import configure_logging\n",
    "from modules.path_utils import resolve_files_path, detect_environment\n",
    "import logging\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "import re  \n",
    "import json \n",
    "\n",
    "log_file = configure_logging(run_name=\"fabric_metadata_generator\", enable_console_logging=log_to_console)\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.info(\"Logfile: %s\", log_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7013b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.spark_session import get_or_create_spark_session\n",
    "\n",
    "spark = get_or_create_spark_session(\n",
    "    app_name=\"Metadata_Generator\",\n",
    "    enable_hive=True\n",
    ")\n",
    "\n",
    "# Configure mssparkutils based on environment\n",
    "environment = detect_environment(spark)\n",
    "logger.info(f\"Detected environment: {environment}\")\n",
    "\n",
    "if environment == 'fabric':\n",
    "    # Use Fabric's native mssparkutils\n",
    "    logger.info(\"Using Fabric native mssparkutils\")\n",
    "    try:\n",
    "        from notebookutils import mssparkutils\n",
    "    except ImportError:\n",
    "        logger.warning(\"notebookutils not found, falling back to mock\")\n",
    "        from modules.notebook_utils import get_mssparkutils\n",
    "        mssparkutils = get_mssparkutils(spark)\n",
    "else:\n",
    "    # Use mock for local/cluster environments\n",
    "    logger.info(\"Using mock mssparkutils for local/cluster environment\")\n",
    "    from modules.notebook_utils import get_mssparkutils\n",
    "    mssparkutils = get_mssparkutils(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99823c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import metadata generation utilities from module\n",
    "from modules.metadata_utils import (\n",
    "    make_safe_identifier,\n",
    "    column_expression,\n",
    "    build_base_query,\n",
    "    load_metadata,\n",
    "    validate_metadata\n",
    ")\n",
    "\n",
    "logger.info(\"âœ“ Metadata generation functions imported from modules.metadata_utils\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f51f3e",
   "metadata": {},
   "source": [
    "## Configuratie DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15df4429",
   "metadata": {},
   "outputs": [],
   "source": [
    "sources_schema = T.StructType([\n",
    "    T.StructField(\"Bron\", T.StringType(), False),\n",
    "    T.StructField(\"Server\", T.StringType(), False),\n",
    "    T.StructField(\"Database\", T.StringType(), False),\n",
    "])\n",
    "\n",
    "df_sources = spark.createDataFrame([\n",
    "    (\"ccs_level\", \"vmdwhidpweu01\", \"InsuranceData_CCS_DWH\"),\n",
    "    (r\"anva_meeus\", r\"vmdwhidpweu01\\MEEUS\", \"InsuranceData_ANVA_DWH\"),\n",
    "    (\"vizier\", \"viz-sql01-mi-p.1d57ac4f4d63.database.windows.net\", \"CRM_DWH\"),\n",
    "    (\"ods_reports\", \"vmdwhodsanvpweu\", \"OG_ODS_Reports\"),\n",
    "    (\"anva_concern\", \"vmdwhidpweu01\", \"InsuranceData_ANVA_DWH\"),\n",
    "    (\"insurance_data_im\", \"vmdwhidpweu01\", \"InsuranceData_OpGroen_DWH\"),\n",
    "], schema=sources_schema)\n",
    "\n",
    "_disabled_schema = T.StructType([\n",
    "    T.StructField(\"Bron\", T.StringType(), False),\n",
    "    T.StructField(\"schema_name\", T.StringType(), False),\n",
    "    T.StructField(\"obj_name\", T.StringType(), False),\n",
    "])\n",
    "\n",
    "df_disabled_tables = spark.createDataFrame([\n",
    "    (\"anva_concern\", \"dbo\", \"Jobmonitor\"),\n",
    "    (\"anva_concern\", \"dbo\", \"LaatsteVerversing\"),\n",
    "    (\"anva_concern\", \"dbo\", \"Metadata\"),\n",
    "    (\"anva_concern\", \"dbo\", \"VrijeLabels\"),\n",
    "    (\"anva_concern\", \"pbi\", \"Nulmeting_Clausules\"),\n",
    "    (\"anva_concern\", \"pbi\", \"Nulmeting_CodesDekking\"),\n",
    "    (\"anva_concern\", \"pbi\", \"Nulmeting_CodesNAW\"),\n",
    "    (\"anva_concern\", \"pbi\", \"Nulmeting_CodesPolis\"),\n",
    "    (\"anva_concern\", \"pbi\", \"Nulmeting_LabelDekking\"),\n",
    "    (\"anva_concern\", \"pbi\", \"Nulmeting_LabelNAW\"),\n",
    "    (\"anva_concern\", \"pbi\", \"Nulmeting_LabelPolis\"),\n",
    "    (\"anva_concern\", \"pbi\", \"Nulmeting_NAWDetails\"),\n",
    "    (\"anva_concern\", \"pbi\", \"Nulmeting_NAWLabels\"),\n",
    "    (\"anva_concern\", \"pbi\", \"Nulmeting_PolisDetails\"),\n",
    "    (\"anva_concern\", \"pbi\", \"Nulmeting_PolisLabels\"),\n",
    "    (\"anva_concern\", \"pbi\", \"Nulmeting_Voorwaarden\"),\n",
    "    (\"anva_meeus\", \"dbo\", \"Jobmonitor\"),\n",
    "    (\"anva_meeus\", \"dbo\", \"LaatsteVerversing\"),\n",
    "    (\"anva_meeus\", \"dbo\", \"Metadata\"),\n",
    "    (\"anva_meeus\", \"dbo\", \"VrijeLabels\"),\n",
    "    (\"anva_meeus\", \"pbi\", \"Nulmeting_Clausules\"),\n",
    "    (\"anva_meeus\", \"pbi\", \"Nulmeting_CodesDekking\"),\n",
    "    (\"anva_meeus\", \"pbi\", \"Nulmeting_CodesNAW\"),\n",
    "    (\"anva_meeus\", \"pbi\", \"Nulmeting_CodesPolis\"),\n",
    "    (\"anva_meeus\", \"pbi\", \"Nulmeting_LabelDekking\"),\n",
    "    (\"anva_meeus\", \"pbi\", \"Nulmeting_LabelNAW\"),\n",
    "    (\"anva_meeus\", \"pbi\", \"Nulmeting_LabelPolis\"),\n",
    "    (\"anva_meeus\", \"pbi\", \"Nulmeting_NAWDetails\"),\n",
    "    (\"anva_meeus\", \"pbi\", \"Nulmeting_NAWLabels\"),\n",
    "    (\"anva_meeus\", \"pbi\", \"Nulmeting_PolisDetails\"),\n",
    "    (\"anva_meeus\", \"pbi\", \"Nulmeting_PolisLabels\"),\n",
    "    (\"anva_meeus\", \"pbi\", \"Nulmeting_Voorwaarden\"),\n",
    "], schema=_disabled_schema)\n",
    "\n",
    "size_schema = T.StructType([\n",
    "    T.StructField(\"Bron\", T.StringType(), False),\n",
    "    T.StructField(\"schema_name\", T.StringType(), False),\n",
    "    T.StructField(\"obj_name\", T.StringType(), False),\n",
    "    T.StructField(\"size_class\", T.StringType(), False),\n",
    "])\n",
    "\n",
    "df_size_class = spark.createDataFrame([\n",
    "    (\"anva_concern\", \"pbi\", \"Fact_PremieFacturen\", \"L\"),\n",
    "    (\"ccs_level\", \"pbi\", \"Fact_PremieBoekingen\", \"L\"),\n",
    "    (\"geintegreerd_model\", \"pbi\", \"Fact_PremieFacturen\", \"L\"),\n",
    "    (\"anva_meeus\", \"pbi\", \"Fact_PremieFacturen\", \"L\"),\n",
    "], schema=size_schema)\n",
    "\n",
    "load_schema = T.StructType([\n",
    "    T.StructField(\"Bron\", T.StringType(), False),\n",
    "    T.StructField(\"schema_name\", T.StringType(), False),\n",
    "    T.StructField(\"obj_name\", T.StringType(), False),\n",
    "    T.StructField(\"load_mode\", T.StringType(), False),\n",
    "    T.StructField(\"filter_column\", T.StringType(), True),\n",
    "    T.StructField(\"kind\", T.StringType(), True),\n",
    "])\n",
    "\n",
    "df_load_mode = spark.createDataFrame([\n",
    "    (\"anva_concern\", \"pbi\", \"Fact_PremieFacturen\", \"window\", \"Boek_Datum\", \"datetime\"),\n",
    "    (\"ccs_level\", \"pbi\", \"Fact_PremieBoekingen\", \"window\", \"Boek_Datum\", \"datetime\"),\n",
    "    (\"geintegreerd_model\", \"pbi\", \"Fact_PremieFacturen\", \"window\", \"Boek_Datum\", \"datetime\"),\n",
    "    (\"anva_meeus\", \"pbi\", \"Fact_PremieFacturen\", \"window\", \"Boek_Datum\", \"datetime\"),\n",
    "    (\"vizier\", \"dbo\", \"Relaties\", \"incremental\", \"Updatedatum\", \"stamp17\"),\n",
    "    (\"vizier\", \"dbo\", \"Contactpersonen\", \"incremental\", \"Upd_dt\", \"stamp17\"),\n",
    "    (\"vizier\", \"dbo\", \"Sleutels\", \"incremental\", \"upd\", \"stamp17\"),\n",
    "    (\"vizier\", \"dbo\", \"Polissen\", \"incremental\", \"upd_dt\", \"stamp17\"),\n",
    "    (\"vizier\", \"dbo\", \"Schades\", \"incremental\", \"upd_dt\", \"stamp17\"),\n",
    "    (\"vizier\", \"dbo\", \"DnB\", \"incremental\", \"upd_dt\", \"stamp17\"),\n",
    "    (\"vizier\", \"dbo\", \"Contactmomenten\", \"incremental\", \"Upd\", \"stamp17\"),\n",
    "    (\"vizier\", \"dbo\", \"Taken\", \"incremental\", \"upd\", \"stamp17\"),\n",
    "    (\"vizier\", \"dbo\", \"Sales\", \"incremental\", \"UPD_DT\", \"stamp17\"),\n",
    "    (\"vizier\", \"dbo\", \"Retenties\", \"incremental\", \"UPD_DT\", \"stamp17\"),\n",
    "    (\"vizier\", \"dbo\", \"Adresbeeld\", \"incremental\", \"UPD_DT\", \"stamp17\"),\n",
    "    (\"vizier\", \"dbo\", \"UBO_Onderzoeken\", \"incremental\", \"UPD_DT\", \"stamp17\"),\n",
    "    (\"vizier\", \"dbo\", \"Producten\", \"incremental\", \"upd\", \"stamp17\"),\n",
    "    (\"vizier\", \"dbo\", \"Medewerkers\", \"incremental\", \"id_upd\", \"stamp17\"),\n",
    "    (\"vizier\", \"dbo\", \"Klachten\", \"incremental\", \"upd_dt\", \"stamp17\"),\n",
    "    (\"vizier\", \"dbo\", \"Verkoopkansen\", \"incremental\", \"upd\", \"stamp17\"),\n",
    "    (\"vizier\", \"dbo\", \"Interesses\", \"incremental\", \"UPD_DT\", \"stamp17\"),\n",
    "], schema=load_schema)\n",
    "\n",
    "window_schema = T.StructType([\n",
    "    T.StructField(\"Bron\", T.StringType(), False),\n",
    "    T.StructField(\"schema_name\", T.StringType(), False),\n",
    "    T.StructField(\"obj_name\", T.StringType(), False),\n",
    "    T.StructField(\"partition_column\", T.StringType(), False),\n",
    "    T.StructField(\"granularity\", T.StringType(), False),\n",
    "    T.StructField(\"lookback_months\", T.IntegerType(), False),\n",
    "])\n",
    "\n",
    "df_window_config = spark.createDataFrame([\n",
    "    (\"anva_concern\", \"pbi\", \"Fact_PremieFacturen\", \"Boek_Datum\", \"month\", 12),\n",
    "    (\"geintegreerd_model\", \"pbi\", \"Fact_PremieFacturen\", \"Boek_Datum\", \"month\", 12),\n",
    "    (\"anva_meeus\", \"pbi\", \"Fact_PremieFacturen\", \"Boek_Datum\", \"month\", 12),\n",
    "    (\"ccs_level\", \"pbi\", \"Fact_PremieBoekingen\", \"Boek_Datum\", \"month\", 12),\n",
    "], schema=window_schema)\n",
    "\n",
    "excluded_schema = T.StructType([\n",
    "    T.StructField(\"Bron\", T.StringType(), False),\n",
    "    T.StructField(\"schema_name\", T.StringType(), False),\n",
    "    T.StructField(\"obj_name\", T.StringType(), False),\n",
    "    T.StructField(\"excluded\", T.IntegerType(), False),\n",
    "])\n",
    "\n",
    "df_excluded_tables = spark.createDataFrame([\n",
    "    (\"vizier\", \"dbo\", \"BO_sleutels_Wim_Verheijen\", 1),\n",
    "    (\"vizier\", \"dbo\", \"UMG_Historie\", 1),\n",
    "], schema=excluded_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43eca608",
   "metadata": {},
   "source": [
    "## Metadata inlezen uit Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457d3003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resolve metadata path to correct environment-specific location\n",
    "resolved_metadata_path = resolve_files_path(metadata_path, spark)\n",
    "logger.info(f\"Resolved metadata path: {resolved_metadata_path}\")\n",
    "\n",
    "metadata_df = validate_metadata(load_metadata(spark, resolved_metadata_path))\n",
    "logger.info(f\"Loaded metadata records: {metadata_df.count()}\")\n",
    "\n",
    "source_mapping = df_sources.filter(F.col(\"Bron\") == F.lit(source))\n",
    "if source_mapping.count() == 0:\n",
    "    raise ValueError(f\"Unknown source '{source}' in df_sources\")\n",
    "\n",
    "metadata_filtered = (\n",
    "    metadata_df.alias(\"m\")\n",
    "    .join(\n",
    "        source_mapping.alias(\"s\"),\n",
    "        (F.col(\"m.server_name\") == F.col(\"s.Server\")) & (F.col(\"m.db_name\") == F.col(\"s.Database\")),\n",
    "        \"inner\",\n",
    "    )\n",
    "    .withColumn(\"Bron\", F.col(\"s.Bron\"))\n",
    "    .filter(F.col(\"s.Bron\") == F.lit(source))\n",
    ")\n",
    "\n",
    "logger.info(f\"Filtered metadata records for source '{source}': {metadata_filtered.count()}\")\n",
    "if metadata_filtered.limit(1).count() == 0:\n",
    "    raise ValueError(\"No metadata records found for the specified source\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9aa7fb",
   "metadata": {},
   "source": [
    "## Base query generatie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c667ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_schema = T.StructType([\n",
    "    T.StructField(\"Bron\", T.StringType(), False),\n",
    "    T.StructField(\"schema_name\", T.StringType(), False),\n",
    "    T.StructField(\"obj_name\", T.StringType(), False),\n",
    "    T.StructField(\"base_query\", T.StringType(), False),\n",
    "])\n",
    "\n",
    "col_struct = F.struct(\n",
    "    \"ordinal_position\", \"column_name\", \"data_type\", \"numeric_precision\", \"numeric_scale\", \"max_len\"\n",
    ")\n",
    "\n",
    "base_query_df = (\n",
    "    metadata_filtered\n",
    "    .select(\"Bron\", \"schema_name\", \"obj_name\", col_struct.alias(\"column\"))\n",
    "    .groupBy(\"Bron\", \"schema_name\", \"obj_name\")\n",
    "    .agg(F.collect_list(\"column\").alias(\"columns\"))\n",
    "    .rdd\n",
    "    .map(lambda row: (row.Bron, row.schema_name, row.obj_name, build_base_query(row.schema_name, row.obj_name, row.columns)))\n",
    "    .toDF(schema=base_schema)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b879efc7",
   "metadata": {},
   "source": [
    "## Configuratie samenvoegen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8915c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build tables DataFrame - conditionally join df_load_mode based on toggle\n",
    "tables = base_query_df.alias(\"bq\")\n",
    "\n",
    "# Always join these\n",
    "tables = (\n",
    "    tables\n",
    "    .join(df_disabled_tables.alias(\"dis\"), [\"Bron\", \"schema_name\", \"obj_name\"], \"left\")\n",
    "    .join(df_size_class.alias(\"sz\"), [\"Bron\", \"schema_name\", \"obj_name\"], \"left\")\n",
    ")\n",
    "\n",
    "# Conditionally join load_mode configuration\n",
    "if use_load_mode_config:\n",
    "    logger.info(\"Using load_mode configuration from df_load_mode\")\n",
    "    tables = (\n",
    "        tables\n",
    "        .join(df_load_mode.alias(\"lm\"), [\"Bron\", \"schema_name\", \"obj_name\"], \"left\")\n",
    "        .join(df_window_config.alias(\"wnd\"), [\"Bron\", \"schema_name\", \"obj_name\"], \"left\")\n",
    "    )\n",
    "else:\n",
    "    logger.info(\"Load_mode toggle OFF - all tables will use 'snapshot' mode\")\n",
    "\n",
    "# Always join excluded tables\n",
    "tables = tables.join(df_excluded_tables.alias(\"ex\"), [\"Bron\", \"schema_name\", \"obj_name\"], \"left\")\n",
    "\n",
    "# Apply transformations\n",
    "tables = (\n",
    "    tables\n",
    "    .withColumn(\"enabled\", F.when(F.col(\"dis.obj_name\").isNull(), F.lit(True)).otherwise(F.lit(False)))\n",
    "    .withColumn(\"size_class\", F.when(F.col(\"sz.size_class\").isNull(), F.lit(\"S\")).otherwise(F.col(\"sz.size_class\")))\n",
    "    .withColumn(\"excluded\", F.when(F.col(\"ex.excluded\").isNull(), F.lit(0)).otherwise(F.col(\"ex.excluded\")))\n",
    ")\n",
    "\n",
    "# Conditionally set load_mode\n",
    "if use_load_mode_config:\n",
    "    tables = tables.withColumn(\"load_mode\", \n",
    "        F.when(F.col(\"lm.load_mode\").isNull(), F.lit(\"snapshot\")).otherwise(F.col(\"lm.load_mode\"))\n",
    "    )\n",
    "    # Select with load_mode columns\n",
    "    tables = tables.select(\n",
    "        F.col(\"bq.obj_name\").alias(\"name\"),\n",
    "        \"schema_name\",\n",
    "        \"Bron\",\n",
    "        \"enabled\",\n",
    "        \"size_class\",\n",
    "        \"load_mode\",\n",
    "        F.col(\"bq.base_query\"),\n",
    "        F.col(\"lm.filter_column\"),\n",
    "        F.col(\"lm.kind\"),\n",
    "        F.col(\"wnd.partition_column\"),\n",
    "        F.col(\"wnd.granularity\"),\n",
    "        F.col(\"wnd.lookback_months\"),\n",
    "        \"excluded\",\n",
    "    )\n",
    "else:\n",
    "    # All snapshot - no filter_column or window config\n",
    "    tables = (\n",
    "        tables\n",
    "        .withColumn(\"load_mode\", F.lit(\"snapshot\"))\n",
    "        .select(\n",
    "            F.col(\"bq.obj_name\").alias(\"name\"),\n",
    "            \"schema_name\",\n",
    "            \"Bron\",\n",
    "            \"enabled\",\n",
    "            \"size_class\",\n",
    "            \"load_mode\",\n",
    "            F.col(\"bq.base_query\"),\n",
    "            F.lit(None).cast(T.StringType()).alias(\"filter_column\"),\n",
    "            F.lit(None).cast(T.StringType()).alias(\"kind\"),\n",
    "            F.lit(None).cast(T.StringType()).alias(\"partition_column\"),\n",
    "            F.lit(None).cast(T.StringType()).alias(\"granularity\"),\n",
    "            F.lit(None).cast(T.IntegerType()).alias(\"lookback_months\"),\n",
    "            \"excluded\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Filter and order\n",
    "tables = (\n",
    "    tables\n",
    "    .filter(F.col(\"excluded\") == 0)\n",
    "    .orderBy(\"name\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b8c5eb",
   "metadata": {},
   "source": [
    "## JSON constructie en wegschrijven"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c90df53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def table_record(row: T.Row) -> Dict[str, object]:\n",
    "    record = {\n",
    "        \"name\": row.name,\n",
    "        \"enabled\": bool(row.enabled),\n",
    "        \"size_class\": row.size_class or \"S\",\n",
    "        \"load_mode\": row.load_mode or \"snapshot\",\n",
    "        \"delta_schema\": source,\n",
    "        \"delta_table\": row.name,\n",
    "        \"base_query\": row.base_query,\n",
    "    }\n",
    "    if record[\"load_mode\"] == \"window\":\n",
    "        record[\"window\"] = {\n",
    "            \"partition_column\": row.partition_column,\n",
    "            \"granularity\": row.granularity,\n",
    "            \"lookback_months\": int(row.lookback_months) if row.lookback_months is not None else None,\n",
    "        }\n",
    "    if record[\"load_mode\"] == \"incremental\":\n",
    "        record[\"incremental_column\"] = {\n",
    "            \"column\": row.filter_column,\n",
    "            \"kind\": row.kind,\n",
    "        }\n",
    "    return record\n",
    "\n",
    "\n",
    "defaults = {\n",
    "    \"concurrency_large\": 2,\n",
    "    \"concurrency_small\": 8,\n",
    "    \"max_rows_per_file_large\": 15_000_000,\n",
    "    \"max_rows_per_file_small\": 1_000_000,\n",
    "}\n",
    "\n",
    "result = {\n",
    "    \"source\": source,\n",
    "    \"run_date_utc\": None,\n",
    "    \"watermarks_path\": \"config/watermarks.json\",\n",
    "    \"base_files\": \"greenhouse_sources\",\n",
    "    \"connection_name\": f\"connection_{source}_prod\",\n",
    "    \"defaults\": defaults,\n",
    "    \"tables\": [table_record(row) for row in tables.collect()],\n",
    "}\n",
    "\n",
    "# Fabric-style path (works in both Fabric and cluster via mssparkutils mock)\n",
    "output_path = f\"Files/config/{source}_metadata.json\"\n",
    "json_payload = json.dumps(result, ensure_ascii=False, indent=4)\n",
    "\n",
    "# Use mssparkutils.fs.put (native in Fabric, mock in cluster)\n",
    "mssparkutils.fs.put(output_path, json_payload, True)\n",
    "\n",
    "logger.info(f\"Metadata JSON geschreven naar {output_path}\")\n",
    "logger.info(f\"  Tables: {len(result['tables'])}\")\n",
    "logger.info(f\"  JSON size: {len(json_payload)} bytes\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dwh-spark-processing (3.11.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
