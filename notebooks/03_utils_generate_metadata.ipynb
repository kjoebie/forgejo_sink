{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "faa6f3dd",
   "metadata": {},
   "source": [
    "# PySpark metadata generator\n",
    "\n",
    "Genereer een JSON configuratiebestand voor datapipeline metadata op basis van SQL Server metadata."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9e53c0",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "Stel de Fabric parameters in voor de gewenste bron en het pad naar het parquetbestand met SQL metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "53ae9f80",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "#source = \"anva_concern\"\n",
    "#source = \"anva_meeus\"\n",
    "#source = \"ccs_level\"\n",
    "#source = \"insurance_data_im\"\n",
    "#source = \"ods_reports\"\n",
    "source = \"vizier\"\n",
    "\n",
    "#metadata_path = \"Files/metadata/connection_anva_concern_prod_metadata.parquet\"\n",
    "metadata_path = f\"Files/metadata/connection_{source}_prod_metadata.parquet\"\n",
    "\n",
    "#metadata_path = \"Files/metadata/connection_anva_meeus_prod_metadata.parquet\"\n",
    "#metadata_path = \"Files/metadata/sqlmetadata.parquet\"\n",
    "\n",
    "# Toggle: gebruik load_mode configuratie of alles snapshot\n",
    "use_load_mode_config = True  # True = gebruik df_load_mode, False = alles snapshot\n",
    "\n",
    "log_to_console = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe74b7af",
   "metadata": {},
   "source": [
    "## Imports en helper functies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062ef2a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 08:21:09,029 [INFO] - Logfile: /data/lakehouse/gh_b_avd/lh_gh_bronze/Files/notebook_outputs/logs/path_utils_20251202_211120.log\n"
     ]
    }
   ],
   "source": [
    "from typing import Iterable, Dict\n",
    "from modules.logging_utils import configure_logging\n",
    "from modules.path_utils import resolve_files_path, detect_environment\n",
    "import logging\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "import re  \n",
    "import json \n",
    "\n",
    "\n",
    "log_file = configure_logging(run_name=\"fabric_metadata_generator\", enable_console_logging=log_to_console)\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.info(\"Logfile: %s\", log_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f7013b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 08:21:09,052 [INFO] - âœ“ Using existing Spark session\n",
      "2025-12-03 08:21:09,053 [INFO] -   Spark version: 3.5.5\n",
      "2025-12-03 08:21:09,054 [INFO] -   Application ID: app-20251202211122-0850\n",
      "2025-12-03 08:21:09,054 [INFO] -   Application name: Metadata_Generator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 08:21:09,061 [INFO] - Detected environment: local\n",
      "2025-12-03 08:21:09,062 [INFO] - Using mock mssparkutils for local/cluster environment\n"
     ]
    }
   ],
   "source": [
    "from modules.spark_session import get_or_create_spark_session\n",
    "\n",
    "spark = get_or_create_spark_session(\n",
    "    app_name=\"Metadata_Generator\",\n",
    "    enable_hive=True\n",
    ")\n",
    "\n",
    "# Configure mssparkutils based on environment\n",
    "environment = detect_environment(spark)\n",
    "logger.info(f\"Detected environment: {environment}\")\n",
    "\n",
    "if environment == 'fabric':\n",
    "    # Use Fabric's native mssparkutils\n",
    "    logger.info(\"Using Fabric native mssparkutils\")\n",
    "    try:\n",
    "        from notebookutils import mssparkutils\n",
    "    except ImportError:\n",
    "        logger.warning(\"notebookutils not found, falling back to mock\")\n",
    "        from modules.notebook_utils import get_mssparkutils\n",
    "        mssparkutils = get_mssparkutils(spark)\n",
    "else:\n",
    "    # Use mock for local/cluster environments\n",
    "    logger.info(\"Using mock mssparkutils for local/cluster environment\")\n",
    "    from modules.notebook_utils import get_mssparkutils\n",
    "    mssparkutils = get_mssparkutils(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "99823c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_safe_identifier(name: str) -> str:\n",
    "    \"\"\"Normaliseer kolomnamen voor gebruik in Delta Lake.\"\"\"\n",
    "    if name is None:\n",
    "        return \"\"\n",
    "    cleaned = re.sub(r\"[^0-9A-Za-z_ ]+\", \"\", name)\n",
    "    cleaned = re.sub(r\"\\s+\", \"_\", cleaned.strip())\n",
    "    if cleaned and cleaned[0].isdigit():\n",
    "        cleaned = f\"_{cleaned}\"\n",
    "    return cleaned or name\n",
    "\n",
    "\n",
    "def column_expression(col: T.Row) -> str:\n",
    "    dt = (col.data_type or \"\").lower()\n",
    "    col_ref = f\"[{col.column_name}]\"\n",
    "\n",
    "    if dt in (\"decimal\", \"numeric\"):\n",
    "        precision = col.numeric_precision or 38\n",
    "        scale = col.numeric_scale or 18\n",
    "        expr = f\"CAST({col_ref} AS decimal({precision},{scale}))\"\n",
    "    elif dt == \"money\":\n",
    "        expr = f\"CAST({col_ref} AS decimal(19,4))\"\n",
    "    elif dt == \"smallmoney\":\n",
    "        expr = f\"CAST({col_ref} AS decimal(10,4))\"\n",
    "    elif dt == \"tinyint\":\n",
    "        expr = f\"CAST({col_ref} AS smallint)\"\n",
    "    elif dt in {\"smallint\", \"int\", \"bigint\", \"bit\", \"float\", \"real\"}:\n",
    "        expr = f\"CAST({col_ref} AS {dt})\"\n",
    "    elif dt == \"date\":\n",
    "        expr = f\"CAST({col_ref} AS date)\"\n",
    "    elif dt == \"datetime\":\n",
    "        expr = f\"CAST({col_ref} AS datetime2(3))\"\n",
    "    elif dt == \"smalldatetime\":\n",
    "        expr = f\"CAST({col_ref} AS datetime2(0))\"\n",
    "    elif dt == \"datetime2\":\n",
    "        expr = f\"CAST({col_ref} AS datetime2(6))\"\n",
    "    elif dt == \"time\":\n",
    "        expr = f\"CONVERT(varchar(8), {col_ref}, 108)\"\n",
    "    elif dt == \"datetimeoffset\":\n",
    "        expr = f\"CAST(SWITCHOFFSET({col_ref}, '+00:00') AS datetime2(6))\"\n",
    "    elif dt in {\"char\", \"varchar\", \"nchar\", \"nvarchar\"}:\n",
    "        expr = col_ref\n",
    "    elif dt == \"text\":\n",
    "        expr = f\"CONVERT(varchar(max), {col_ref})\"\n",
    "    elif dt == \"ntext\":\n",
    "        expr = f\"CONVERT(nvarchar(max), {col_ref})\"\n",
    "    elif dt == \"uniqueidentifier\":\n",
    "        expr = f\"CONVERT(varchar(36), {col_ref})\"\n",
    "    elif dt == \"xml\":\n",
    "        expr = f\"CONVERT(nvarchar(max), {col_ref})\"\n",
    "    else:\n",
    "        expr = col_ref\n",
    "\n",
    "    alias = make_safe_identifier(col.column_name)\n",
    "    return f\"{expr} AS [{alias}]\"\n",
    "\n",
    "\n",
    "def build_base_query(schema_name: str, table_name: str, columns):\n",
    "    ordered_cols = sorted(columns, key=lambda r: r.ordinal_position or 0)\n",
    "    select_parts = [column_expression(col) for col in ordered_cols]\n",
    "    select_clause = \",\".join(select_parts)\n",
    "    \n",
    "    return f\"SELECT {select_clause} FROM [{schema_name}].[{table_name}]\"\n",
    "\n",
    "\n",
    "def load_metadata(path: str):\n",
    "    #df = spark.read.parquet(path)\n",
    "    #return df.filter(df[\"obj_name\"].isNotNull())\n",
    "    return spark.read.parquet(path).filter(F.col(\"obj_name\").isNotNull())\n",
    "\n",
    "\n",
    "def validate_metadata(df):\n",
    "    required_cols = [\n",
    "        \"server_name\", \"db_name\", \"obj_name\", \"column_name\"\n",
    "    ]\n",
    "    \n",
    "    # Check missing columns\n",
    "    missing = [c for c in required_cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Required columns are missing: {', '.join(missing)}\")\n",
    "    \n",
    "    # Check for nulls in ONE query instead of 4 separate count() calls\n",
    "    null_checks = [F.sum(F.when(F.col(c).isNull(), 1).otherwise(0)).alias(f\"{c}_nulls\") \n",
    "                   for c in required_cols]\n",
    "    \n",
    "    null_counts = df.select(null_checks).first()\n",
    "    \n",
    "    emptycols = [c for c in required_cols if null_counts[f\"{c}_nulls\"] > 0]\n",
    "    if emptycols:\n",
    "        raise ValueError(f\"Required columns contain empty values: {', '.join(emptycols)}\")\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f51f3e",
   "metadata": {},
   "source": [
    "## Configuratie DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "15df4429",
   "metadata": {},
   "outputs": [],
   "source": [
    "sources_schema = T.StructType([\n",
    "    T.StructField(\"Bron\", T.StringType(), False),\n",
    "    T.StructField(\"Server\", T.StringType(), False),\n",
    "    T.StructField(\"Database\", T.StringType(), False),\n",
    "])\n",
    "\n",
    "df_sources = spark.createDataFrame([\n",
    "    (\"ccs_level\", \"vmdwhidpweu01\", \"InsuranceData_CCS_DWH\"),\n",
    "    (r\"anva_meeus\", r\"vmdwhidpweu01\\MEEUS\", \"InsuranceData_ANVA_DWH\"),\n",
    "    (\"vizier\", \"viz-sql01-mi-p.1d57ac4f4d63.database.windows.net\", \"CRM_DWH\"),\n",
    "    (\"ods_reports\", \"vmdwhodsanvpweu\", \"OG_ODS_Reports\"),\n",
    "    (\"anva_concern\", \"vmdwhidpweu01\", \"InsuranceData_ANVA_DWH\"),\n",
    "    (\"insurance_data_im\", \"vmdwhidpweu01\", \"InsuranceData_OpGroen_DWH\"),\n",
    "], schema=sources_schema)\n",
    "\n",
    "_disabled_schema = T.StructType([\n",
    "    T.StructField(\"Bron\", T.StringType(), False),\n",
    "    T.StructField(\"schema_name\", T.StringType(), False),\n",
    "    T.StructField(\"obj_name\", T.StringType(), False),\n",
    "])\n",
    "\n",
    "df_disabled_tables = spark.createDataFrame([\n",
    "    (\"anva_concern\", \"dbo\", \"Jobmonitor\"),\n",
    "    (\"anva_concern\", \"dbo\", \"LaatsteVerversing\"),\n",
    "    (\"anva_concern\", \"dbo\", \"Metadata\"),\n",
    "    (\"anva_concern\", \"dbo\", \"VrijeLabels\"),\n",
    "    (\"anva_concern\", \"pbi\", \"Nulmeting_Clausules\"),\n",
    "    (\"anva_concern\", \"pbi\", \"Nulmeting_CodesDekking\"),\n",
    "    (\"anva_concern\", \"pbi\", \"Nulmeting_CodesNAW\"),\n",
    "    (\"anva_concern\", \"pbi\", \"Nulmeting_CodesPolis\"),\n",
    "    (\"anva_concern\", \"pbi\", \"Nulmeting_LabelDekking\"),\n",
    "    (\"anva_concern\", \"pbi\", \"Nulmeting_LabelNAW\"),\n",
    "    (\"anva_concern\", \"pbi\", \"Nulmeting_LabelPolis\"),\n",
    "    (\"anva_concern\", \"pbi\", \"Nulmeting_NAWDetails\"),\n",
    "    (\"anva_concern\", \"pbi\", \"Nulmeting_NAWLabels\"),\n",
    "    (\"anva_concern\", \"pbi\", \"Nulmeting_PolisDetails\"),\n",
    "    (\"anva_concern\", \"pbi\", \"Nulmeting_PolisLabels\"),\n",
    "    (\"anva_concern\", \"pbi\", \"Nulmeting_Voorwaarden\"),\n",
    "    (\"anva_meeus\", \"dbo\", \"Jobmonitor\"),\n",
    "    (\"anva_meeus\", \"dbo\", \"LaatsteVerversing\"),\n",
    "    (\"anva_meeus\", \"dbo\", \"Metadata\"),\n",
    "    (\"anva_meeus\", \"dbo\", \"VrijeLabels\"),\n",
    "    (\"anva_meeus\", \"pbi\", \"Nulmeting_Clausules\"),\n",
    "    (\"anva_meeus\", \"pbi\", \"Nulmeting_CodesDekking\"),\n",
    "    (\"anva_meeus\", \"pbi\", \"Nulmeting_CodesNAW\"),\n",
    "    (\"anva_meeus\", \"pbi\", \"Nulmeting_CodesPolis\"),\n",
    "    (\"anva_meeus\", \"pbi\", \"Nulmeting_LabelDekking\"),\n",
    "    (\"anva_meeus\", \"pbi\", \"Nulmeting_LabelNAW\"),\n",
    "    (\"anva_meeus\", \"pbi\", \"Nulmeting_LabelPolis\"),\n",
    "    (\"anva_meeus\", \"pbi\", \"Nulmeting_NAWDetails\"),\n",
    "    (\"anva_meeus\", \"pbi\", \"Nulmeting_NAWLabels\"),\n",
    "    (\"anva_meeus\", \"pbi\", \"Nulmeting_PolisDetails\"),\n",
    "    (\"anva_meeus\", \"pbi\", \"Nulmeting_PolisLabels\"),\n",
    "    (\"anva_meeus\", \"pbi\", \"Nulmeting_Voorwaarden\"),\n",
    "], schema=_disabled_schema)\n",
    "\n",
    "size_schema = T.StructType([\n",
    "    T.StructField(\"Bron\", T.StringType(), False),\n",
    "    T.StructField(\"schema_name\", T.StringType(), False),\n",
    "    T.StructField(\"obj_name\", T.StringType(), False),\n",
    "    T.StructField(\"size_class\", T.StringType(), False),\n",
    "])\n",
    "\n",
    "df_size_class = spark.createDataFrame([\n",
    "    (\"anva_concern\", \"pbi\", \"Fact_PremieFacturen\", \"L\"),\n",
    "    (\"ccs_level\", \"pbi\", \"Fact_PremieBoekingen\", \"L\"),\n",
    "    (\"geintegreerd_model\", \"pbi\", \"Fact_PremieFacturen\", \"L\"),\n",
    "    (\"anva_meeus\", \"pbi\", \"Fact_PremieFacturen\", \"L\"),\n",
    "], schema=size_schema)\n",
    "\n",
    "load_schema = T.StructType([\n",
    "    T.StructField(\"Bron\", T.StringType(), False),\n",
    "    T.StructField(\"schema_name\", T.StringType(), False),\n",
    "    T.StructField(\"obj_name\", T.StringType(), False),\n",
    "    T.StructField(\"load_mode\", T.StringType(), False),\n",
    "    T.StructField(\"filter_column\", T.StringType(), True),\n",
    "    T.StructField(\"kind\", T.StringType(), True),\n",
    "])\n",
    "\n",
    "df_load_mode = spark.createDataFrame([\n",
    "    (\"anva_concern\", \"pbi\", \"Fact_PremieFacturen\", \"window\", \"Boek_Datum\", \"datetime\"),\n",
    "    (\"ccs_level\", \"pbi\", \"Fact_PremieBoekingen\", \"window\", \"Boek_Datum\", \"datetime\"),\n",
    "    (\"geintegreerd_model\", \"pbi\", \"Fact_PremieFacturen\", \"window\", \"Boek_Datum\", \"datetime\"),\n",
    "    (\"anva_meeus\", \"pbi\", \"Fact_PremieFacturen\", \"window\", \"Boek_Datum\", \"datetime\"),\n",
    "    (\"vizier\", \"dbo\", \"Relaties\", \"incremental\", \"Updatedatum\", \"stamp17\"),\n",
    "    (\"vizier\", \"dbo\", \"Contactpersonen\", \"incremental\", \"Upd_dt\", \"stamp17\"),\n",
    "    (\"vizier\", \"dbo\", \"Sleutels\", \"incremental\", \"upd\", \"stamp17\"),\n",
    "    (\"vizier\", \"dbo\", \"Polissen\", \"incremental\", \"upd_dt\", \"stamp17\"),\n",
    "    (\"vizier\", \"dbo\", \"Schades\", \"incremental\", \"upd_dt\", \"stamp17\"),\n",
    "    (\"vizier\", \"dbo\", \"DnB\", \"incremental\", \"upd_dt\", \"stamp17\"),\n",
    "    (\"vizier\", \"dbo\", \"Contactmomenten\", \"incremental\", \"Upd\", \"stamp17\"),\n",
    "    (\"vizier\", \"dbo\", \"Taken\", \"incremental\", \"upd\", \"stamp17\"),\n",
    "    (\"vizier\", \"dbo\", \"Sales\", \"incremental\", \"UPD_DT\", \"stamp17\"),\n",
    "    (\"vizier\", \"dbo\", \"Retenties\", \"incremental\", \"UPD_DT\", \"stamp17\"),\n",
    "    (\"vizier\", \"dbo\", \"Adresbeeld\", \"incremental\", \"UPD_DT\", \"stamp17\"),\n",
    "    (\"vizier\", \"dbo\", \"UBO_Onderzoeken\", \"incremental\", \"UPD_DT\", \"stamp17\"),\n",
    "    (\"vizier\", \"dbo\", \"Producten\", \"incremental\", \"upd\", \"stamp17\"),\n",
    "    (\"vizier\", \"dbo\", \"Medewerkers\", \"incremental\", \"id_upd\", \"stamp17\"),\n",
    "    (\"vizier\", \"dbo\", \"Klachten\", \"incremental\", \"upd_dt\", \"stamp17\"),\n",
    "    (\"vizier\", \"dbo\", \"Verkoopkansen\", \"incremental\", \"upd\", \"stamp17\"),\n",
    "    (\"vizier\", \"dbo\", \"Interesses\", \"incremental\", \"UPD_DT\", \"stamp17\"),\n",
    "], schema=load_schema)\n",
    "\n",
    "window_schema = T.StructType([\n",
    "    T.StructField(\"Bron\", T.StringType(), False),\n",
    "    T.StructField(\"schema_name\", T.StringType(), False),\n",
    "    T.StructField(\"obj_name\", T.StringType(), False),\n",
    "    T.StructField(\"partition_column\", T.StringType(), False),\n",
    "    T.StructField(\"granularity\", T.StringType(), False),\n",
    "    T.StructField(\"lookback_months\", T.IntegerType(), False),\n",
    "])\n",
    "\n",
    "df_window_config = spark.createDataFrame([\n",
    "    (\"anva_concern\", \"pbi\", \"Fact_PremieFacturen\", \"Boek_Datum\", \"month\", 12),\n",
    "    (\"geintegreerd_model\", \"pbi\", \"Fact_PremieFacturen\", \"Boek_Datum\", \"month\", 12),\n",
    "    (\"anva_meeus\", \"pbi\", \"Fact_PremieFacturen\", \"Boek_Datum\", \"month\", 12),\n",
    "    (\"ccs_level\", \"pbi\", \"Fact_PremieBoekingen\", \"Boek_Datum\", \"month\", 12),\n",
    "], schema=window_schema)\n",
    "\n",
    "excluded_schema = T.StructType([\n",
    "    T.StructField(\"Bron\", T.StringType(), False),\n",
    "    T.StructField(\"schema_name\", T.StringType(), False),\n",
    "    T.StructField(\"obj_name\", T.StringType(), False),\n",
    "    T.StructField(\"excluded\", T.IntegerType(), False),\n",
    "])\n",
    "\n",
    "df_excluded_tables = spark.createDataFrame([\n",
    "    (\"vizier\", \"dbo\", \"BO_sleutels_Wim_Verheijen\", 1),\n",
    "    (\"vizier\", \"dbo\", \"UMG_Historie\", 1),\n",
    "], schema=excluded_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43eca608",
   "metadata": {},
   "source": [
    "## Metadata inlezen uit Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "457d3003",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 08:21:10,213 [INFO] - Detected cluster Files path: /data/lakehouse/gh_b_avd/lh_gh_bronze/Files\n",
      "2025-12-03 08:21:10,214 [INFO] - Resolved metadata path: /data/lakehouse/gh_b_avd/lh_gh_bronze/Files/metadata/connection_vizier_prod_metadata.parquet\n",
      "2025-12-03 08:21:10,371 [INFO] - Loaded metadata records: 568\n",
      "2025-12-03 08:21:10,629 [INFO] - Filtered metadata records for source 'vizier': 568\n"
     ]
    }
   ],
   "source": [
    "# Resolve metadata path to correct environment-specific location\n",
    "resolved_metadata_path = resolve_files_path(metadata_path, spark)\n",
    "logger.info(f\"Resolved metadata path: {resolved_metadata_path}\")\n",
    "\n",
    "metadata_df = validate_metadata(load_metadata(resolved_metadata_path))\n",
    "logger.info(f\"Loaded metadata records: {metadata_df.count()}\")\n",
    "\n",
    "source_mapping = df_sources.filter(F.col(\"Bron\") == F.lit(source))\n",
    "if source_mapping.count() == 0:\n",
    "    raise ValueError(f\"Unknown source '{source}' in df_sources\")\n",
    "\n",
    "metadata_filtered = (\n",
    "    metadata_df.alias(\"m\")\n",
    "    .join(\n",
    "        source_mapping.alias(\"s\"),\n",
    "        (F.col(\"m.server_name\") == F.col(\"s.Server\")) & (F.col(\"m.db_name\") == F.col(\"s.Database\")),\n",
    "        \"inner\",\n",
    "    )\n",
    "    .withColumn(\"Bron\", F.col(\"s.Bron\"))\n",
    "    .filter(F.col(\"s.Bron\") == F.lit(source))\n",
    ")\n",
    "\n",
    "logger.info(f\"Filtered metadata records for source '{source}': {metadata_filtered.count()}\")\n",
    "if metadata_filtered.limit(1).count() == 0:\n",
    "    raise ValueError(\"No metadata records found for the specified source\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9aa7fb",
   "metadata": {},
   "source": [
    "## Base query generatie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "35c667ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_schema = T.StructType([\n",
    "    T.StructField(\"Bron\", T.StringType(), False),\n",
    "    T.StructField(\"schema_name\", T.StringType(), False),\n",
    "    T.StructField(\"obj_name\", T.StringType(), False),\n",
    "    T.StructField(\"base_query\", T.StringType(), False),\n",
    "])\n",
    "\n",
    "col_struct = F.struct(\n",
    "    \"ordinal_position\", \"column_name\", \"data_type\", \"numeric_precision\", \"numeric_scale\", \"max_len\"\n",
    ")\n",
    "\n",
    "base_query_df = (\n",
    "    metadata_filtered\n",
    "    .select(\"Bron\", \"schema_name\", \"obj_name\", col_struct.alias(\"column\"))\n",
    "    .groupBy(\"Bron\", \"schema_name\", \"obj_name\")\n",
    "    .agg(F.collect_list(\"column\").alias(\"columns\"))\n",
    "    .rdd\n",
    "    .map(lambda row: (row.Bron, row.schema_name, row.obj_name, build_base_query(row.schema_name, row.obj_name, row.columns)))\n",
    "    .toDF(schema=base_schema)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b879efc7",
   "metadata": {},
   "source": [
    "## Configuratie samenvoegen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "b8915c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 08:21:10,945 [INFO] - Using load_mode configuration from df_load_mode\n"
     ]
    }
   ],
   "source": [
    "# Build tables DataFrame - conditionally join df_load_mode based on toggle\n",
    "tables = base_query_df.alias(\"bq\")\n",
    "\n",
    "# Always join these\n",
    "tables = (\n",
    "    tables\n",
    "    .join(df_disabled_tables.alias(\"dis\"), [\"Bron\", \"schema_name\", \"obj_name\"], \"left\")\n",
    "    .join(df_size_class.alias(\"sz\"), [\"Bron\", \"schema_name\", \"obj_name\"], \"left\")\n",
    ")\n",
    "\n",
    "# Conditionally join load_mode configuration\n",
    "if use_load_mode_config:\n",
    "    logger.info(\"Using load_mode configuration from df_load_mode\")\n",
    "    tables = (\n",
    "        tables\n",
    "        .join(df_load_mode.alias(\"lm\"), [\"Bron\", \"schema_name\", \"obj_name\"], \"left\")\n",
    "        .join(df_window_config.alias(\"wnd\"), [\"Bron\", \"schema_name\", \"obj_name\"], \"left\")\n",
    "    )\n",
    "else:\n",
    "    logger.info(\"Load_mode toggle OFF - all tables will use 'snapshot' mode\")\n",
    "\n",
    "# Always join excluded tables\n",
    "tables = tables.join(df_excluded_tables.alias(\"ex\"), [\"Bron\", \"schema_name\", \"obj_name\"], \"left\")\n",
    "\n",
    "# Apply transformations\n",
    "tables = (\n",
    "    tables\n",
    "    .withColumn(\"enabled\", F.when(F.col(\"dis.obj_name\").isNull(), F.lit(True)).otherwise(F.lit(False)))\n",
    "    .withColumn(\"size_class\", F.when(F.col(\"sz.size_class\").isNull(), F.lit(\"S\")).otherwise(F.col(\"sz.size_class\")))\n",
    "    .withColumn(\"excluded\", F.when(F.col(\"ex.excluded\").isNull(), F.lit(0)).otherwise(F.col(\"ex.excluded\")))\n",
    ")\n",
    "\n",
    "# Conditionally set load_mode\n",
    "if use_load_mode_config:\n",
    "    tables = tables.withColumn(\"load_mode\", \n",
    "        F.when(F.col(\"lm.load_mode\").isNull(), F.lit(\"snapshot\")).otherwise(F.col(\"lm.load_mode\"))\n",
    "    )\n",
    "    # Select with load_mode columns\n",
    "    tables = tables.select(\n",
    "        F.col(\"bq.obj_name\").alias(\"name\"),\n",
    "        \"schema_name\",\n",
    "        \"Bron\",\n",
    "        \"enabled\",\n",
    "        \"size_class\",\n",
    "        \"load_mode\",\n",
    "        F.col(\"bq.base_query\"),\n",
    "        F.col(\"lm.filter_column\"),\n",
    "        F.col(\"lm.kind\"),\n",
    "        F.col(\"wnd.partition_column\"),\n",
    "        F.col(\"wnd.granularity\"),\n",
    "        F.col(\"wnd.lookback_months\"),\n",
    "        \"excluded\",\n",
    "    )\n",
    "else:\n",
    "    # All snapshot - no filter_column or window config\n",
    "    tables = (\n",
    "        tables\n",
    "        .withColumn(\"load_mode\", F.lit(\"snapshot\"))\n",
    "        .select(\n",
    "            F.col(\"bq.obj_name\").alias(\"name\"),\n",
    "            \"schema_name\",\n",
    "            \"Bron\",\n",
    "            \"enabled\",\n",
    "            \"size_class\",\n",
    "            \"load_mode\",\n",
    "            F.col(\"bq.base_query\"),\n",
    "            F.lit(None).cast(T.StringType()).alias(\"filter_column\"),\n",
    "            F.lit(None).cast(T.StringType()).alias(\"kind\"),\n",
    "            F.lit(None).cast(T.StringType()).alias(\"partition_column\"),\n",
    "            F.lit(None).cast(T.StringType()).alias(\"granularity\"),\n",
    "            F.lit(None).cast(T.IntegerType()).alias(\"lookback_months\"),\n",
    "            \"excluded\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Filter and order\n",
    "tables = (\n",
    "    tables\n",
    "    .filter(F.col(\"excluded\") == 0)\n",
    "    .orderBy(\"name\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b8c5eb",
   "metadata": {},
   "source": [
    "## JSON constructie en wegschrijven"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "0c90df53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 08:21:12,225 [INFO] - Detected cluster Files path: /data/lakehouse/gh_b_avd/lh_gh_bronze/Files\n",
      "2025-12-03 08:21:12,247 [INFO] - Metadata JSON geschreven naar Files/config/vizier_metadata.json\n",
      "2025-12-03 08:21:12,247 [INFO] -   Tables: 19\n",
      "2025-12-03 08:21:12,248 [INFO] -   JSON size: 28339 bytes\n"
     ]
    }
   ],
   "source": [
    "def table_record(row: T.Row) -> Dict[str, object]:\n",
    "    record = {\n",
    "        \"name\": row.name,\n",
    "        \"enabled\": bool(row.enabled),\n",
    "        \"size_class\": row.size_class or \"S\",\n",
    "        \"load_mode\": row.load_mode or \"snapshot\",\n",
    "        \"delta_schema\": source,\n",
    "        \"delta_table\": row.name,\n",
    "        \"base_query\": row.base_query,\n",
    "    }\n",
    "    if record[\"load_mode\"] == \"window\":\n",
    "        record[\"window\"] = {\n",
    "            \"partition_column\": row.partition_column,\n",
    "            \"granularity\": row.granularity,\n",
    "            \"lookback_months\": int(row.lookback_months) if row.lookback_months is not None else None,\n",
    "        }\n",
    "    if record[\"load_mode\"] == \"incremental\":\n",
    "        record[\"incremental_column\"] = {\n",
    "            \"column\": row.filter_column,\n",
    "            \"kind\": row.kind,\n",
    "        }\n",
    "    return record\n",
    "\n",
    "\n",
    "defaults = {\n",
    "    \"concurrency_large\": 2,\n",
    "    \"concurrency_small\": 8,\n",
    "    \"max_rows_per_file_large\": 15_000_000,\n",
    "    \"max_rows_per_file_small\": 1_000_000,\n",
    "}\n",
    "\n",
    "result = {\n",
    "    \"source\": source,\n",
    "    \"run_date_utc\": None,\n",
    "    \"watermarks_path\": \"config/watermarks.json\",\n",
    "    \"base_files\": \"greenhouse_sources\",\n",
    "    \"connection_name\": f\"connection_{source}_prod\",\n",
    "    \"defaults\": defaults,\n",
    "    \"tables\": [table_record(row) for row in tables.collect()],\n",
    "}\n",
    "\n",
    "# Fabric-style path (works in both Fabric and cluster via mssparkutils mock)\n",
    "output_path = f\"Files/config/{source}_metadata.json\"\n",
    "json_payload = json.dumps(result, ensure_ascii=False, indent=4)\n",
    "\n",
    "# Use mssparkutils.fs.put (native in Fabric, mock in cluster)\n",
    "mssparkutils.fs.put(output_path, json_payload, True)\n",
    "\n",
    "logger.info(f\"Metadata JSON geschreven naar {output_path}\")\n",
    "logger.info(f\"  Tables: {len(result['tables'])}\")\n",
    "logger.info(f\"  JSON size: {len(json_payload)} bytes\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dwh-spark-processing (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
