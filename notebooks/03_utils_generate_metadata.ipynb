{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "faa6f3dd",
   "metadata": {},
   "source": [
    "# PySpark metadata generator\n",
    "\n",
    "Genereer een JSON configuratiebestand voor datapipeline metadata op basis van SQL Server metadata."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9e53c0",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "Stel de Fabric parameters in voor de gewenste bron en het pad naar het parquetbestand met SQL metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53ae9f80",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "source = \"anva_concern\"\n",
    "#source = \"anva_meeus\"\n",
    "#source = \"ccs_level\"\n",
    "#source = \"insurance_data_im\"\n",
    "#source = \"ods_reports\"\n",
    "#source = \"vizier\"\n",
    "\n",
    "#metadata_path = \"Files/metadata/connection_anva_concern_prod_metadata.parquet\"\n",
    "metadata_path = f\"Files/metadata/connection_{source}_prod_metadata.parquet\"\n",
    "\n",
    "#metadata_path = \"Files/metadata/connection_anva_meeus_prod_metadata.parquet\"\n",
    "#metadata_path = \"Files/metadata/sqlmetadata.parquet\"\n",
    "\n",
    "# Toggle: gebruik load_mode configuratie of alles snapshot\n",
    "use_load_mode_config = False  # True = gebruik df_load_mode, False = alles snapshot\n",
    "\n",
    "log_to_console = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29dadd3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Module fabric.bootstrap\n",
    "# ---------------------\n",
    "# This cell enables a flexible module loading strategy:\n",
    "#\n",
    "# PRODUCTION (default): The `Files/code` directory is empty. This function does nothing,\n",
    "# and Python imports all modules from the stable, versioned Wheel in the Environment.\n",
    "#\n",
    "# DEVELOPMENT / HOTFIX: To bypass the 15-20 minute Fabric publish cycle for urgent fixes,\n",
    "# upload individual .py files to `Files/code` in the Lakehouse. This function prepends\n",
    "# that path to sys.path, so Python finds the override files first. All other modules\n",
    "# continue to load from the Wheel - only the uploaded files are replaced.\n",
    "#\n",
    "# Usage: Keep `Files/code` empty for production stability. Use it only for rapid\n",
    "# iteration during development or emergency hotfixes.\n",
    "\n",
    "from modules.fabric_bootstrap import ensure_module_path\n",
    "ensure_module_path()  # Now Python can find the rest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe74b7af",
   "metadata": {},
   "source": [
    "## Imports en helper functies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "062ef2a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 09:12:50,381 [INFO] - Logfile: /data/lakehouse/gh_b_avd/lh_gh_bronze/Files/notebook_outputs/logs/fabric_metadata_generator_20251209_091250.log\n"
     ]
    }
   ],
   "source": [
    "from typing import Iterable, Dict\n",
    "from modules.logging_utils import configure_logging\n",
    "from modules.path_utils import resolve_files_path, detect_environment\n",
    "import logging\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "import re  \n",
    "import json \n",
    "\n",
    "log_file = configure_logging(run_name=\"fabric_metadata_generator\", enable_console_logging=log_to_console)\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.info(\"Logfile: %s\", log_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7013b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 09:12:50,387 [INFO] - Creating new Spark session for local/cluster environment...\n",
      "2025-12-09 09:12:50,389 [INFO] -   - Hive support enabled\n",
      "2025-12-09 09:12:50,390 [INFO] -   - Detection: Local/Cluster environment detected\n",
      "2025-12-09 09:12:50,391 [INFO] -   - Python VENV gevonden: /home/sparkadmin/source/repos/dwh_spark_processing/.venv/bin/python\n",
      "2025-12-09 09:12:50,400 [INFO] -   - Modules gezipt voor distributie: /tmp/dwh_modules_package.zip\n",
      "25/12/09 09:12:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/12/09 09:12:51 WARN StandaloneSchedulerBackend: Dynamic allocation enabled without spark.executor.cores explicitly set, you may get more executors allocated than expected. It's recommended to set spark.executor.cores explicitly. Please check SPARK-30299 for more details.\n",
      "2025-12-09 09:12:52,700 [INFO] -   Spark version: 3.5.5\n",
      "2025-12-09 09:12:52,702 [INFO] -   Application ID: app-20251209091251-0009\n",
      "2025-12-09 09:12:52,704 [INFO] -   Application name: Metadata_Generator\n",
      "2025-12-09 09:12:52,883 [INFO] - Detected environment: local\n",
      "2025-12-09 09:12:52,884 [INFO] - Using mock mssparkutils for local/cluster environment\n"
     ]
    }
   ],
   "source": [
    "from modules.spark_session import get_or_create_spark_session\n",
    "\n",
    "spark = get_or_create_spark_session(\n",
    "    app_name=\"Metadata_Generator\",\n",
    "    enable_hive=True\n",
    ")\n",
    "\n",
    "# Configure mssparkutils based on environment\n",
    "environment = detect_environment(spark)\n",
    "logger.info(f\"Detected environment: {environment}\")\n",
    "\n",
    "if environment == 'fabric':\n",
    "    # Use Fabric's native mssparkutils\n",
    "    logger.info(\"Using Fabric native mssparkutils\")\n",
    "    try:\n",
    "        from notebookutils import mssparkutils\n",
    "    except ImportError:\n",
    "        logger.warning(\"notebookutils not found, falling back to mock\")\n",
    "        from modules.notebook_utils import get_mssparkutils\n",
    "        mssparkutils = get_mssparkutils(spark)\n",
    "else:\n",
    "    # Use mock for local/cluster environments\n",
    "    logger.info(\"Using mock mssparkutils for local/cluster environment\")\n",
    "    from modules.notebook_utils import get_mssparkutils\n",
    "    mssparkutils = get_mssparkutils(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99823c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 09:12:53,754 [INFO] - ✓ Metadata generation functions imported from modules.metadata_utils\n"
     ]
    }
   ],
   "source": [
    "# Import metadata generation utilities from module\n",
    "from modules.metadata_utils import (\n",
    "    make_safe_identifier,\n",
    "    column_expression,\n",
    "    build_base_query,\n",
    "    load_metadata,\n",
    "    validate_metadata\n",
    ")\n",
    "\n",
    "logger.info(\"Metadata generation functions imported from modules.metadata_utils\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f51f3e",
   "metadata": {},
   "source": [
    "## Configuratie DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15df4429",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 09:12:55,101 [INFO] - Loading config from: sources.json\n",
      "2025-12-09 09:12:56,412 [INFO] - Loading config from: disabled_tables.json\n",
      "2025-12-09 09:12:57,272 [INFO] - Loading config from: size_class.json\n",
      "2025-12-09 09:12:58,164 [INFO] - Loading config from: load_mode.json\n",
      "2025-12-09 09:12:59,033 [INFO] - Loading config from: window_config.json\n",
      "2025-12-09 09:12:59,960 [INFO] - Loading config from: excluded_tables.json\n",
      "2025-12-09 09:12:59,974 [INFO] - ✓ All configuration files loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# --- CONFIGURATIE SCHEMA DEFINITIES ---\n",
    "sources_schema = T.StructType([\n",
    "    T.StructField(\"Bron\", T.StringType(), False),\n",
    "    T.StructField(\"Server\", T.StringType(), False),\n",
    "    T.StructField(\"Database\", T.StringType(), False),\n",
    "])\n",
    "\n",
    "_disabled_schema = T.StructType([\n",
    "    T.StructField(\"Bron\", T.StringType(), False),\n",
    "    T.StructField(\"schema_name\", T.StringType(), False),\n",
    "    T.StructField(\"obj_name\", T.StringType(), False),\n",
    "])\n",
    "\n",
    "size_schema = T.StructType([\n",
    "    T.StructField(\"Bron\", T.StringType(), False),\n",
    "    T.StructField(\"schema_name\", T.StringType(), False),\n",
    "    T.StructField(\"obj_name\", T.StringType(), False),\n",
    "    T.StructField(\"size_class\", T.StringType(), False),\n",
    "])\n",
    "\n",
    "load_schema = T.StructType([\n",
    "    T.StructField(\"Bron\", T.StringType(), False),\n",
    "    T.StructField(\"schema_name\", T.StringType(), False),\n",
    "    T.StructField(\"obj_name\", T.StringType(), False),\n",
    "    T.StructField(\"load_mode\", T.StringType(), False),\n",
    "    T.StructField(\"filter_column\", T.StringType(), True),\n",
    "    T.StructField(\"kind\", T.StringType(), True),\n",
    "])\n",
    "\n",
    "window_schema = T.StructType([\n",
    "    T.StructField(\"Bron\", T.StringType(), False),\n",
    "    T.StructField(\"schema_name\", T.StringType(), False),\n",
    "    T.StructField(\"obj_name\", T.StringType(), False),\n",
    "    T.StructField(\"partition_column\", T.StringType(), False),\n",
    "    T.StructField(\"granularity\", T.StringType(), False),\n",
    "    T.StructField(\"lookback_months\", T.IntegerType(), False),\n",
    "])\n",
    "\n",
    "excluded_schema = T.StructType([\n",
    "    T.StructField(\"Bron\", T.StringType(), False),\n",
    "    T.StructField(\"schema_name\", T.StringType(), False),\n",
    "    T.StructField(\"obj_name\", T.StringType(), False),\n",
    "    T.StructField(\"excluded\", T.IntegerType(), False),\n",
    "])\n",
    "\n",
    "# --- JSON LOADER FUNCTIE ---\n",
    "def load_config_table(filename: str, schema: T.StructType):\n",
    "    \"\"\"Laadt een JSON configuratiebestand of geeft een lege DataFrame terug bij fouten.\"\"\"\n",
    "    try:\n",
    "        # Pad oplossen (werkt voor Fabric en Lokaal)\n",
    "        file_path = resolve_files_path(f\"Files/config/definitions/{filename}\", spark)\n",
    "        \n",
    "        # Lezen met multiline optie voor leesbaarheid van JSON\n",
    "        logger.info(f\"Loading config from: {filename}\")\n",
    "        return spark.read.schema(schema).option(\"multiline\", \"true\").option(\"mode\", \"PERMISSIVE\").json(file_path)\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Unable to load {filename} (or file does not exist): {e}\")\n",
    "        return spark.createDataFrame([], schema)\n",
    "\n",
    "# --- CONFIGURATIE INLEZEN ---\n",
    "df_sources = load_config_table(\"sources.json\", sources_schema)\n",
    "df_disabled_tables = load_config_table(\"disabled_tables.json\", _disabled_schema)\n",
    "df_size_class = load_config_table(\"size_class.json\", size_schema)\n",
    "df_load_mode = load_config_table(\"load_mode.json\", load_schema)\n",
    "df_window_config = load_config_table(\"window_config.json\", window_schema)\n",
    "df_excluded_tables = load_config_table(\"excluded_tables.json\", excluded_schema)\n",
    "\n",
    "logger.info(\"✓ All configuration files loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43eca608",
   "metadata": {},
   "source": [
    "## Metadata inlezen uit Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "457d3003",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 09:13:00,810 [INFO] - Resolved metadata path: /data/lakehouse/gh_b_avd/lh_gh_bronze/Files/metadata/connection_anva_concern_prod_metadata.parquet\n",
      "2025-12-09 09:13:03,208 [INFO] - Loaded metadata records: 1557                  \n",
      "2025-12-09 09:13:03,998 [INFO] - Filtered metadata records for source 'anva_concern': 1557\n"
     ]
    }
   ],
   "source": [
    "# Resolve metadata path to correct environment-specific location\n",
    "resolved_metadata_path = resolve_files_path(metadata_path, spark)\n",
    "logger.info(f\"Resolved metadata path: {resolved_metadata_path}\")\n",
    "\n",
    "metadata_df = validate_metadata(load_metadata(spark, resolved_metadata_path))\n",
    "logger.info(f\"Loaded metadata records: {metadata_df.count()}\")\n",
    "\n",
    "source_mapping = df_sources.filter(F.col(\"Bron\") == F.lit(source))\n",
    "if source_mapping.count() == 0:\n",
    "    raise ValueError(f\"Unknown source '{source}' in df_sources\")\n",
    "\n",
    "metadata_filtered = (\n",
    "    metadata_df.alias(\"m\")\n",
    "    .join(\n",
    "        source_mapping.alias(\"s\"),\n",
    "        (F.col(\"m.server_name\") == F.col(\"s.Server\")) & (F.col(\"m.db_name\") == F.col(\"s.Database\")),\n",
    "        \"inner\",\n",
    "    )\n",
    "    .withColumn(\"Bron\", F.col(\"s.Bron\"))\n",
    "    .filter(F.col(\"s.Bron\") == F.lit(source))\n",
    ")\n",
    "\n",
    "logger.info(f\"Filtered metadata records for source '{source}': {metadata_filtered.count()}\")\n",
    "if metadata_filtered.limit(1).count() == 0:\n",
    "    raise ValueError(\"No metadata records found for the specified source\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9aa7fb",
   "metadata": {},
   "source": [
    "## Base query generatie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35c667ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_schema = T.StructType([\n",
    "    T.StructField(\"Bron\", T.StringType(), False),\n",
    "    T.StructField(\"schema_name\", T.StringType(), False),\n",
    "    T.StructField(\"obj_name\", T.StringType(), False),\n",
    "    T.StructField(\"base_query\", T.StringType(), False),\n",
    "])\n",
    "\n",
    "col_struct = F.struct(\n",
    "    \"ordinal_position\", \"column_name\", \"data_type\", \"numeric_precision\", \"numeric_scale\", \"max_len\"\n",
    ")\n",
    "\n",
    "base_query_df = (\n",
    "    metadata_filtered\n",
    "    .select(\"Bron\", \"schema_name\", \"obj_name\", col_struct.alias(\"column\"))\n",
    "    .groupBy(\"Bron\", \"schema_name\", \"obj_name\")\n",
    "    .agg(F.collect_list(\"column\").alias(\"columns\"))\n",
    "    .rdd\n",
    "    .map(lambda row: (row.Bron, row.schema_name, row.obj_name, build_base_query(row.schema_name, row.obj_name, row.columns)))\n",
    "    .toDF(schema=base_schema)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b879efc7",
   "metadata": {},
   "source": [
    "## Configuratie samenvoegen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8915c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 09:13:04,839 [INFO] - Load_mode toggle OFF - all tables will use 'snapshot' mode\n"
     ]
    }
   ],
   "source": [
    "# Helper functie voor case-insensitive joins\n",
    "def normalize_keys(df):\n",
    "    \"\"\"Normaliseert de Bron, schema_name en obj_name kolommen.\"\"\"\n",
    "    return df.withColumn(\"join_key_bronze\", F.lower(F.trim(F.col(\"Bron\")))) \\\n",
    "             .withColumn(\"join_key_schema\", F.lower(F.trim(F.col(\"schema_name\")))) \\\n",
    "             .withColumn(\"join_key_obj\", F.lower(F.trim(F.col(\"obj_name\"))))\n",
    "\n",
    "# 1. Bereid de basis tabel voor (metadata)\n",
    "# We gebruiken 'bq' als alias zodat we later expliciet deze kolommen kunnen kiezen\n",
    "tables = normalize_keys(base_query_df).alias(\"bq\")\n",
    "\n",
    "# 2. Bereid de altijd-nodige config tabellen voor\n",
    "df_dis_norm = normalize_keys(df_disabled_tables).alias(\"dis\")\n",
    "df_sz_norm = normalize_keys(df_size_class).alias(\"sz\")\n",
    "df_ex_norm = normalize_keys(df_excluded_tables).alias(\"ex\")\n",
    "\n",
    "# Definieer de sleutels waarop we joinen\n",
    "join_keys = [\"join_key_bronze\", \"join_key_schema\", \"join_key_obj\"]\n",
    "\n",
    "# 3. Voer de standaard joins uit\n",
    "tables = (\n",
    "    tables\n",
    "    .join(df_dis_norm, join_keys, \"left\")\n",
    "    .join(df_sz_norm, join_keys, \"left\")\n",
    "    .join(df_ex_norm, join_keys, \"left\")\n",
    ")\n",
    "\n",
    "# 4. Conditionele joins voor Load Mode configuratie\n",
    "if use_load_mode_config:\n",
    "    logger.info(\"Using load_mode configuration from df_load_mode\")\n",
    "    df_lm_norm = normalize_keys(df_load_mode).alias(\"lm\")\n",
    "    df_wnd_norm = normalize_keys(df_window_config).alias(\"wnd\")\n",
    "    \n",
    "    tables = (\n",
    "        tables\n",
    "        .join(df_lm_norm, join_keys, \"left\")\n",
    "        .join(df_wnd_norm, join_keys, \"left\")\n",
    "    )\n",
    "else:\n",
    "    logger.info(\"Load_mode toggle OFF - all tables will use 'snapshot' mode\")\n",
    "\n",
    "# 5. Pas logica toe (Enabled / Size / Excluded) en ruim op\n",
    "tables = (\n",
    "    tables\n",
    "    .drop(*join_keys)  # Verwijder de hulp-kolommen\n",
    "    .withColumn(\"enabled\", F.when(F.col(\"dis.obj_name\").isNull(), F.lit(True)).otherwise(F.lit(False)))\n",
    "    .withColumn(\"size_class\", F.when(F.col(\"sz.size_class\").isNull(), F.lit(\"S\")).otherwise(F.col(\"sz.size_class\")))\n",
    "    .withColumn(\"excluded\", F.when(F.col(\"ex.excluded\").isNull(), F.lit(0)).otherwise(F.col(\"ex.excluded\")))\n",
    ")\n",
    "\n",
    "# 6. Finale Selectie en Load Mode logica\n",
    "if use_load_mode_config:\n",
    "    # Bepaal load_mode uit config (fallback naar snapshot)\n",
    "    tables = tables.withColumn(\"load_mode\", \n",
    "        F.when(F.col(\"lm.load_mode\").isNull(), F.lit(\"snapshot\")).otherwise(F.col(\"lm.load_mode\"))\n",
    "    )\n",
    "    \n",
    "    # Selecteer de specifieke kolommen uit de config-joins\n",
    "    tables = tables.select(\n",
    "        F.col(\"bq.obj_name\").alias(\"name\"),\n",
    "        F.col(\"bq.schema_name\"),  # Gebruik expliciet bq.* om ambiguïteit te voorkomen\n",
    "        F.col(\"bq.Bron\"),\n",
    "        \"enabled\",\n",
    "        \"size_class\",\n",
    "        \"load_mode\",\n",
    "        F.col(\"bq.base_query\"),\n",
    "        F.col(\"lm.filter_column\"),\n",
    "        F.col(\"lm.kind\"),\n",
    "        F.col(\"wnd.partition_column\"),\n",
    "        F.col(\"wnd.granularity\"),\n",
    "        F.col(\"wnd.lookback_months\"),\n",
    "        \"excluded\",\n",
    "    )\n",
    "else:\n",
    "    # Forceer snapshot en zet config kolommen op NULL\n",
    "    tables = (\n",
    "        tables\n",
    "        .withColumn(\"load_mode\", F.lit(\"snapshot\"))\n",
    "        .select(\n",
    "            F.col(\"bq.obj_name\").alias(\"name\"),\n",
    "            F.col(\"bq.schema_name\"),\n",
    "            F.col(\"bq.Bron\"),\n",
    "            \"enabled\",\n",
    "            \"size_class\",\n",
    "            \"load_mode\",\n",
    "            F.col(\"bq.base_query\"),\n",
    "            F.lit(None).cast(T.StringType()).alias(\"filter_column\"),\n",
    "            F.lit(None).cast(T.StringType()).alias(\"kind\"),\n",
    "            F.lit(None).cast(T.StringType()).alias(\"partition_column\"),\n",
    "            F.lit(None).cast(T.StringType()).alias(\"granularity\"),\n",
    "            F.lit(None).cast(T.IntegerType()).alias(\"lookback_months\"),\n",
    "            \"excluded\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "# 7. Filter en sorteer\n",
    "tables = (\n",
    "    tables\n",
    "    .filter(F.col(\"excluded\") == 0)\n",
    "    .orderBy(\"name\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b8c5eb",
   "metadata": {},
   "source": [
    "## JSON constructie en wegschrijven"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c90df53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 09:13:06,833 [INFO] - Metadata JSON geschreven naar Files/config/anva_concern_metadata.json\n",
      "2025-12-09 09:13:06,835 [INFO] -   Tables: 84\n",
      "2025-12-09 09:13:06,836 [INFO] -   JSON size: 99879 bytes\n"
     ]
    }
   ],
   "source": [
    "def table_record(row: T.Row) -> Dict[str, object]:\n",
    "    record = {\n",
    "        \"name\": row.name,\n",
    "        \"enabled\": bool(row.enabled),\n",
    "        \"size_class\": row.size_class or \"S\",\n",
    "        \"load_mode\": row.load_mode or \"snapshot\",\n",
    "        \"delta_schema\": source,\n",
    "        \"delta_table\": row.name,\n",
    "        \"base_query\": row.base_query,\n",
    "    }\n",
    "    if record[\"load_mode\"] == \"window\":\n",
    "        record[\"window\"] = {\n",
    "            \"partition_column\": row.partition_column,\n",
    "            \"granularity\": row.granularity,\n",
    "            \"lookback_months\": int(row.lookback_months) if row.lookback_months is not None else None,\n",
    "        }\n",
    "    if record[\"load_mode\"] == \"incremental\":\n",
    "        record[\"incremental_column\"] = {\n",
    "            \"column\": row.filter_column,\n",
    "            \"kind\": row.kind,\n",
    "        }\n",
    "    return record\n",
    "\n",
    "\n",
    "defaults = {\n",
    "    \"concurrency_large\": 2,\n",
    "    \"concurrency_small\": 8,\n",
    "    \"max_rows_per_file_large\": 15_000_000,\n",
    "    \"max_rows_per_file_small\": 1_000_000,\n",
    "}\n",
    "\n",
    "result = {\n",
    "    \"source\": source,\n",
    "    \"run_date_utc\": None,\n",
    "    \"watermarks_path\": \"config/watermarks.json\",\n",
    "    \"base_files\": \"greenhouse_sources\",\n",
    "    \"connection_name\": f\"connection_{source}_prod\",\n",
    "    \"defaults\": defaults,\n",
    "    \"tables\": [table_record(row) for row in tables.collect()],\n",
    "}\n",
    "\n",
    "# Fabric-style path (works in both Fabric and cluster via mssparkutils mock)\n",
    "output_path = f\"Files/config/{source}_metadata.json\"\n",
    "json_payload = json.dumps(result, ensure_ascii=False, indent=4)\n",
    "\n",
    "# Use mssparkutils.fs.put (native in Fabric, mock in cluster)\n",
    "mssparkutils.fs.put(output_path, json_payload, True)\n",
    "\n",
    "logger.info(f\"Metadata JSON geschreven naar {output_path}\")\n",
    "logger.info(f\"  Tables: {len(result['tables'])}\")\n",
    "logger.info(f\"  JSON size: {len(json_payload)} bytes\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dwh-spark-processing (3.11.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
