{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "faa6f3dd",
   "metadata": {},
   "source": [
    "# Fabric PySpark metadata generator\n",
    "\n",
    "Genereer een JSON configuratiebestand voor datapipeline metadata op basis van SQL Server metadata."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9e53c0",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "Stel de Fabric parameters in voor de gewenste bron en het pad naar het parquetbestand met SQL metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ae9f80",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "source = \"anva_meeus\"\n",
    "metadata_path = \"Files/metadata/metadata_anva_meeus.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe74b7af",
   "metadata": {},
   "source": [
    "## Imports en helper functies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062ef2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import importlib\n",
    "from typing import Iterable, Dict\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "if importlib.util.find_spec(\"notebookutils\"):\n",
    "    from notebookutils import mssparkutils\n",
    "else:\n",
    "    from pyspark.dbutils import DBUtils\n",
    "    dbutils = DBUtils(spark)\n",
    "    mssparkutils = dbutils.notebookutils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99823c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_safe_identifier(name: str) -> str:\n",
    "    \"\"\"Normaliseer kolomnamen voor gebruik in Delta Lake.\"\"\"\n",
    "    if name is None:\n",
    "        return \"\"\n",
    "    cleaned = re.sub(r\"[^0-9A-Za-z_ ]+\", \"\", name)\n",
    "    cleaned = re.sub(r\"\\s+\", \"_\", cleaned.strip())\n",
    "    if cleaned and cleaned[0].isdigit():\n",
    "        cleaned = f\"_{cleaned}\"\n",
    "    return cleaned or name\n",
    "\n",
    "\n",
    "def column_expression(col: T.Row) -> str:\n",
    "    dt = (col.data_type or \"\").lower()\n",
    "    col_ref = f\"[{col.column_name}]\"\n",
    "\n",
    "    if dt in (\"decimal\", \"numeric\"):\n",
    "        precision = col.numeric_precision or 38\n",
    "        scale = col.numeric_scale or 18\n",
    "        expr = f\"CAST({col_ref} AS decimal({precision},{scale}))\"\n",
    "    elif dt == \"money\":\n",
    "        expr = f\"CAST({col_ref} AS decimal(19,4))\"\n",
    "    elif dt == \"smallmoney\":\n",
    "        expr = f\"CAST({col_ref} AS decimal(10,4))\"\n",
    "    elif dt == \"tinyint\":\n",
    "        expr = f\"CAST({col_ref} AS smallint)\"\n",
    "    elif dt in {\"smallint\", \"int\", \"bigint\", \"bit\", \"float\", \"real\"}:\n",
    "        expr = f\"CAST({col_ref} AS {dt})\"\n",
    "    elif dt == \"date\":\n",
    "        expr = f\"CAST({col_ref} AS date)\"\n",
    "    elif dt == \"datetime\":\n",
    "        expr = f\"CAST({col_ref} AS datetime2(3))\"\n",
    "    elif dt == \"smalldatetime\":\n",
    "        expr = f\"CAST({col_ref} AS datetime2(0))\"\n",
    "    elif dt == \"datetime2\":\n",
    "        expr = f\"CAST({col_ref} AS datetime2(6))\"\n",
    "    elif dt == \"time\":\n",
    "        expr = f\"CONVERT(varchar(8), {col_ref}, 108)\"\n",
    "    elif dt == \"datetimeoffset\":\n",
    "        expr = f\"CAST(SWITCHOFFSET({col_ref}, '+00:00') AS datetime2(6))\"\n",
    "    elif dt in {\"char\", \"varchar\", \"nchar\", \"nvarchar\"}:\n",
    "        expr = col_ref\n",
    "    elif dt == \"text\":\n",
    "        expr = f\"CONVERT(varchar(max), {col_ref})\"\n",
    "    elif dt == \"ntext\":\n",
    "        expr = f\"CONVERT(nvarchar(max), {col_ref})\"\n",
    "    elif dt == \"uniqueidentifier\":\n",
    "        expr = f\"CONVERT(varchar(36), {col_ref})\"\n",
    "    elif dt == \"xml\":\n",
    "        expr = f\"CONVERT(nvarchar(max), {col_ref})\"\n",
    "    else:\n",
    "        expr = col_ref\n",
    "\n",
    "    alias = make_safe_identifier(col.column_name)\n",
    "    return f\"{expr} AS [{alias}]\"\n",
    "\n",
    "\n",
    "def build_base_query(schema_name: str, table_name: str, columns):\n",
    "    ordered_cols = sorted(columns, key=lambda r: r.ordinal_position or 0)\n",
    "    select_parts = [column_expression(col) for col in ordered_cols]\n",
    "    select_clause = \",\n",
    "    \".join(select_parts)\n",
    "    return f\"SELECT\n",
    "    {select_clause}\n",
    "FROM [{schema_name}].[{table_name}]\"\n",
    "\n",
    "\n",
    "def load_metadata(path: str):\n",
    "    return spark.read.parquet(path)\n",
    "\n",
    "\n",
    "def validate_metadata(df):\n",
    "    required_cols = [\n",
    "        \"external_obj_id\", \"server_type_id\", \"server_name\", \"db_name\", \"schema_name\", \"obj_name\",\n",
    "        \"obj_type_id\", \"ordinal_position\", \"column_name\", \"column_type_id\", \"is_nullable\",\n",
    "        \"data_type\", \"max_len\", \"numeric_precision\", \"numeric_scale\", \"primary_key_sorting\",\n",
    "        \"default_value\", \"source\", \"src_obj_id\", \"obj_def_id\", \"is_incremental\",\n",
    "        \"is_incremental_column\", \"incremental_criteria\",\n",
    "    ]\n",
    "    missing = [c for c in required_cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Metadata ontbreekt kolommen: {', '.join(missing)}\")\n",
    "    if df.filter(F.col('column_name').isNull()).count() > 0:\n",
    "        raise ValueError(\"Metadata bevat lege kolomnamen\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f51f3e",
   "metadata": {},
   "source": [
    "## Configuratie DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15df4429",
   "metadata": {},
   "outputs": [],
   "source": [
    "sources_schema = T.StructType([\n",
    "    T.StructField(\"Bron\", T.StringType(), False),\n",
    "    T.StructField(\"Server\", T.StringType(), False),\n",
    "    T.StructField(\"Database\", T.StringType(), False),\n",
    "])\n",
    "\n",
    "df_sources = spark.createDataFrame([\n",
    "    (\"ccs_level\", \"vmdwhidpweu01\", \"InsuranceData_CCS_DWH\"),\n",
    "    (\"anva_meeus\", \"vmdwhidpweu01\\MEEUS\", \"InsuranceData_ANVA_DWH\"),\n",
    "    (\"vizier\", \"viz-sql01-mi-p.1d57ac4f4d63.database.windows.net\", \"CRM_DWH\"),\n",
    "    (\"ods_reports\", \"vmdwhodsanvpweu\", \"OG_ODS_Reports\"),\n",
    "    (\"anva_concern\", \"vmdwhidpweu01\", \"InsuranceData_ANVA_DWH\"),\n",
    "    (\"insurance_data_im\", \"vmdwhidpweu01\", \"InsuranceData_OpGroen_DWH\"),\n",
    "], schema=sources_schema)\n",
    "\n",
    "_disabled_schema = T.StructType([\n",
    "    T.StructField(\"Bron\", T.StringType(), False),\n",
    "    T.StructField(\"schema_name\", T.StringType(), False),\n",
    "    T.StructField(\"obj_name\", T.StringType(), False),\n",
    "])\n",
    "\n",
    "df_disabled_tables = spark.createDataFrame([\n",
    "    (\"anva_concern\", \"dbo\", \"Jobmonitor\"),\n",
    "    (\"anva_concern\", \"dbo\", \"LaatsteVerversing\"),\n",
    "    (\"anva_concern\", \"dbo\", \"Metadata\"),\n",
    "    (\"anva_concern\", \"dbo\", \"VrijeLabels\"),\n",
    "    (\"anva_concern\", \"pbi\", \"Nulmeting_Clausules\"),\n",
    "    (\"anva_concern\", \"pbi\", \"Nulmeting_CodesDekking\"),\n",
    "    (\"anva_concern\", \"pbi\", \"Nulmeting_CodesNAW\"),\n",
    "    (\"anva_concern\", \"pbi\", \"Nulmeting_CodesPolis\"),\n",
    "    (\"anva_concern\", \"pbi\", \"Nulmeting_LabelDekking\"),\n",
    "    (\"anva_concern\", \"pbi\", \"Nulmeting_LabelNAW\"),\n",
    "    (\"anva_concern\", \"pbi\", \"Nulmeting_LabelPolis\"),\n",
    "    (\"anva_concern\", \"pbi\", \"Nulmeting_NAWDetails\"),\n",
    "    (\"anva_concern\", \"pbi\", \"Nulmeting_NAWLabels\"),\n",
    "    (\"anva_concern\", \"pbi\", \"Nulmeting_PolisDetails\"),\n",
    "    (\"anva_concern\", \"pbi\", \"Nulmeting_PolisLabels\"),\n",
    "    (\"anva_concern\", \"pbi\", \"Nulmeting_Voorwaarden\"),\n",
    "    (\"anva_meeus\", \"dbo\", \"Jobmonitor\"),\n",
    "    (\"anva_meeus\", \"dbo\", \"LaatsteVerversing\"),\n",
    "    (\"anva_meeus\", \"dbo\", \"Metadata\"),\n",
    "    (\"anva_meeus\", \"dbo\", \"VrijeLabels\"),\n",
    "    (\"anva_meeus\", \"pbi\", \"Nulmeting_Clausules\"),\n",
    "    (\"anva_meeus\", \"pbi\", \"Nulmeting_CodesDekking\"),\n",
    "    (\"anva_meeus\", \"pbi\", \"Nulmeting_CodesNAW\"),\n",
    "    (\"anva_meeus\", \"pbi\", \"Nulmeting_CodesPolis\"),\n",
    "    (\"anva_meeus\", \"pbi\", \"Nulmeting_LabelDekking\"),\n",
    "    (\"anva_meeus\", \"pbi\", \"Nulmeting_LabelNAW\"),\n",
    "    (\"anva_meeus\", \"pbi\", \"Nulmeting_LabelPolis\"),\n",
    "    (\"anva_meeus\", \"pbi\", \"Nulmeting_NAWDetails\"),\n",
    "    (\"anva_meeus\", \"pbi\", \"Nulmeting_NAWLabels\"),\n",
    "    (\"anva_meeus\", \"pbi\", \"Nulmeting_PolisDetails\"),\n",
    "    (\"anva_meeus\", \"pbi\", \"Nulmeting_PolisLabels\"),\n",
    "    (\"anva_meeus\", \"pbi\", \"Nulmeting_Voorwaarden\"),\n",
    "], schema=_disabled_schema)\n",
    "\n",
    "size_schema = T.StructType([\n",
    "    T.StructField(\"Bron\", T.StringType(), False),\n",
    "    T.StructField(\"schema_name\", T.StringType(), False),\n",
    "    T.StructField(\"obj_name\", T.StringType(), False),\n",
    "    T.StructField(\"size_class\", T.StringType(), False),\n",
    "])\n",
    "\n",
    "df_size_class = spark.createDataFrame([\n",
    "    (\"anva_concern\", \"pbi\", \"Fact_PremieFacturen\", \"L\"),\n",
    "    (\"ccs_level\", \"pbi\", \"Fact_PremieBoekingen\", \"L\"),\n",
    "    (\"geintegreerd_model\", \"pbi\", \"Fact_PremieFacturen\", \"L\"),\n",
    "    (\"anva_meeus\", \"pbi\", \"Fact_PremieFacturen\", \"L\"),\n",
    "], schema=size_schema)\n",
    "\n",
    "load_schema = T.StructType([\n",
    "    T.StructField(\"Bron\", T.StringType(), False),\n",
    "    T.StructField(\"schema_name\", T.StringType(), False),\n",
    "    T.StructField(\"obj_name\", T.StringType(), False),\n",
    "    T.StructField(\"load_mode\", T.StringType(), False),\n",
    "    T.StructField(\"filter_column\", T.StringType(), True),\n",
    "    T.StructField(\"kind\", T.StringType(), True),\n",
    "])\n",
    "\n",
    "df_load_mode = spark.createDataFrame([\n",
    "    (\"anva_concern\", \"pbi\", \"Fact_PremieFacturen\", \"window\", \"Boek_Datum\", \"datetime\"),\n",
    "    (\"ccs_level\", \"pbi\", \"Fact_PremieBoekingen\", \"window\", \"Boek_Datum\", \"datetime\"),\n",
    "    (\"geintegreerd_model\", \"pbi\", \"Fact_PremieFacturen\", \"window\", \"Boek_Datum\", \"datetime\"),\n",
    "    (\"anva_meeus\", \"pbi\", \"Fact_PremieFacturen\", \"window\", \"Boek_Datum\", \"datetime\"),\n",
    "    (\"vizier\", \"dbo\", \"Relaties\", \"incremental\", \"Updatedatum\", \"stamp17\"),\n",
    "    (\"vizier\", \"dbo\", \"Contactpersonen\", \"incremental\", \"Upd_dt\", \"stamp17\"),\n",
    "    (\"vizier\", \"dbo\", \"Sleutels\", \"incremental\", \"upd\", \"stamp17\"),\n",
    "    (\"vizier\", \"dbo\", \"Polissen\", \"incremental\", \"upd_dt\", \"stamp17\"),\n",
    "    (\"vizier\", \"dbo\", \"Schades\", \"incremental\", \"upd_dt\", \"stamp17\"),\n",
    "    (\"vizier\", \"dbo\", \"DnB\", \"incremental\", \"upd_dt\", \"stamp17\"),\n",
    "    (\"vizier\", \"dbo\", \"Contactmomenten\", \"incremental\", \"Upd\", \"stamp17\"),\n",
    "    (\"vizier\", \"dbo\", \"Taken\", \"incremental\", \"upd\", \"stamp17\"),\n",
    "    (\"vizier\", \"dbo\", \"Sales\", \"incremental\", \"UPD_DT\", \"stamp17\"),\n",
    "    (\"vizier\", \"dbo\", \"Retenties\", \"incremental\", \"UPD_DT\", \"stamp17\"),\n",
    "    (\"vizier\", \"dbo\", \"Adresbeeld\", \"incremental\", \"UPD_DT\", \"stamp17\"),\n",
    "    (\"vizier\", \"dbo\", \"UBO_Onderzoeken\", \"incremental\", \"UPD_DT\", \"stamp17\"),\n",
    "    (\"vizier\", \"dbo\", \"Producten\", \"incremental\", \"upd\", \"stamp17\"),\n",
    "    (\"vizier\", \"dbo\", \"Medewerkers\", \"incremental\", \"id_upd\", \"stamp17\"),\n",
    "    (\"vizier\", \"dbo\", \"Klachten\", \"incremental\", \"upd_dt\", \"stamp17\"),\n",
    "    (\"vizier\", \"dbo\", \"Verkoopkansen\", \"incremental\", \"upd\", \"stamp17\"),\n",
    "    (\"vizier\", \"dbo\", \"Interesses\", \"incremental\", \"UPD_DT\", \"stamp17\"),\n",
    "], schema=load_schema)\n",
    "\n",
    "window_schema = T.StructType([\n",
    "    T.StructField(\"Bron\", T.StringType(), False),\n",
    "    T.StructField(\"schema_name\", T.StringType(), False),\n",
    "    T.StructField(\"obj_name\", T.StringType(), False),\n",
    "    T.StructField(\"partition_column\", T.StringType(), False),\n",
    "    T.StructField(\"granularity\", T.StringType(), False),\n",
    "    T.StructField(\"lookback_months\", T.IntegerType(), False),\n",
    "])\n",
    "\n",
    "df_window_config = spark.createDataFrame([\n",
    "    (\"anva_concern\", \"pbi\", \"Fact_PremieFacturen\", \"Boek_Datum\", \"month\", 12),\n",
    "    (\"geintegreerd_model\", \"pbi\", \"Fact_PremieFacturen\", \"Boek_Datum\", \"month\", 12),\n",
    "    (\"anva_meeus\", \"pbi\", \"Fact_PremieFacturen\", \"Boek_Datum\", \"month\", 12),\n",
    "    (\"ccs_level\", \"pbi\", \"Fact_PremieBoekingen\", \"Boek_Datum\", \"month\", 12),\n",
    "], schema=window_schema)\n",
    "\n",
    "excluded_schema = T.StructType([\n",
    "    T.StructField(\"Bron\", T.StringType(), False),\n",
    "    T.StructField(\"schema_name\", T.StringType(), False),\n",
    "    T.StructField(\"obj_name\", T.StringType(), False),\n",
    "    T.StructField(\"excluded\", T.IntegerType(), False),\n",
    "])\n",
    "\n",
    "df_excluded_tables = spark.createDataFrame([\n",
    "    (\"vizier\", \"dbo\", \"BO_sleutels_Wim_Verheijen\", 1),\n",
    "    (\"vizier\", \"dbo\", \"UMG_Historie\", 1),\n",
    "], schema=excluded_schema)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43eca608",
   "metadata": {},
   "source": [
    "## Metadata inlezen uit Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457d3003",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_df = validate_metadata(load_metadata(metadata_path))\n",
    "\n",
    "source_mapping = df_sources.filter(F.col(\"Bron\") == F.lit(source))\n",
    "\n",
    "if source_mapping.count() == 0:\n",
    "    raise ValueError(f\"Onbekende source '{source}' in df_sources\")\n",
    "\n",
    "metadata_filtered = (\n",
    "    metadata_df.alias(\"m\")\n",
    "    .join(\n",
    "        source_mapping.alias(\"s\"),\n",
    "        (F.col(\"m.server_name\") == F.col(\"s.Server\")) & (F.col(\"m.db_name\") == F.col(\"s.Database\")),\n",
    "        \"inner\",\n",
    "    )\n",
    "    .withColumn(\"Bron\", F.col(\"s.Bron\"))\n",
    "    .filter(F.col(\"s.Bron\") == F.lit(source))\n",
    ")\n",
    "\n",
    "if metadata_filtered.limit(1).count() == 0:\n",
    "    raise ValueError(\"Geen metadata records gevonden voor de opgegeven source\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9aa7fb",
   "metadata": {},
   "source": [
    "## Base query generatie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c667ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_schema = T.StructType([\n",
    "    T.StructField(\"Bron\", T.StringType(), False),\n",
    "    T.StructField(\"schema_name\", T.StringType(), False),\n",
    "    T.StructField(\"obj_name\", T.StringType(), False),\n",
    "    T.StructField(\"base_query\", T.StringType(), False),\n",
    "])\n",
    "\n",
    "col_struct = F.struct(\n",
    "    \"ordinal_position\", \"column_name\", \"data_type\", \"numeric_precision\", \"numeric_scale\", \"max_len\"\n",
    ")\n",
    "\n",
    "base_query_df = (\n",
    "    metadata_filtered\n",
    "    .select(\"Bron\", \"schema_name\", \"obj_name\", col_struct.alias(\"column\"))\n",
    "    .groupBy(\"Bron\", \"schema_name\", \"obj_name\")\n",
    "    .agg(F.collect_list(\"column\").alias(\"columns\"))\n",
    "    .rdd\n",
    "    .map(lambda row: (row.Bron, row.schema_name, row.obj_name, build_base_query(row.schema_name, row.obj_name, row.columns)))\n",
    "    .toDF(schema=base_schema)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b879efc7",
   "metadata": {},
   "source": [
    "## Configuratie samenvoegen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8915c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = (\n",
    "    base_query_df.alias(\"bq\")\n",
    "    .join(df_disabled_tables.alias(\"dis\"), [\"Bron\", \"schema_name\", \"obj_name\"], \"left\")\n",
    "    .join(df_size_class.alias(\"sz\"), [\"Bron\", \"schema_name\", \"obj_name\"], \"left\")\n",
    "    .join(df_load_mode.alias(\"lm\"), [\"Bron\", \"schema_name\", \"obj_name\"], \"left\")\n",
    "    .join(df_window_config.alias(\"wnd\"), [\"Bron\", \"schema_name\", \"obj_name\"], \"left\")\n",
    "    .join(df_excluded_tables.alias(\"ex\"), [\"Bron\", \"schema_name\", \"obj_name\"], \"left\")\n",
    "    .withColumn(\"enabled\", F.when(F.col(\"dis.obj_name\").isNull(), F.lit(True)).otherwise(F.lit(False)))\n",
    "    .withColumn(\"size_class\", F.when(F.col(\"sz.size_class\").isNull(), F.lit(\"S\")).otherwise(F.col(\"sz.size_class\")))\n",
    "    .withColumn(\"load_mode\", F.when(F.col(\"lm.load_mode\").isNull(), F.lit(\"snapshot\")).otherwise(F.col(\"lm.load_mode\")))\n",
    "    .withColumn(\"excluded\", F.when(F.col(\"ex.excluded\").isNull(), F.lit(0)).otherwise(F.col(\"ex.excluded\")))\n",
    "    .select(\n",
    "        F.col(\"bq.obj_name\").alias(\"name\"),\n",
    "        \"schema_name\",\n",
    "        \"Bron\",\n",
    "        \"enabled\",\n",
    "        \"size_class\",\n",
    "        \"load_mode\",\n",
    "        F.col(\"bq.base_query\"),\n",
    "        F.col(\"lm.filter_column\"),\n",
    "        F.col(\"lm.kind\"),\n",
    "        F.col(\"wnd.partition_column\"),\n",
    "        F.col(\"wnd.granularity\"),\n",
    "        F.col(\"wnd.lookback_months\"),\n",
    "        \"excluded\",\n",
    "    )\n",
    "    .filter(F.col(\"excluded\") == 0)\n",
    "    .orderBy(\"name\")\n",
    ")\n",
    "\n",
    "tables.cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b8c5eb",
   "metadata": {},
   "source": [
    "## JSON constructie en wegschrijven"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c90df53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def table_record(row: T.Row) -> Dict[str, object]:\n",
    "    record = {\n",
    "        \"name\": row.name,\n",
    "        \"enabled\": bool(row.enabled),\n",
    "        \"size_class\": row.size_class or \"S\",\n",
    "        \"load_mode\": row.load_mode or \"snapshot\",\n",
    "        \"delta_schema\": source,\n",
    "        \"delta_table\": row.name,\n",
    "        \"base_query\": row.base_query,\n",
    "    }\n",
    "    if record[\"load_mode\"] == \"window\":\n",
    "        record[\"window\"] = {\n",
    "            \"partition_column\": row.partition_column,\n",
    "            \"granularity\": row.granularity,\n",
    "            \"lookback_months\": int(row.lookback_months) if row.lookback_months is not None else None,\n",
    "        }\n",
    "    if record[\"load_mode\"] == \"incremental\":\n",
    "        record[\"incremental_column\"] = {\n",
    "            \"column\": row.filter_column,\n",
    "            \"kind\": row.kind,\n",
    "        }\n",
    "    return record\n",
    "\n",
    "\n",
    "defaults = {\n",
    "    \"concurrency_large\": 2,\n",
    "    \"concurrency_small\": 8,\n",
    "    \"max_rows_per_file_large\": 15_000_000,\n",
    "    \"max_rows_per_file_small\": 1_000_000,\n",
    "}\n",
    "\n",
    "result = {\n",
    "    \"source\": source,\n",
    "    \"run_date_utc\": None,\n",
    "    \"watermarks_path\": \"config/watermarks.json\",\n",
    "    \"base_files\": \"greenhouse_sources\",\n",
    "    \"connection_name\": f\"connection_{source}_prod\",\n",
    "    \"defaults\": defaults,\n",
    "    \"tables\": [table_record(row) for row in tables.collect()],\n",
    "}\n",
    "\n",
    "output_path = f\"Files/config/{source}_metadata.json\"\n",
    "json_payload = json.dumps(result, ensure_ascii=False, indent=4)\n",
    "\n",
    "mssparkutils.fs.put(output_path, json_payload, True)\n",
    "print(f\"Metadata JSON geschreven naar {output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
