{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export Delta Table to Parquet\n",
    "\n",
    "Export a Delta table from Tables/ to a Parquet file in Files/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "delta_table = \"logs.bronze_processing_log\"  # Schema.table name\n",
    "output_path = \"Files/metadata/logs_bronze_processing_log.parquet\"\n",
    "mode = \"overwrite\"  # \"overwrite\" or \"append\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module fabric.bootstrap\n",
    "# ---------------------\n",
    "# This cell enables a flexible module loading strategy:\n",
    "#\n",
    "# PRODUCTION (default): The `Files/code` directory is empty. This function does nothing,\n",
    "# and Python imports all modules from the stable, versioned Wheel in the Environment.\n",
    "#\n",
    "# DEVELOPMENT / HOTFIX: To bypass the 15-20 minute Fabric publish cycle for urgent fixes,\n",
    "# upload individual .py files to `Files/code` in the Lakehouse. This function prepends\n",
    "# that path to sys.path, so Python finds the override files first. All other modules\n",
    "# continue to load from the Wheel - only the uploaded files are replaced.\n",
    "#\n",
    "# Usage: Keep `Files/code` empty for production stability. Use it only for rapid\n",
    "# iteration during development or emergency hotfixes.\n",
    "\n",
    "from modules.fabric_bootstrap import ensure_module_path\n",
    "ensure_module_path()  # Now Python can find the rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.spark_session import get_or_create_spark_session\n",
    "\n",
    "spark = get_or_create_spark_session(app_name=\"Export_Delta_to_Parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Delta table\n",
    "df = spark.table(delta_table)\n",
    "\n",
    "print(f\"Reading {delta_table}: {df.count():,} rows\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to Parquet\n",
    "df.write.mode(mode).parquet(output_path)\n",
    "\n",
    "print(f\"\\nâœ“ Exported to {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
