{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# 02 — Configuration Utilities for Bronze and Silver Processing\n",
    "\n",
    "This notebook provides configuration management for the data pipeline:\n",
    "\n",
    "## Configuration Files\n",
    "- **DAG files** (`dag_<source>_<schedule>.json`) - Table definitions and load modes\n",
    "- **Watermarks** (`watermarks.json`) - Incremental loading state (READ-ONLY)\n",
    "- **Runplan** (`runplan.json`) - Scheduling configuration\n",
    "\n",
    "## Key Features\n",
    "- DAG loading and validation\n",
    "- Path resolution (Fabric vs Local)\n",
    "- Table filtering (enabled, retry_tables)\n",
    "- Load mode validation\n",
    "- Watermark reading (managed by data pipeline)\n",
    "\n",
    "**Important:** Notebooks NEVER modify watermarks.json - this is managed by the extraction pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters (Papermill compatible)\n",
    "config_base_path = None  # Will be auto-detected if None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## [1] Imports and Path Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from typing import Dict, List, Any, Optional\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "if not logger.handlers:\n",
    "    handler = logging.StreamHandler()\n",
    "    formatter = logging.Formatter(\"%(asctime)s %(levelname)s %(name)s - %(message)s\")\n",
    "    handler.setFormatter(formatter)\n",
    "    logger.addHandler(handler)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "logger.info(\"✓ Imports loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_base_path() -> str:\n",
    "    \"\"\"\n",
    "    Auto-detect base path for Files directory.\n",
    "    \n",
    "    Tries multiple possible locations in order:\n",
    "    1. /lakehouse/default/Files (Microsoft Fabric)\n",
    "    2. /data/lakehouse/*/Files (Custom cluster with mounted storage)\n",
    "    3. Files (Relative path for local development)\n",
    "    \n",
    "    Returns:\n",
    "        Absolute or relative path to Files directory\n",
    "    \"\"\"\n",
    "    import glob\n",
    "    \n",
    "    # Option 1: Microsoft Fabric\n",
    "    fabric_path = '/lakehouse/default/Files'\n",
    "    if os.path.exists(fabric_path):\n",
    "        return fabric_path\n",
    "    \n",
    "    # Option 2: Custom cluster with mounted lakehouse storage\n",
    "    # Pattern: /data/lakehouse/{lakehouse_name}/Files\n",
    "    logger.info(f\"Checking for custom cluster Files directory /data/lakehouse... {os.path.exists('/data/lakehouse')}\")\n",
    "    if os.path.exists('/data/lakehouse'):\n",
    "        pattern = '/data/lakehouse/**/Files'\n",
    "        matches = glob.glob(pattern, recursive=True)\n",
    "        logger.info(f\"Detected matches for custom cluster Files directories: {matches}\")\n",
    "        if matches:\n",
    "            # Use first match (sorted for consistency)\n",
    "            base = sorted(matches)[0]\n",
    "            return base\n",
    "    \n",
    "    # Option 3: Relative path (local development / repository)\n",
    "    return 'Files'\n",
    "\n",
    "# Set base path\\n\",\n",
    "if config_base_path is None:\n",
    "    BASE_PATH = detect_base_path()\n",
    "else:\n",
    "    BASE_PATH = config_base_path\n",
    "\n",
    "# Detect environment type\n",
    "if BASE_PATH == '/lakehouse/default/Files':\n",
    "    env_type = 'Fabric'\n",
    "elif BASE_PATH.startswith('/data/lakehouse/'):\n",
    "    env_type = 'Custom Cluster'\n",
    "elif BASE_PATH.startswith('/'):\n",
    "    env_type = 'Absolute Path'\n",
    "else:\n",
    "    env_type = 'Local/Relative'\n",
    "\n",
    "logger.info(f\"✓ Base path: {BASE_PATH}\")\n",
    "logger.info(f\"✓ Environment: {env_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## [2] Configuration Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard config locations\n",
    "CONFIG_DIR = f\"{BASE_PATH}/config\"\n",
    "WATERMARKS_PATH = f\"{CONFIG_DIR}/watermarks.json\"\n",
    "RUNPLAN_PATH = f\"{CONFIG_DIR}/runplan.json\"\n",
    "\n",
    "# Data paths\n",
    "DATA_BASE = f\"{BASE_PATH}/greenhouse_sources\"  # Default, can be overridden in DAG\n",
    "\n",
    "logger.info(f\"✓ Config directory: {CONFIG_DIR}\")\n",
    "logger.info(f\"✓ Watermarks path: {WATERMARKS_PATH}\")\n",
    "logger.info(f\"✓ Runplan path: {RUNPLAN_PATH}\")\n",
    "logger.info(f\"✓ Data base path: {DATA_BASE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## [3] DAG Loading and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dag(dag_path: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Load and validate a DAG configuration file.\n",
    "    \n",
    "    Args:\n",
    "        dag_path: Path to DAG JSON file (relative to BASE_PATH or absolute)\n",
    "    \n",
    "    Returns:\n",
    "        Dict with DAG configuration\n",
    "    \n",
    "    Raises:\n",
    "        FileNotFoundError: If DAG file doesn't exist\n",
    "        ValueError: If DAG validation fails\n",
    "    \"\"\"\n",
    "    # Handle both absolute and relative paths\n",
    "    if not os.path.isabs(dag_path):\n",
    "        # Relative path - check if it starts with BASE_PATH\n",
    "        if not dag_path.startswith(BASE_PATH):\n",
    "            dag_path = f\"{BASE_PATH}/{dag_path}\"\n",
    "    \n",
    "    if not os.path.exists(dag_path):\n",
    "        raise FileNotFoundError(f\"DAG file not found: {dag_path}\")\n",
    "    \n",
    "    with open(dag_path, 'r') as f:\n",
    "        dag = json.load(f)\n",
    "    \n",
    "    # Validate required fields\n",
    "    validate_dag(dag)\n",
    "    \n",
    "    return dag\n",
    "\n",
    "\n",
    "def validate_dag(dag: Dict[str, Any]) -> None:\n",
    "    \"\"\"\n",
    "    Validate DAG structure and required fields.\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: If validation fails\n",
    "    \"\"\"\n",
    "    # Required top-level fields\n",
    "    required_fields = [\"source\", \"tables\"]\n",
    "    \n",
    "    for field in required_fields:\n",
    "        if field not in dag:\n",
    "            raise ValueError(f\"DAG missing required field: {field}\")\n",
    "    \n",
    "    # Validate source name\n",
    "    if not dag[\"source\"] or not isinstance(dag[\"source\"], str):\n",
    "        raise ValueError(f\"Invalid source name: {dag.get('source')}\")\n",
    "    \n",
    "    # Validate tables\n",
    "    if not isinstance(dag[\"tables\"], list):\n",
    "        raise ValueError(\"DAG 'tables' must be a list\")\n",
    "    \n",
    "    if len(dag[\"tables\"]) == 0:\n",
    "        raise ValueError(\"DAG has no tables defined\")\n",
    "    \n",
    "    # Validate each table\n",
    "    valid_load_modes = {\"snapshot\", \"incremental\", \"window\"}\n",
    "    \n",
    "    for idx, table in enumerate(dag[\"tables\"]):\n",
    "        if \"name\" not in table:\n",
    "            raise ValueError(f\"Table at index {idx} missing 'name' field\")\n",
    "        \n",
    "        # Validate load_mode if present\n",
    "        if \"load_mode\" in table:\n",
    "            load_mode = table[\"load_mode\"].lower()\n",
    "            if load_mode not in valid_load_modes:\n",
    "                logger.info(f\"⚠️  Warning: Table '{table['name']}' has unsupported load_mode: {load_mode}\")\n",
    "\n",
    "\n",
    "logger.info(\"✓ DAG loading functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## [4] Table Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_enabled_tables(dag: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Get all enabled tables from DAG.\n",
    "    \n",
    "    A table is enabled if:\n",
    "    - 'enabled' field is True, 1, or missing (default=enabled)\n",
    "    \n",
    "    Returns:\n",
    "        List of table definitions\n",
    "    \"\"\"\n",
    "    enabled = []\n",
    "    \n",
    "    for table in dag[\"tables\"]:\n",
    "        # Default to enabled if field missing\n",
    "        enabled_flag = table.get(\"enabled\", True)\n",
    "        \n",
    "        # Handle various true values (True, 1, \"1\", \"true\")\n",
    "        if enabled_flag in (True, 1, \"1\", \"true\", \"True\"):\n",
    "            enabled.append(table)\n",
    "    \n",
    "    return enabled\n",
    "\n",
    "\n",
    "def filter_retry_tables(\n",
    "    tables: List[Dict[str, Any]], \n",
    "    retry_tables: Optional[List[str]]\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Filter tables to only those in retry_tables list.\n",
    "    \n",
    "    Args:\n",
    "        tables: List of table definitions\n",
    "        retry_tables: List of table names to retry, or None for all\n",
    "    \n",
    "    Returns:\n",
    "        Filtered list of tables\n",
    "    \"\"\"\n",
    "    if retry_tables is None or len(retry_tables) == 0:\n",
    "        return tables\n",
    "    \n",
    "    retry_set = set(retry_tables)\n",
    "    filtered = [t for t in tables if t[\"name\"] in retry_set]\n",
    "    \n",
    "    # Warn about missing tables\n",
    "    found_names = {t[\"name\"] for t in filtered}\n",
    "    missing = retry_set - found_names\n",
    "    if missing:\n",
    "        logger.info(f\"⚠️  Warning: Retry tables not found in DAG: {sorted(missing)}\")\n",
    "    \n",
    "    return filtered\n",
    "\n",
    "\n",
    "def get_tables_by_load_mode(\n",
    "    tables: List[Dict[str, Any]], \n",
    "    load_mode: str\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Filter tables by load_mode.\n",
    "    \n",
    "    Args:\n",
    "        tables: List of table definitions\n",
    "        load_mode: Load mode to filter (e.g., \"incremental\", \"snapshot\")\n",
    "    \n",
    "    Returns:\n",
    "        Filtered list of tables\n",
    "    \"\"\"\n",
    "    load_mode_lower = load_mode.lower()\n",
    "    return [\n",
    "        t for t in tables \n",
    "        if t.get(\"load_mode\", \"snapshot\").lower() == load_mode_lower\n",
    "    ]\n",
    "\n",
    "\n",
    "def get_tables_to_process(\n",
    "    dag: Dict[str, Any],\n",
    "    retry_tables: Optional[List[str]] = None,\n",
    "    only_enabled: bool = True\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Get final list of tables to process based on filters.\n",
    "    \n",
    "    This is the main entry point for determining which tables to load.\n",
    "    \n",
    "    Args:\n",
    "        dag: DAG configuration\n",
    "        retry_tables: Optional list of specific tables to retry\n",
    "        only_enabled: If True, only return enabled tables\n",
    "    \n",
    "    Returns:\n",
    "        List of table definitions to process\n",
    "    \"\"\"\n",
    "    tables = dag[\"tables\"]\n",
    "    \n",
    "    # Filter by enabled status\n",
    "    if only_enabled:\n",
    "        tables = get_enabled_tables(dag)\n",
    "    \n",
    "    # Filter by retry list if provided\n",
    "    if retry_tables:\n",
    "        tables = filter_retry_tables(tables, retry_tables)\n",
    "    \n",
    "    return tables\n",
    "\n",
    "\n",
    "logger.info(\"✓ Table filtering functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## [5] Watermark Management (READ-ONLY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_watermarks() -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Load watermarks configuration.\n",
    "    \n",
    "    NOTE: This is READ-ONLY. Watermarks are managed by the data extraction pipeline.\n",
    "    \n",
    "    Returns:\n",
    "        Dict with watermarks configuration\n",
    "    \"\"\"\n",
    "    if not os.path.exists(WATERMARKS_PATH):\n",
    "        raise FileNotFoundError(f\"Watermarks file not found: {WATERMARKS_PATH}\")\n",
    "    \n",
    "    with open(WATERMARKS_PATH, 'r') as f:\n",
    "        watermarks = json.load(f)\n",
    "    \n",
    "    return watermarks\n",
    "\n",
    "\n",
    "def get_source_watermarks(source: str) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Get watermarks for a specific source.\n",
    "    \n",
    "    Args:\n",
    "        source: Source system name (e.g., \"vizier\", \"anva_concern\")\n",
    "    \n",
    "    Returns:\n",
    "        Dict with table watermarks, or None if source not found\n",
    "    \"\"\"\n",
    "    watermarks = load_watermarks()\n",
    "    \n",
    "    # Watermarks structure: {\"source\": [{\"name\": \"vizier\", \"tables\": {...}}]}\n",
    "    sources = watermarks.get(\"source\", [])\n",
    "    \n",
    "    for src in sources:\n",
    "        if src.get(\"name\") == source:\n",
    "            return src.get(\"tables\", {})\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def get_table_watermark(\n",
    "    source: str, \n",
    "    table_name: str\n",
    ") -> Optional[Any]:\n",
    "    \"\"\"\n",
    "    Get watermark value for a specific table.\n",
    "    \n",
    "    Args:\n",
    "        source: Source system name\n",
    "        table_name: Table name\n",
    "    \n",
    "    Returns:\n",
    "        Watermark value (string, int, or None)\n",
    "    \"\"\"\n",
    "    source_wm = get_source_watermarks(source)\n",
    "    \n",
    "    if source_wm is None:\n",
    "        return None\n",
    "    \n",
    "    return source_wm.get(table_name)\n",
    "\n",
    "\n",
    "logger.info(\"✓ Watermark functions defined (READ-ONLY)\")\n",
    "logger.info(\"⚠️  NOTE: Watermarks are managed by extraction pipeline, not by notebooks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## [6] Path Building Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bronze_table_name(\n",
    "    table_def: Dict[str, Any],\n",
    "    default_schema: str = \"bronze\"\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Build full Bronze table name from table definition.\n",
    "    \n",
    "    Args:\n",
    "        table_def: Table definition from DAG\n",
    "        default_schema: Default schema if not specified in table_def\n",
    "    \n",
    "    Returns:\n",
    "        Full table name (schema.table)\n",
    "    \"\"\"\n",
    "    table_name = table_def.get(\"name\")\n",
    "    delta_table = table_def.get(\"delta_table\", table_name)\n",
    "    \n",
    "    # Check if delta_table already has schema\n",
    "    if \".\" in delta_table:\n",
    "        return delta_table\n",
    "    \n",
    "    # Get schema from table_def or use default\n",
    "    schema = table_def.get(\"delta_schema\", default_schema)\n",
    "    \n",
    "    return f\"{schema}.{delta_table}\"\n",
    "\n",
    "\n",
    "def build_silver_table_name(\n",
    "    table_def: Dict[str, Any],\n",
    "    default_schema: str = \"silver\"\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Build full Silver table name from table definition.\n",
    "    \n",
    "    Args:\n",
    "        table_def: Table definition from DAG\n",
    "        default_schema: Default schema if not specified\n",
    "    \n",
    "    Returns:\n",
    "        Full table name (schema.table)\n",
    "    \"\"\"\n",
    "    # Check if delta_table specifies Silver schema\n",
    "    delta_table = table_def.get(\"delta_table\")\n",
    "    if delta_table and delta_table.startswith(\"silver.\"):\n",
    "        return delta_table\n",
    "    \n",
    "    # Otherwise use table name with default Silver schema\n",
    "    table_name = table_def.get(\"name\")\n",
    "    return f\"{default_schema}.{table_name}\"\n",
    "\n",
    "\n",
    "logger.info(\"✓ Path building functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## [7] DAG Query Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dag_metadata(dag: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Extract metadata from DAG.\n",
    "    \n",
    "    Returns:\n",
    "        Dict with metadata fields\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"source\": dag.get(\"source\"),\n",
    "        \"base_files\": dag.get(\"base_files\", \"greenhouse_sources\"),\n",
    "        \"watermarks_path\": dag.get(\"watermarks_path\", \"config/watermarks.json\"),\n",
    "        \"connection_name\": dag.get(\"connection_name\"),\n",
    "        \"defaults\": dag.get(\"defaults\", {}),\n",
    "    }\n",
    "\n",
    "\n",
    "def get_business_keys(table_def: Dict[str, Any]) -> Optional[List[str]]:\n",
    "    \"\"\"\n",
    "    Get business keys for a table (used for CDC merge).\n",
    "    \n",
    "    Returns:\n",
    "        List of business key column names, or None if not defined\n",
    "    \"\"\"\n",
    "    return table_def.get(\"business_keys\")\n",
    "\n",
    "\n",
    "def get_incremental_column(table_def: Dict[str, Any]) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Get incremental column for a table.\n",
    "    \n",
    "    Returns:\n",
    "        Column name used for incremental loading, or None\n",
    "    \"\"\"\n",
    "    incremental = table_def.get(\"incremental\", {})\n",
    "    return incremental.get(\"column\")\n",
    "\n",
    "\n",
    "def get_window_config(table_def: Dict[str, Any]) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Get window configuration for a table.\n",
    "    \n",
    "    Returns:\n",
    "        Dict with window config (column, granularity, lookback, etc.)\n",
    "    \"\"\"\n",
    "    return table_def.get(\"window\")\n",
    "\n",
    "\n",
    "def get_partitioning_config(table_def: Dict[str, Any]) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Get partitioning configuration for a table.\n",
    "    \n",
    "    Returns:\n",
    "        Dict with partitioning config (type, year_col, month_col)\n",
    "    \"\"\"\n",
    "    return table_def.get(\"partitioning\")\n",
    "\n",
    "\n",
    "def summarize_dag(dag: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Generate summary statistics for a DAG.\n",
    "    \n",
    "    Returns:\n",
    "        Dict with counts by load_mode, enabled status, etc.\n",
    "    \"\"\"\n",
    "    tables = dag[\"tables\"]\n",
    "    enabled = get_enabled_tables(dag)\n",
    "    \n",
    "    # Count by load_mode\n",
    "    load_mode_counts = {}\n",
    "    for table in enabled:\n",
    "        mode = table.get(\"load_mode\", \"snapshot\")\n",
    "        load_mode_counts[mode] = load_mode_counts.get(mode, 0) + 1\n",
    "    \n",
    "    return {\n",
    "        \"source\": dag.get(\"source\"),\n",
    "        \"total_tables\": len(tables),\n",
    "        \"enabled_tables\": len(enabled),\n",
    "        \"disabled_tables\": len(tables) - len(enabled),\n",
    "        \"load_mode_counts\": load_mode_counts,\n",
    "    }\n",
    "\n",
    "\n",
    "logger.info(\"✓ DAG query helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## [8] Runplan Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_runplan() -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Load runplan configuration.\n",
    "    \n",
    "    Returns:\n",
    "        List of scheduled runs\n",
    "    \"\"\"\n",
    "    if not os.path.exists(RUNPLAN_PATH):\n",
    "        logger.info(f\"⚠️  Warning: Runplan file not found: {RUNPLAN_PATH}\")\n",
    "        return []\n",
    "    \n",
    "    with open(RUNPLAN_PATH, 'r') as f:\n",
    "        runplan = json.load(f)\n",
    "    \n",
    "    return runplan\n",
    "\n",
    "\n",
    "def get_source_schedule(source: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Get schedule entries for a specific source.\n",
    "    \n",
    "    Args:\n",
    "        source: Source system name\n",
    "    \n",
    "    Returns:\n",
    "        List of schedule entries (may be multiple for weekday/weekend)\n",
    "    \"\"\"\n",
    "    runplan = load_runplan()\n",
    "    return [entry for entry in runplan if entry.get(\"source\") == source]\n",
    "\n",
    "\n",
    "logger.info(\"✓ Runplan functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## [9] Verification and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logger.info(\"=\" * 80)\n",
    "# logger.info(\"CONFIGURATION UTILITIES VERIFICATION\")\n",
    "# logger.info(\"=\" * 80)\n",
    "\n",
    "# # Test environment detection\n",
    "# logger.info(f\"\\n1. Environment Detection:\")\n",
    "# logger.info(f\"   Base path: {BASE_PATH}\")\n",
    "# logger.info(f\"   Environment: {'Fabric' if '/lakehouse' in BASE_PATH else 'Local'}\")\n",
    "\n",
    "# # Test config paths\n",
    "# logger.info(f\"\\n2. Configuration Paths:\")\n",
    "# logger.info(f\"   Config dir: {CONFIG_DIR}\")\n",
    "# logger.info(f\"   Watermarks: {WATERMARKS_PATH}\")\n",
    "# logger.info(f\"   Runplan: {RUNPLAN_PATH}\")\n",
    "\n",
    "# # Check if config files exist\n",
    "# logger.info(f\"\\n3. Config Files Status:\")\n",
    "# logger.info(f\"   Config dir exists: {os.path.exists(CONFIG_DIR)}\")\n",
    "# logger.info(f\"   Watermarks exists: {os.path.exists(WATERMARKS_PATH)}\")\n",
    "# logger.info(f\"   Runplan exists: {os.path.exists(RUNPLAN_PATH)}\")\n",
    "\n",
    "# # Try to list DAG files\n",
    "# if os.path.exists(CONFIG_DIR):\n",
    "#     dag_files = [f for f in os.listdir(CONFIG_DIR) if f.startswith('dag_') and f.endswith('.json')]\n",
    "#     logger.info(f\"\\n4. Available DAG files: {len(dag_files)}\")\n",
    "#     for dag_file in sorted(dag_files)[:5]:  # Show first 5\n",
    "#         logger.info(f\"   - {dag_file}\")\n",
    "#     if len(dag_files) > 5:\n",
    "#         logger.info(f\"   ... and {len(dag_files) - 5} more\")\n",
    "\n",
    "# logger.info(\"\\n✓ Configuration utilities ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## [10] Example Usage\n",
    "\n",
    "Example of how to use these configuration functions in other notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Load a DAG and get tables to process\n",
    "# \n",
    "# dag_path = f\"{CONFIG_DIR}/dag_vizier_weekday.json\"\n",
    "# dag = load_dag(dag_path)\n",
    "# \n",
    "# # Get all enabled tables\n",
    "# tables = get_tables_to_process(dag)\n",
    "# logger.info(f\"Found {len(tables)} enabled tables\")\n",
    "# \n",
    "# # Get only incremental tables\n",
    "# incremental_tables = get_tables_by_load_mode(tables, \"incremental\")\n",
    "# logger.info(f\"Found {len(incremental_tables)} incremental tables\")\n",
    "# \n",
    "# # Get metadata\n",
    "# metadata = get_dag_metadata(dag)\n",
    "# logger.info(f\"Source: {metadata['source']}\")\n",
    "# logger.info(f\"Base files: {metadata['base_files']}\")\n",
    "# \n",
    "# # Build parquet path for a table\n",
    "# table = tables[0]\n",
    "# parquet_path = build_parquet_path(\n",
    "#     source=metadata['source'],\n",
    "#     run_ts=\"20251105T120000123\",\n",
    "#     table_name=table['name'],\n",
    "#     base_files=metadata['base_files']\n",
    "# )\n",
    "# logger.info(f\"Parquet path: {parquet_path}\")\n",
    "# \n",
    "# # Get watermark for incremental table\n",
    "# if table.get('load_mode') == 'incremental':\n",
    "#     watermark = get_table_watermark(metadata['source'], table['name'])\n",
    "#     logger.info(f\"Watermark for {table['name']}: {watermark}\")\n",
    "\n",
    "#logger.info(\"✓ Example usage documented (commented out)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
