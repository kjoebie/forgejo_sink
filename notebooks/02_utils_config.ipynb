{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# 02 — Configuration Utilities for Bronze and Silver Processing\n",
    "\n",
    "This notebook provides configuration management for the data pipeline:\n",
    "\n",
    "## Configuration Files\n",
    "- **DAG files** (`dag_<source>_<schedule>.json`) - Table definitions and load modes\n",
    "- **Watermarks** (`watermarks.json`) - Incremental loading state (READ-ONLY)\n",
    "- **Runplan** (`runplan.json`) - Scheduling configuration\n",
    "\n",
    "## Key Features\n",
    "- DAG loading and validation\n",
    "- Path resolution (Fabric vs Local)\n",
    "- Table filtering (enabled, retry_tables)\n",
    "- Load mode validation\n",
    "- Watermark reading (managed by data pipeline)\n",
    "\n",
    "**Important:** Notebooks NEVER modify watermarks.json - this is managed by the extraction pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters (Papermill compatible)\n",
    "config_base_path = None  # Will be auto-detected if None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## [1] Imports and Path Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from typing import Dict, List, Any, Optional\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "logger.info(\"✓ Imports loaded\")\n",
    "from modules.path_utils import get_base_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bepaal Files-basispad met de centrale helper\n",
    "if config_base_path is None:\n",
    "    BASE_PATH = get_base_path()\n",
    "else:\n",
    "    BASE_PATH = config_base_path\n",
    "\n",
    "# Detect environment type\n",
    "if BASE_PATH == '/lakehouse/default/Files':\n",
    "    env_type = 'Fabric'\n",
    "elif '/data/lakehouse/' in BASE_PATH:\n",
    "    env_type = 'Custom Cluster'\n",
    "elif BASE_PATH.startswith('/'):\n",
    "    env_type = 'Absolute Path'\n",
    "else:\n",
    "    env_type = 'Local/Relative'\n",
    "\n",
    "logger.info(f\"✓ Base path: {BASE_PATH}\")\n",
    "logger.info(f\"✓ Environment: {env_type}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## [2] Configuration Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard config locations\n",
    "CONFIG_DIR = f\"{BASE_PATH}/config\"\n",
    "WATERMARKS_PATH = f\"{CONFIG_DIR}/watermarks.json\"\n",
    "RUNPLAN_PATH = f\"{CONFIG_DIR}/runplan.json\"\n",
    "\n",
    "# Data paths\n",
    "DATA_BASE = f\"{BASE_PATH}/greenhouse_sources\"  # Default, can be overridden in DAG\n",
    "\n",
    "logger.info(f\"✓ Config directory: {CONFIG_DIR}\")\n",
    "logger.info(f\"✓ Watermarks path: {WATERMARKS_PATH}\")\n",
    "logger.info(f\"✓ Runplan path: {RUNPLAN_PATH}\")\n",
    "logger.info(f\"✓ Data base path: {DATA_BASE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## [3] DAG Loading and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": "# Import all configuration functions from module\nfrom modules.config_utils import (\n    load_dag,\n    validate_dag,\n    get_enabled_tables,\n    filter_retry_tables,\n    get_tables_by_load_mode,\n    get_tables_to_process\n)\n\nlogger.info(\"✓ DAG loading and filtering functions imported from modules.config_utils\")"
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## [4] Table Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_enabled_tables(dag: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Get all enabled tables from DAG.\n",
    "    \n",
    "    A table is enabled if:\n",
    "    - 'enabled' field is True, 1, or missing (default=enabled)\n",
    "    \n",
    "    Returns:\n",
    "        List of table definitions\n",
    "    \"\"\"\n",
    "    enabled = []\n",
    "    \n",
    "    for table in dag[\"tables\"]:\n",
    "        # Default to enabled if field missing\n",
    "        enabled_flag = table.get(\"enabled\", True)\n",
    "        \n",
    "        # Handle various true values (True, 1, \"1\", \"true\")\n",
    "        if enabled_flag in (True, 1, \"1\", \"true\", \"True\"):\n",
    "            enabled.append(table)\n",
    "    \n",
    "    return enabled\n",
    "\n",
    "\n",
    "def filter_retry_tables(\n",
    "    tables: List[Dict[str, Any]], \n",
    "    retry_tables: Optional[List[str]]\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Filter tables to only those in retry_tables list.\n",
    "    \n",
    "    Args:\n",
    "        tables: List of table definitions\n",
    "        retry_tables: List of table names to retry, or None for all\n",
    "    \n",
    "    Returns:\n",
    "        Filtered list of tables\n",
    "    \"\"\"\n",
    "    if retry_tables is None or len(retry_tables) == 0:\n",
    "        return tables\n",
    "    \n",
    "    retry_set = set(retry_tables)\n",
    "    filtered = [t for t in tables if t[\"name\"] in retry_set]\n",
    "    \n",
    "    # Warn about missing tables\n",
    "    found_names = {t[\"name\"] for t in filtered}\n",
    "    missing = retry_set - found_names\n",
    "    if missing:\n",
    "        logger.info(f\"⚠️  Warning: Retry tables not found in DAG: {sorted(missing)}\")\n",
    "    \n",
    "    return filtered\n",
    "\n",
    "\n",
    "def get_tables_by_load_mode(\n",
    "    tables: List[Dict[str, Any]], \n",
    "    load_mode: str\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Filter tables by load_mode.\n",
    "    \n",
    "    Args:\n",
    "        tables: List of table definitions\n",
    "        load_mode: Load mode to filter (e.g., \"incremental\", \"snapshot\")\n",
    "    \n",
    "    Returns:\n",
    "        Filtered list of tables\n",
    "    \"\"\"\n",
    "    load_mode_lower = load_mode.lower()\n",
    "    return [\n",
    "        t for t in tables \n",
    "        if t.get(\"load_mode\", \"snapshot\").lower() == load_mode_lower\n",
    "    ]\n",
    "\n",
    "\n",
    "def get_tables_to_process(\n",
    "    dag: Dict[str, Any],\n",
    "    retry_tables: Optional[List[str]] = None,\n",
    "    only_enabled: bool = True\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Get final list of tables to process based on filters.\n",
    "    \n",
    "    This is the main entry point for determining which tables to load.\n",
    "    \n",
    "    Args:\n",
    "        dag: DAG configuration\n",
    "        retry_tables: Optional list of specific tables to retry\n",
    "        only_enabled: If True, only return enabled tables\n",
    "    \n",
    "    Returns:\n",
    "        List of table definitions to process\n",
    "    \"\"\"\n",
    "    tables = dag[\"tables\"]\n",
    "    \n",
    "    # Filter by enabled status\n",
    "    if only_enabled:\n",
    "        tables = get_enabled_tables(dag)\n",
    "    \n",
    "    # Filter by retry list if provided\n",
    "    if retry_tables:\n",
    "        tables = filter_retry_tables(tables, retry_tables)\n",
    "    \n",
    "    return tables\n",
    "\n",
    "\n",
    "logger.info(\"✓ Table filtering functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## [5] Watermark Management (READ-ONLY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": "# Import watermark functions from module\nfrom modules.config_utils import (\n    load_watermarks,\n    get_source_watermarks,\n    get_table_watermark\n)\n\nlogger.info(\"✓ Watermark functions imported from modules.config_utils (READ-ONLY)\")\nlogger.info(\"⚠️  NOTE: Watermarks are managed by extraction pipeline, not by notebooks\")"
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## [6] Path Building Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": "# Import path building utilities from module\nfrom modules.config_utils import (\n    build_bronze_table_name,\n    build_silver_table_name\n)\n\nlogger.info(\"✓ Path building functions imported from modules.config_utils\")"
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## [7] DAG Query Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": "# Import DAG query helpers from module\nfrom modules.config_utils import (\n    get_dag_metadata,\n    get_business_keys,\n    get_incremental_column,\n    get_window_config,\n    get_partitioning_config,\n    summarize_dag\n)\n\nlogger.info(\"✓ DAG query helper functions imported from modules.config_utils\")"
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## [8] Runplan Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": "# Import runplan management functions from module\nfrom modules.config_utils import (\n    load_runplan,\n    get_source_schedule\n)\n\nlogger.info(\"✓ Runplan functions imported from modules.config_utils\")"
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## [9] Verification and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logger.info(\"=\" * 80)\n",
    "# logger.info(\"CONFIGURATION UTILITIES VERIFICATION\")\n",
    "# logger.info(\"=\" * 80)\n",
    "\n",
    "# # Test environment detection\n",
    "# logger.info(f\"\\n1. Environment Detection:\")\n",
    "# logger.info(f\"   Base path: {BASE_PATH}\")\n",
    "# logger.info(f\"   Environment: {'Fabric' if '/lakehouse' in BASE_PATH else 'Local'}\")\n",
    "\n",
    "# # Test config paths\n",
    "# logger.info(f\"\\n2. Configuration Paths:\")\n",
    "# logger.info(f\"   Config dir: {CONFIG_DIR}\")\n",
    "# logger.info(f\"   Watermarks: {WATERMARKS_PATH}\")\n",
    "# logger.info(f\"   Runplan: {RUNPLAN_PATH}\")\n",
    "\n",
    "# # Check if config files exist\n",
    "# logger.info(f\"\\n3. Config Files Status:\")\n",
    "# logger.info(f\"   Config dir exists: {os.path.exists(CONFIG_DIR)}\")\n",
    "# logger.info(f\"   Watermarks exists: {os.path.exists(WATERMARKS_PATH)}\")\n",
    "# logger.info(f\"   Runplan exists: {os.path.exists(RUNPLAN_PATH)}\")\n",
    "\n",
    "# # Try to list DAG files\n",
    "# if os.path.exists(CONFIG_DIR):\n",
    "#     dag_files = [f for f in os.listdir(CONFIG_DIR) if f.startswith('dag_') and f.endswith('.json')]\n",
    "#     logger.info(f\"\\n4. Available DAG files: {len(dag_files)}\")\n",
    "#     for dag_file in sorted(dag_files)[:5]:  # Show first 5\n",
    "#         logger.info(f\"   - {dag_file}\")\n",
    "#     if len(dag_files) > 5:\n",
    "#         logger.info(f\"   ... and {len(dag_files) - 5} more\")\n",
    "\n",
    "# logger.info(\"\\n✓ Configuration utilities ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## [10] Example Usage\n",
    "\n",
    "Example of how to use these configuration functions in other notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Load a DAG and get tables to process\n",
    "# \n",
    "# dag_path = f\"{CONFIG_DIR}/dag_vizier_weekday.json\"\n",
    "# dag = load_dag(dag_path)\n",
    "# \n",
    "# # Get all enabled tables\n",
    "# tables = get_tables_to_process(dag)\n",
    "# logger.info(f\"Found {len(tables)} enabled tables\")\n",
    "# \n",
    "# # Get only incremental tables\n",
    "# incremental_tables = get_tables_by_load_mode(tables, \"incremental\")\n",
    "# logger.info(f\"Found {len(incremental_tables)} incremental tables\")\n",
    "# \n",
    "# # Get metadata\n",
    "# metadata = get_dag_metadata(dag)\n",
    "# logger.info(f\"Source: {metadata['source']}\")\n",
    "# logger.info(f\"Base files: {metadata['base_files']}\")\n",
    "# \n",
    "# # Build parquet path for a table\n",
    "# table = tables[0]\n",
    "# parquet_path = build_parquet_path(\n",
    "#     source=metadata['source'],\n",
    "#     run_ts=\"20251105T120000123\",\n",
    "#     table_name=table['name'],\n",
    "#     base_files=metadata['base_files']\n",
    "# )\n",
    "# logger.info(f\"Parquet path: {parquet_path}\")\n",
    "# \n",
    "# # Get watermark for incremental table\n",
    "# if table.get('load_mode') == 'incremental':\n",
    "#     watermark = get_table_watermark(metadata['source'], table['name'])\n",
    "#     logger.info(f\"Watermark for {table['name']}: {watermark}\")\n",
    "\n",
    "#logger.info(\"✓ Example usage documented (commented out)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}