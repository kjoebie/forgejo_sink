{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# 00 ‚Äî Master Orchestrator: Bronze ‚Üí Silver Processing\n",
    "\n",
    "Main orchestration notebook for processing parquet files through Bronze and Silver layers.\n",
    "\n",
    "## Architecture Overview\n",
    "\n",
    "```\n",
    "Parquet Files (Files/{source}/{run_ts}/)\n",
    "    ‚Üì\n",
    "Bronze Layer (append with run_ts for CDC)\n",
    "    ‚Üì\n",
    "Silver Layer (CDC merge: INSERT/UPDATE/DELETE)\n",
    "    ‚Üì\n",
    "Watermark Update (incremental tables only)\n",
    "```\n",
    "\n",
    "## Process Flow\n",
    "\n",
    "1. **Load Configuration** (DAG, enabled tables, retry filter)\n",
    "2. **Check Incremental** ‚Üí Run watermark merge if needed\n",
    "3. **Bronze Processing** ‚Üí Parallel table loading (10 workers)\n",
    "4. **Bronze Logging** ‚Üí Batch log all results\n",
    "5. **Silver Processing** ‚Üí Parallel CDC merge (tables with business_keys)\n",
    "6. **Silver Logging** ‚Üí Batch log all results\n",
    "7. **Summary Statistics** ‚Üí Performance metrics, efficiency\n",
    "\n",
    "## Key Features\n",
    "\n",
    "- **Parallel Processing**: ThreadPoolExecutor for 5-10x speedup\n",
    "- **Idempotency**: Check logs before reprocessing\n",
    "- **Retry Support**: Process only specific tables\n",
    "- **Error Resilience**: Continue on failure, comprehensive logging\n",
    "- **Performance Tracking**: Efficiency metrics (theoretical vs actual time)\n",
    "\n",
    "## Parameters\n",
    "\n",
    "- `source`: Source system name (e.g., \"vizier\")\n",
    "- `run_ts`: Run timestamp (e.g., \"20251105T142752505\")\n",
    "- `dag_path`: Path to DAG configuration JSON\n",
    "- `retry_tables`: Optional list of tables to retry\n",
    "- `force_reload`: Ignore log and reload all\n",
    "- `max_workers`: Parallel workers (default: 10)\n",
    "- `debug`: Enable debug output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters (Papermill compatible)\n",
    "source = \"anva_meeus\"                               # Source system name\n",
    "run_ts = \"20251001T183103260\"                       # Run timestamp\n",
    "dag_path = \"config/dag_anva_meeus_week.json\"        # DAG configuration path\n",
    "retry_tables = None                                 # Optional: list of table names to retry\n",
    "force_reload = True                                 # If True, ignore logs and reload all\n",
    "debug = True                                        # Enable debug output\n",
    "log_to_console = True                               # Also stream logs to stdout/stderr\n",
    "optimize_for = \"throughput\"                         # Worker profile optimization goal, choose throughput or efficiency\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## [1] Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 15:07:08,390 [INFO] - Logfile: /data/lakehouse/gh_b_avd/lh_gh_bronze/Files/notebook_outputs/logs/master_orchestrator_20251203_145516.log\n",
      "2025-12-03 15:07:08,391 [INFO] - ================================================================================\n",
      "2025-12-03 15:07:08,392 [INFO] - MASTER ORCHESTRATOR STARTING\n",
      "2025-12-03 15:07:08,392 [INFO] - ================================================================================\n",
      "2025-12-03 15:07:08,392 [INFO] - Source: anva_meeus\n",
      "2025-12-03 15:07:08,393 [INFO] - Run TS: 20251001T183103260\n",
      "2025-12-03 15:07:08,393 [INFO] - DAG: config/dag_anva_meeus_week.json\n",
      "2025-12-03 15:07:08,393 [INFO] - Retry tables: None\n",
      "2025-12-03 15:07:08,393 [INFO] - Force reload: True\n",
      "2025-12-03 15:07:08,394 [INFO] - Debug: True\n",
      "2025-12-03 15:07:08,394 [INFO] - ================================================================================\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timezone\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from uuid import uuid4\n",
    "\n",
    "from modules.logging_utils import configure_logging\n",
    "import logging\n",
    "from modules.worker_utils import choose_worker_profile_from_history\n",
    "\n",
    "log_file = configure_logging(run_name=\"master_orchestrator\", enable_console_logging=log_to_console)\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.info(\"Logfile: %s\", log_file)\n",
    "\n",
    "logger.info(\"=\"*80)\n",
    "logger.info(\"MASTER ORCHESTRATOR STARTING\")\n",
    "logger.info(\"=\"*80)\n",
    "logger.info(f\"Source: {source}\")\n",
    "logger.info(f\"Run TS: {run_ts}\")\n",
    "logger.info(f\"DAG: {dag_path}\")\n",
    "logger.info(f\"Retry tables: {retry_tables}\")\n",
    "logger.info(f\"Force reload: {force_reload}\")\n",
    "logger.info(f\"Debug: {debug}\")\n",
    "logger.info(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## [2] Load Utility Notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all required utilities from modules\n",
    "from modules.config_utils import (\n",
    "    load_dag, get_enabled_tables, get_tables_to_process,\n",
    "    get_tables_by_load_mode, get_dag_metadata, summarize_dag,\n",
    "    get_business_keys\n",
    ")\n",
    "\n",
    "from modules.logging_utils import (\n",
    "    build_run_date,\n",
    "    get_successful_tables,\n",
    "    log_batch,\n",
    "    log_summary\n",
    ")\n",
    "\n",
    "from modules.path_utils import get_base_path\n",
    "\n",
    "# Import worker functions directly\n",
    "from modules.bronze_processor import process_bronze_table\n",
    "from modules.silver_processor import process_silver_cdc_merge\n",
    "\n",
    "logger.info(\"‚úì Utility functions imported from modules\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 15:07:08,421 [INFO] - ‚úì Using existing Spark session\n",
      "2025-12-03 15:07:08,422 [INFO] -   Spark version: 3.5.5\n",
      "2025-12-03 15:07:08,423 [INFO] -   Application ID: app-20251203145518-0867\n",
      "2025-12-03 15:07:08,423 [INFO] -   Application name: DWH_Bronze_Silver_Processing\n"
     ]
    }
   ],
   "source": [
    "from modules.spark_session import get_or_create_spark_session\n",
    "\n",
    "spark = get_or_create_spark_session(\n",
    "    app_name=\"DWH_Bronze_Silver_Processing\",\n",
    "    enable_hive=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 15:07:08,448 [INFO] - ‚úì Logging utilities imported from modules (notebook 01 no longer needed)\n"
     ]
    }
   ],
   "source": [
    "# Notebook 01 no longer needed - all functions imported from modules\n",
    "logger.info(\"‚úì Logging utilities imported from modules (notebook 01 no longer needed)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 15:07:08,468 [INFO] - ‚úì All utilities imported from modules (notebook 02 no longer needed)\n"
     ]
    }
   ],
   "source": [
    "# This cell is no longer needed - all functions are imported from modules\n",
    "logger.info(\"‚úì All utilities imported from modules (notebook 02 no longer needed)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## [3] Load DAG Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 15:07:08,489 [INFO] - \n",
      "üìã Loading DAG configuration...\n",
      "2025-12-03 15:07:09,483 [INFO] -   Base Files path: /data/lakehouse/gh_b_avd/lh_gh_bronze/Files\n",
      "2025-12-03 15:07:09,486 [INFO] - ‚úì DAG loaded: anva_meeus\n",
      "2025-12-03 15:07:09,486 [INFO] -   Base files: greenhouse_sources\n",
      "2025-12-03 15:07:09,494 [INFO] - Schemas ensured: anva_meeus\n",
      "2025-12-03 15:07:09,495 [INFO] - \n",
      "üìä Tables to process: 58\n",
      "2025-12-03 15:07:09,495 [INFO] -   Total enabled: 58\n",
      "2025-12-03 15:07:09,496 [INFO] -   Load modes: {'snapshot': 57, 'window': 1}\n"
     ]
    }
   ],
   "source": [
    "# Load and validate DAG\n",
    "logger.info(f\"\\nüìã Loading DAG configuration...\")\n",
    "\n",
    "# Get base path for Files directory (environment-aware)\n",
    "base_files_path = get_base_path(spark)\n",
    "logger.info(f\"  Base Files path: {base_files_path}\")\n",
    "\n",
    "# Load DAG (handles both absolute and relative paths)\n",
    "dag = load_dag(dag_path, base_path=base_files_path)\n",
    "logger.info(f\"‚úì DAG loaded: {dag.get('source')}\")\n",
    "\n",
    "# Get metadata\n",
    "dag_metadata = get_dag_metadata(dag)\n",
    "base_files = dag_metadata['base_files']\n",
    "\n",
    "logger.info(f\"  Base files: {base_files}\")\n",
    "\n",
    "# Get tables to process\n",
    "tables_to_process = get_tables_to_process(\n",
    "    dag=dag,\n",
    "    retry_tables=retry_tables,\n",
    "    only_enabled=True\n",
    ")\n",
    "\n",
    "# Ensure schemas exist        \n",
    "schemas = set()\n",
    "\n",
    "for t in tables_to_process:\n",
    "    delta_table = t.get(\"delta_table\")\n",
    "    delta_schema = t.get(\"delta_schema\")\n",
    "\n",
    "    if delta_table and \".\" in delta_table:\n",
    "        # Vorm: schema.tabel in delta_table\n",
    "        schema = delta_table.split(\".\")[0]\n",
    "    else:\n",
    "        # Anders: gebruik delta_schema of standaard 'bronze'\n",
    "        schema = (delta_schema or \"bronze\")\n",
    "\n",
    "    schemas.add(schema)\n",
    "\n",
    "for schema in sorted(schemas):\n",
    "    spark.sql(f\"CREATE SCHEMA IF NOT EXISTS `{schema}`\")\n",
    "\n",
    "logger.info(\"Schemas ensured: %s\", \", \".join(sorted(schemas)))\n",
    "\n",
    "logger.info(f\"\\nüìä Tables to process: {len(tables_to_process)}\")\n",
    "\n",
    "# Show summary\n",
    "dag_summary = summarize_dag(dag)\n",
    "logger.info(f\"  Total enabled: {dag_summary['enabled_tables']}\")\n",
    "logger.info(f\"  Load modes: {dag_summary['load_mode_counts']}\")\n",
    "\n",
    "if not tables_to_process:\n",
    "    logger.info(\"\\n‚ö†Ô∏è  No tables to process. Exiting.\")\n",
    "    raise SystemExit(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## [4] Generate Run ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 15:07:09,500 [INFO] - \n",
      "üÜî Run ID: 20251001T183103260_2dc7d8f3\n"
     ]
    }
   ],
   "source": [
    "# Generate unique run ID\n",
    "RUN_ID = f\"{run_ts}_{uuid4().hex[:8]}\"\n",
    "run_date = build_run_date(run_ts)\n",
    "logger.info(f\"\\nüÜî Run ID: {RUN_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## [5] Check for Incremental Tables (Watermark Merge)\n",
    "\n",
    "If incremental tables are present, run watermark merge notebook.\n",
    "This must happen BEFORE Bronze loading starts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 15:07:09,528 [INFO] - \n",
      "üíß Checking for incremental tables...\n",
      "2025-12-03 15:07:09,530 [INFO] -   ‚óØ No incremental tables - skipping watermark merge\n",
      "2025-12-03 15:07:09,530 [INFO] - ================================================================================\n"
     ]
    }
   ],
   "source": [
    "logger.info(f\"\\nüíß Checking for incremental tables...\")\n",
    "\n",
    "# Filter incremental tables\n",
    "incremental_tables = get_tables_by_load_mode(tables_to_process, \"incremental\")\n",
    "\n",
    "if len(incremental_tables) > 0:\n",
    "    logger.info(f\"  Found {len(incremental_tables)} incremental tables\")\n",
    "    logger.info(f\"  Tables: {[t['name'] for t in incremental_tables[:5]]}\")\n",
    "    \n",
    "    # Get watermarks path from DAG\n",
    "    wm_configpath = dag_metadata.get('watermarks_path', 'config/watermarks.json')\n",
    "    \n",
    "    # Build watermark folder path (where extraction pipeline writes watermarks)\n",
    "    wm_folder = f\"runtime/{source}/{run_ts}/\"\n",
    "    \n",
    "    logger.info(f\"  Config: {wm_configpath}\")\n",
    "    logger.info(f\"  Runtime folder: {wm_folder}\")\n",
    "    \n",
    "    # Note: In Fabric, this would use mssparkutils.notebook.run()\n",
    "    # For local testing, we skip watermark merge (not critical for Bronze/Silver testing)\n",
    "    logger.info(f\"\\n  ‚ö†Ô∏è  Watermark merge would run here (11_bronze_watermark_merge.ipynb)\")\n",
    "    logger.info(f\"     Skipping for now - watermarks managed by extraction pipeline\")\n",
    "else:\n",
    "    logger.info(f\"  ‚óØ No incremental tables - skipping watermark merge\")\n",
    "\n",
    "logger.info(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## [6] Bronze Processing (Parallel)\n",
    "\n",
    "Load all tables from parquet to Bronze Delta tables in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 15:07:09,554 [INFO] - ‚úì Bronze worker imported from modules (notebook 10 no longer needed)\n"
     ]
    }
   ],
   "source": [
    "# Notebook 10 no longer needed - process_bronze_table imported from modules\n",
    "logger.info(\"‚úì Bronze worker imported from modules (notebook 10 no longer needed)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 15:07:09,574 [INFO] - \n",
      "üîµ BRONZE: Loading parquet to Delta tables...\n",
      "2025-12-03 15:07:09,575 [INFO] -   Tables: 58\n",
      "2025-12-03 15:07:09,576 [INFO] -   ‚ö†Ô∏è  Force reload enabled - processing all tables\n",
      "2025-12-03 15:07:09,576 [INFO] - \n",
      "  üöÄ Processing 58 tables in parallel...\n",
      "\n",
      "2025-12-03 15:07:10,374 [INFO] - [WORKER_OPTIMIZER] source=anva_meeus, median_rows=3,939,267, last_workers=8, target=8, new_workers=8, best_throughput=189093 throughput (rows/s)\n",
      "2025-12-03 15:07:10,374 [INFO] - Using MAX_WORKERS=8 for bronze processing\n",
      "25/12/03 15:07:13 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "2025-12-03 15:07:14,284 [INFO] - [1/58]‚úì Dim_Agent                      SUCCESS         3,193 rows \n",
      "2025-12-03 15:07:14,286 [INFO] - [2/58]‚úì Dim_Collectiviteit             SUCCESS        10,212 rows \n",
      "2025-12-03 15:07:14,290 [INFO] - [3/58]‚úì Dim_DekkingVariabel            SUCCESS        94,002 rows \n",
      "2025-12-03 15:07:14,293 [INFO] - [4/58]‚úì Dim_FactuurSoort               SUCCESS            24 rows \n",
      "2025-12-03 15:07:14,299 [INFO] - [5/58]‚úì Dim_Calamiteit                 SUCCESS            14 rows \n",
      "2025-12-03 15:07:14,302 [INFO] - [6/58]‚úì Dim_DetailMaatschappij         SUCCESS         2,289 rows \n",
      "2025-12-03 15:07:14,309 [INFO] - [7/58]‚úì Dim_Branche                    SUCCESS           617 rows \n",
      "2025-12-03 15:07:14,311 [INFO] - [8/58]‚úì Dim_DekkingCode                SUCCESS         2,575 rows \n",
      "2025-12-03 15:07:17,264 [INFO] - [9/58]‚úì Dim_MeldingRDW                 SUCCESS             7 rows \n",
      "2025-12-03 15:07:17,444 [INFO] - [10/58]‚úì Dim_HoofdBranche               SUCCESS            23 rows \n",
      "2025-12-03 15:07:17,452 [INFO] - [11/58]‚úì Dim_Incassowijze               SUCCESS           168 rows \n",
      "2025-12-03 15:07:17,495 [INFO] - [12/58]‚úì Dim_PolisProducent             SUCCESS         1,473 rows \n",
      "2025-12-03 15:07:17,503 [INFO] - [13/58]‚úì Dim_Kantoor                    SUCCESS             3 rows \n",
      "2025-12-03 15:07:17,512 [INFO] - [14/58]‚úì Dim_Maatschappij               SUCCESS         2,289 rows \n",
      "2025-12-03 15:07:17,615 [INFO] - [15/58]‚úì Dim_Medewerker                 SUCCESS         9,861 rows \n",
      "2025-12-03 15:07:20,043 [INFO] - [16/58]‚óØ Dim_PolisVariabel_VrijeLabels  EMPTY               0 rows Parquet exists but contains 0 rows\n",
      "2025-12-03 15:07:20,113 [INFO] - [17/58]‚úì Dim_PolisVariabel              SUCCESS       480,490 rows \n",
      "2025-12-03 15:07:20,145 [INFO] - [18/58]‚úì Dim_Polisvoorwaarden           SUCCESS         8,738 rows \n",
      "2025-12-03 15:07:20,321 [INFO] - [19/58]‚úì Dim_RelatieProducent           SUCCESS         1,473 rows \n",
      "2025-12-03 15:07:20,368 [INFO] - [20/58]‚úì Dim_RelatieSoort               SUCCESS            16 rows \n",
      "2025-12-03 15:07:20,420 [INFO] - [21/58]‚óØ Dim_RelatieVariabel_VrijeLabels EMPTY               0 rows Parquet exists but contains 0 rows\n",
      "2025-12-03 15:07:23,394 [INFO] - [22/58]‚úì Dim_Verzekeringsvorm           SUCCESS           982 rows \n",
      "2025-12-03 15:07:23,403 [INFO] - [23/58]‚úì Dim_RelatieVariabel            SUCCESS       554,497 rows \n",
      "2025-12-03 15:07:23,404 [INFO] - [24/58]‚úì Dim_SBICode                    SUCCESS           956 rows \n",
      "2025-12-03 15:07:23,441 [INFO] - [25/58]‚úì Dim_SchadeCode                 SUCCESS            17 rows \n",
      "2025-12-03 15:07:23,448 [INFO] - [26/58]‚úì Dim_SchadeSoort                SUCCESS           609 rows \n",
      "2025-12-03 15:07:23,451 [INFO] - [27/58]‚úì Dim_Tekenjaar                  SUCCESS            27 rows \n",
      "2025-12-03 15:07:23,727 [INFO] - [28/58]‚úì Dim_Schade                     SUCCESS         3,659 rows \n",
      "2025-12-03 15:07:23,844 [INFO] - [29/58]‚úì Dim_Relatie                    SUCCESS       554,498 rows \n",
      "2025-12-03 15:07:25,696 [INFO] - [30/58]‚úì Dim_Wijzigingsreden            SUCCESS            83 rows \n",
      "2025-12-03 15:07:26,090 [INFO] - [31/58]‚óØ Fact_Agent_Borderel            EMPTY               0 rows Parquet exists but contains 0 rows\n",
      "2025-12-03 15:07:26,154 [INFO] - [32/58]‚óØ Fact_Agent_RekeningCourant     EMPTY               0 rows Parquet exists but contains 0 rows\n",
      "2025-12-03 15:07:26,231 [INFO] - [33/58]‚óØ Fact_Cashbroker                EMPTY               0 rows Parquet exists but contains 0 rows\n",
      "2025-12-03 15:07:26,272 [INFO] - [34/58]‚óØ Fact_Agent_RekeningCourantDetail EMPTY               0 rows Parquet exists but contains 0 rows\n",
      "2025-12-03 15:07:26,286 [INFO] - [35/58]‚úì Fact_Agenda                    SUCCESS         9,623 rows \n",
      "2025-12-03 15:07:27,107 [INFO] - [36/58]‚úì Fact_Betalingen                SUCCESS       129,986 rows \n",
      "2025-12-03 15:07:27,557 [INFO] - [37/58]‚úì Fact_Dekking                   SUCCESS       246,052 rows \n",
      "2025-12-03 15:07:28,137 [INFO] - [38/58]‚óØ Fact_KentekensRDW              EMPTY               0 rows Parquet exists but contains 0 rows\n",
      "2025-12-03 15:07:28,241 [INFO] - [39/58]‚úì Fact_MedewerkerPermissies      SUCCESS         9,861 rows \n",
      "2025-12-03 15:07:28,580 [INFO] - [40/58]‚úì Fact_OpenstaandeFacturen       SUCCESS         1,612 rows \n",
      "2025-12-03 15:07:28,596 [INFO] - [41/58]‚óØ Fact_OpenstaandeFacturen_Cashbrokers EMPTY               0 rows Parquet exists but contains 0 rows\n",
      "2025-12-03 15:07:28,665 [INFO] - [42/58]‚úì Fact_Pakket                    SUCCESS        10,408 rows \n",
      "2025-12-03 15:07:29,239 [INFO] - [43/58]‚úì Fact_Dekking_Detail            SUCCESS       266,648 rows \n",
      "2025-12-03 15:07:29,693 [INFO] - [44/58]‚úì Fact_PoolInrichting            SUCCESS        23,870 rows \n",
      "2025-12-03 15:07:31,851 [INFO] - [45/58]‚óØ Fact_Portefeuillestand_hist    EMPTY               0 rows Parquet exists but contains 0 rows\n",
      "2025-12-03 15:07:31,920 [INFO] - [46/58]‚úì Fact_PremieReserveBeurs        SUCCESS            36 rows \n",
      "2025-12-03 15:07:32,442 [INFO] - [47/58]‚úì Fact_Polis                     SUCCESS       233,825 rows \n",
      "2025-12-03 15:07:32,445 [INFO] - [48/58]‚úì Fact_PremieReserve             SUCCESS        98,913 rows \n",
      "2025-12-03 15:07:32,685 [INFO] - [49/58]‚úì Fact_PremieFacturen            SUCCESS       106,697 rows \n",
      "2025-12-03 15:07:32,908 [INFO] - [50/58]‚úì Fact_Productie                 SUCCESS        17,103 rows \n",
      "2025-12-03 15:07:34,471 [INFO] - [51/58]‚úì Fact_Portefeuillestand         SUCCESS       935,308 rows \n",
      "2025-12-03 15:07:34,703 [INFO] - [52/58]‚úì Fact_WachtDekking              SUCCESS         1,008 rows \n",
      "2025-12-03 15:07:34,771 [INFO] - [53/58]‚úì Fact_SchadeFacturen            SUCCESS        12,303 rows \n",
      "2025-12-03 15:07:34,774 [INFO] - [54/58]‚úì Fact_Schadeboekingen           SUCCESS        88,265 rows \n",
      "2025-12-03 15:07:35,050 [INFO] - [55/58]‚úì Facturatiegroep                SUCCESS             1 rows \n",
      "2025-12-03 15:07:35,054 [INFO] - [56/58]‚úì Fact_Werklijst                 SUCCESS        13,724 rows \n",
      "2025-12-03 15:07:35,303 [INFO] - [57/58]‚úì Landcode                       SUCCESS           249 rows \n",
      "2025-12-03 15:07:35,361 [INFO] - [58/58]‚úì Fact_WachtPolis                SUCCESS           980 rows \n",
      "2025-12-03 15:07:35,362 [INFO] - Closing down clientserver connection\n",
      "2025-12-03 15:07:35,362 [INFO] - Closing down clientserver connection\n",
      "2025-12-03 15:07:35,362 [INFO] - Closing down clientserver connection\n",
      "2025-12-03 15:07:35,362 [INFO] - Closing down clientserver connection\n",
      "2025-12-03 15:07:35,362 [INFO] - Closing down clientserver connection\n",
      "2025-12-03 15:07:35,363 [INFO] - Closing down clientserver connection\n",
      "2025-12-03 15:07:35,363 [INFO] - Closing down clientserver connection\n",
      "2025-12-03 15:07:35,363 [INFO] - Closing down clientserver connection\n",
      "2025-12-03 15:07:35,368 [INFO] - \n",
      "‚úì Bronze processing completed in 25.792331s\n"
     ]
    }
   ],
   "source": [
    "logger.info(f\"\\nüîµ BRONZE: Loading parquet to Delta tables...\")\n",
    "logger.info(f\"  Tables: {len(tables_to_process)}\")\n",
    "\n",
    "bronze_start = datetime.now(timezone.utc)\n",
    "bronze_results = []\n",
    "\n",
    "# Filter tables if not force_reload (check logs)\n",
    "if not force_reload:\n",
    "    logger.info(f\"\\n  üìã Checking logs for already processed tables...\")\n",
    "    \n",
    "    # Get successfully processed tables from log\n",
    "    processed_tables = get_successful_tables(spark, run_ts, layer=\"bronze\")\n",
    "    \n",
    "    if processed_tables:\n",
    "        logger.info(f\"    Found {len(processed_tables)} already processed tables\")\n",
    "        \n",
    "        # Filter out already processed\n",
    "        tables_to_process_bronze = [\n",
    "            t for t in tables_to_process \n",
    "            if t['name'] not in processed_tables\n",
    "        ]\n",
    "\n",
    "        logger.info(f\"    Remaining: {len(tables_to_process_bronze)} tables\")\n",
    "    else:\n",
    "        tables_to_process_bronze = tables_to_process\n",
    "else:\n",
    "    tables_to_process_bronze = tables_to_process\n",
    "    logger.info(f\"  ‚ö†Ô∏è  Force reload enabled - processing all tables\")\n",
    "\n",
    "if not tables_to_process_bronze:\n",
    "    logger.info(f\"\\n  ‚úì All tables already processed for this run_ts\")\n",
    "else:\n",
    "    logger.info(f\"\\n  üöÄ Processing {len(tables_to_process_bronze)} tables in parallel...\\n\")\n",
    "    \n",
    "    # Wrapper function for parallel execution\n",
    "    def process_table_wrapper(table_def):\n",
    "        \"\"\"Wrapper to catch exceptions and always return a result.\"\"\"\n",
    "        try:\n",
    "            return process_bronze_table(\n",
    "                spark=spark,\n",
    "                table_def=table_def,\n",
    "                source_name=source,\n",
    "                run_id=RUN_ID,\n",
    "                run_ts=run_ts,\n",
    "                run_date=run_date,\n",
    "                base_files=base_files,\n",
    "                debug=False  # Disable per-table debug in parallel mode\n",
    "            )\n",
    "        except Exception as e:\n",
    "            # If worker throws unhandled exception, create error result\n",
    "            return {\n",
    "                \"log_id\": f\"{source}:{table_def['name']}:{run_ts}:error\",\n",
    "                \"run_id\": RUN_ID,\n",
    "                \"run_date\": run_date,\n",
    "                \"run_ts\": run_ts,\n",
    "                \"source\": source,\n",
    "                \"table_name\": table_def['name'],\n",
    "                \"load_mode\": table_def.get('load_mode'),\n",
    "                \"status\": \"FAILED\",\n",
    "                \"rows_read\": None,\n",
    "                \"rows_processed\": None,\n",
    "                \"start_time\": datetime.now(timezone.utc),\n",
    "                \"end_time\": datetime.now(timezone.utc),\n",
    "                \"duration_seconds\": 0,\n",
    "                \"error_message\": f\"Unhandled exception: {str(e)[:500]}\",\n",
    "                \"parquet_path\": None,\n",
    "                \"delta_table\": None,\n",
    "            }\n",
    "    \n",
    "    # Optimize voor throughput (snelheid)\n",
    "    MAX_WORKERS = choose_worker_profile_from_history(\n",
    "        spark=spark,\n",
    "        source_name=source,\n",
    "        summary_table=\"logs.bronze_run_summary\",\n",
    "        default_workers=10,\n",
    "        min_workers=2,\n",
    "        max_workers_cap=12,\n",
    "        lookback_runs=5,\n",
    "        optimize_for=optimize_for,  # Focus on rows/second\n",
    "        debug=debug\n",
    "    )\n",
    "    # Cap on number of tables\n",
    "    MAX_WORKERS = min(MAX_WORKERS, len(tables_to_process_bronze))\n",
    "    logger.info(f\"Using MAX_WORKERS={MAX_WORKERS} for bronze processing\")\n",
    "\n",
    "\n",
    "    # Parallel execution\n",
    "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        futures = {\n",
    "            executor.submit(process_table_wrapper, table): table \n",
    "            for table in tables_to_process_bronze\n",
    "        }\n",
    "        \n",
    "        completed = 0\n",
    "        for future in as_completed(futures):\n",
    "            result = future.result()\n",
    "            bronze_results.append(result)\n",
    "            completed += 1\n",
    "            \n",
    "            # Progress indicator\n",
    "            status_icon = \"‚úì\" if result['status'] == 'SUCCESS' else \"‚úó\" if result['status'] == 'FAILED' else \"‚óØ\"\n",
    "            # Kort foutfragment erbij (max 120 chars, 1 regel)\n",
    "            error_snippet = (result.get(\"error_message\") or \"\")[:120].replace(\"\\n\", \" \")\n",
    "            \n",
    "            logger.info(\n",
    "                f\"[{completed}/{len(tables_to_process_bronze)}]\"\n",
    "                f\"{status_icon} {result['table_name']:<30} {result['status']:<10} \"\n",
    "                f\"{(result.get('rows_processed') or 0):>10,} rows {error_snippet}\"\n",
    "                )\n",
    "\n",
    "bronze_end = datetime.now(timezone.utc)\n",
    "bronze_duration = float((bronze_end - bronze_start).total_seconds())\n",
    "\n",
    "logger.info(f\"\\n‚úì Bronze processing completed in {bronze_duration}s\")\n",
    "\n",
    "#sys.exit(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## [7] Bronze Logging and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 15:07:35,375 [INFO] - \n",
      "üìä Logging Bronze results...\n",
      "2025-12-03 15:07:37,398 [INFO] - ‚úì Logged Bronze summary to logs.bronze_run_summary\n",
      "2025-12-03 15:07:37,944 [INFO] - ‚úì Logged 58 Bronze records to logs.bronze_processing_log\n",
      "2025-12-03 15:07:37,944 [INFO] - \n",
      "  Summary:\n",
      "2025-12-03 15:07:37,945 [INFO] -     Success: 49\n",
      "2025-12-03 15:07:37,945 [INFO] -     Failed:  0\n",
      "2025-12-03 15:07:37,945 [INFO] -     Empty:   9\n",
      "2025-12-03 15:07:37,946 [INFO] -     Skipped: 0\n",
      "2025-12-03 15:07:37,946 [INFO] -     Total rows: 3,939,267\n",
      "2025-12-03 15:07:37,946 [INFO] -     Efficiency: 79.5%\n"
     ]
    }
   ],
   "source": [
    "if bronze_results:\n",
    "    logger.info(f\"\\nüìä Logging Bronze results...\")\n",
    "        \n",
    "    # Calculate summary statistics\n",
    "    success_count = sum(1 for r in bronze_results if r['status'] == 'SUCCESS')\n",
    "    failed_count = sum(1 for r in bronze_results if r['status'] == 'FAILED')\n",
    "    empty_count = sum(1 for r in bronze_results if r['status'] == 'EMPTY')\n",
    "    skipped_count = sum(1 for r in bronze_results if r['status'] == 'SKIPPED')\n",
    "    \n",
    "    total_rows = sum(r.get('rows_processed', 0) or 0 for r in bronze_results)\n",
    "    \n",
    "    # Performance metrics\n",
    "    sum_task_seconds = float(sum(r.get('duration_seconds', 0) or 0 for r in bronze_results))\n",
    "    theoretical_min_sec = float(sum_task_seconds / MAX_WORKERS if MAX_WORKERS > 0 else sum_task_seconds)\n",
    "    actual_time_sec = bronze_duration # float\n",
    "    efficiency_pct = float((theoretical_min_sec / actual_time_sec * 100) if actual_time_sec > 0 else 0)\n",
    "    \n",
    "    # Failed tables list\n",
    "    failed_tables = [r['table_name'] for r in bronze_results if r['status'] == 'FAILED']\n",
    "    \n",
    "    # Log summary\n",
    "    bronze_summary = {\n",
    "    \"run_id\": RUN_ID,\n",
    "    \"run_date\": run_date,\n",
    "    \"run_ts\": run_ts,\n",
    "    \"source\": source,\n",
    "    \"total_tables\": len(bronze_results),\n",
    "    \"tables_success\": success_count,\n",
    "    \"tables_empty\": empty_count,\n",
    "    \"tables_failed\": failed_count,\n",
    "    \"tables_skipped\": skipped_count,\n",
    "    \"total_rows\": total_rows,\n",
    "    \"workers\": MAX_WORKERS,\n",
    "    \"sum_task_seconds\": sum_task_seconds,\n",
    "    \"theoretical_min_sec\": theoretical_min_sec,\n",
    "    \"actual_time_sec\": actual_time_sec,\n",
    "    \"efficiency_pct\": efficiency_pct,\n",
    "    \"run_start\": bronze_start,\n",
    "    \"run_end\": bronze_end,\n",
    "    \"duration_seconds\": bronze_duration,\n",
    "    \"error_message\": None,\n",
    "    \"failed_tables\": failed_tables,\n",
    "    }\n",
    "\n",
    "    run_log_id = log_summary(spark, bronze_summary, layer=\"bronze\")\n",
    "\n",
    "    log_batch(spark, records=bronze_results, layer=\"bronze\", run_log_id=run_log_id)\n",
    "\n",
    "    \n",
    "    # Print summary\n",
    "    logger.info(f\"\\n  Summary:\")\n",
    "    logger.info(f\"    Success: {success_count}\")\n",
    "    logger.info(f\"    Failed:  {failed_count}\")\n",
    "    logger.info(f\"    Empty:   {empty_count}\")\n",
    "    logger.info(f\"    Skipped: {skipped_count}\")\n",
    "    logger.info(f\"    Total rows: {total_rows:,}\")\n",
    "    logger.info(f\"    Efficiency: {efficiency_pct:.1f}%\")\n",
    "    \n",
    "    if failed_tables:\n",
    "        logger.info(f\"\\n  ‚ö†Ô∏è  Failed tables: {failed_tables}\")\n",
    "else:\n",
    "    logger.info(f\"\\n  ‚ÑπÔ∏è  No Bronze results to log\")\n",
    "\n",
    "#sys.exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.sql(\"SHOW TABLES IN logs\").show(truncate=False)\n",
    "#spark.table(\"logs.bronze_processing_log\").printSchema()\n",
    "#spark.table(\"logs.bronze_run_summary\").printSchema()\n",
    "\n",
    "#spark.sql(\"drop table if exists logs.silver_run_summary\").show()\n",
    "#spark.sql(\"select * from logs.bronze_run_summary order by run_end desc limit 5\").show(truncate=False)\n",
    "\n",
    "#spark.sql(\"drop table if exists logs.bronze_processing_log\").show()\n",
    "#spark.sql(\"drop table if exists logs.bronze_run_summary\").show()\n",
    "\n",
    "# spark.sql(\"drop table if exists logs.silver_processing_log\").show()\n",
    "# spark.sql(\"drop table if exists logs.silver_run_summary\").show()\n",
    "# spark.table(\"logs.bronze_run_summary\") \\\n",
    "#       .orderBy(\"run_end\", \"source\") \\\n",
    "#       .show(20, truncate=False)\n",
    "\n",
    "# spark.table(\"logs.bronze_processing_log\") \\\n",
    "#       .orderBy(\"run_ts\", \"table_name\") \\\n",
    "#       .show(200, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## [8] Silver Processing (Parallel CDC Merge)\n",
    "\n",
    "Process tables that have business_keys defined for CDC merge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 15:07:37,990 [INFO] - ‚úì Silver CDC merge worker imported from modules (notebook 20 no longer needed)\n"
     ]
    }
   ],
   "source": [
    "# Notebook 20 no longer needed - process_silver_cdc_merge imported from modules\n",
    "logger.info(\"‚úì Silver CDC merge worker imported from modules (notebook 20 no longer needed)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 15:07:38,012 [INFO] - \n",
      "üî∑ SILVER: CDC merge from Bronze...\n",
      "2025-12-03 15:07:38,012 [INFO] -   Tables with business_keys: 0\n",
      "2025-12-03 15:07:38,013 [INFO] -   Successful Bronze loads: 49\n",
      "2025-12-03 15:07:38,013 [INFO] -   Tables to process in Silver: 0\n",
      "2025-12-03 15:07:38,013 [INFO] - \n",
      "  ‚ÑπÔ∏è  No tables to process in Silver\n"
     ]
    }
   ],
   "source": [
    "logger.info(f\"\\nüî∑ SILVER: CDC merge from Bronze...\")\n",
    "\n",
    "# Filter tables for Silver processing:\n",
    "# 1. Must have business_keys defined\n",
    "# 2. Must have been successfully loaded to Bronze\n",
    "\n",
    "successful_bronze_tables = [r['table_name'] for r in bronze_results if r['status'] == 'SUCCESS']\n",
    "\n",
    "tables_for_silver = [\n",
    "    t for t in tables_to_process \n",
    "    if t.get('business_keys') and t['name'] in successful_bronze_tables\n",
    "]\n",
    "\n",
    "logger.info(f\"  Tables with business_keys: {len([t for t in tables_to_process if t.get('business_keys')])}\")\n",
    "logger.info(f\"  Successful Bronze loads: {len(successful_bronze_tables)}\")\n",
    "logger.info(f\"  Tables to process in Silver: {len(tables_for_silver)}\")\n",
    "\n",
    "silver_results = []\n",
    "\n",
    "if not tables_for_silver:\n",
    "    logger.info(f\"\\n  ‚ÑπÔ∏è  No tables to process in Silver\")\n",
    "else:\n",
    "    silver_start = datetime.now(timezone.utc)\n",
    "    \n",
    "    logger.info(f\"\\n  üöÄ Processing {len(tables_for_silver)} tables in parallel...\\n\")\n",
    "    \n",
    "    # Wrapper function for parallel execution\n",
    "    def process_silver_wrapper(table_def):\n",
    "        \"\"\"Wrapper to catch exceptions and always return a result.\"\"\"\n",
    "        try:\n",
    "            return process_silver_cdc_merge(\n",
    "                spark=spark,\n",
    "                table_def=table_def,\n",
    "                source_name=source,\n",
    "                run_id=RUN_ID,\n",
    "                run_ts=run_ts,\n",
    "                debug=False\n",
    "            )\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"log_id\": f\"{source}:{table_def['name']}:{run_ts}:silver:error\",\n",
    "                \"run_id\": RUN_ID,\n",
    "                \"run_ts\": run_ts,\n",
    "                \"source\": source,\n",
    "                \"table_name\": table_def['name'],\n",
    "                \"load_mode\": table_def.get('load_mode'),\n",
    "                \"status\": \"FAILED\",\n",
    "                \"rows_inserted\": None,\n",
    "                \"rows_updated\": None,\n",
    "                \"rows_deleted\": None,\n",
    "                \"rows_unchanged\": None,\n",
    "                \"total_silver_rows\": None,\n",
    "                \"bronze_rows\": None,\n",
    "                \"bronze_table\": None,\n",
    "                \"silver_table\": None,\n",
    "                \"start_time\": datetime.now(timezone.utc),\n",
    "                \"end_time\": datetime.now(timezone.utc),\n",
    "                \"duration_seconds\": 0,\n",
    "                \"error_message\": f\"Unhandled exception: {str(e)[:500]}\",\n",
    "            }\n",
    "    \n",
    "    # Parallel execution\n",
    "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        futures = {\n",
    "            executor.submit(process_silver_wrapper, table): table \n",
    "            for table in tables_for_silver\n",
    "        }\n",
    "        \n",
    "        completed = 0\n",
    "        for future in as_completed(futures):\n",
    "            result = future.result()\n",
    "            silver_results.append(result)\n",
    "            completed += 1\n",
    "            \n",
    "            status_icon = \"‚úì\" if result['status'] == 'SUCCESS' else \"‚úó\"\n",
    "            deletes = result.get('rows_deleted', 0) or 0\n",
    "            delete_info = f\" ({deletes} deleted)\" if deletes > 0 else \"\"\n",
    "            logger.info(f\"    [{completed}/{len(tables_for_silver)}] {status_icon} {result['table_name']:<30} {result['status']:<10}{delete_info}\")\n",
    "            \n",
    "    \n",
    "    silver_end = datetime.now(timezone.utc)\n",
    "    silver_duration = int((silver_end - silver_start).total_seconds())\n",
    "    \n",
    "    logger.info(f\"\\n‚úì Silver processing completed in {silver_duration}s\")\n",
    "    #sys.exit(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "## [9] Silver Logging and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 15:07:38,036 [INFO] - \n",
      "  ‚ÑπÔ∏è  No Silver results to log\n"
     ]
    }
   ],
   "source": [
    "if silver_results:\n",
    "    logger.info(f\"\\nüìä Logging Silver results...\")\n",
    "    \n",
    "    # Batch log\n",
    "    log_batch(spark, records=silver_results, layer=\"silver\")\n",
    "    \n",
    "    # Calculate summary\n",
    "    success_count = sum(1 for r in silver_results if r['status'] == 'SUCCESS')\n",
    "    failed_count = sum(1 for r in silver_results if r['status'] == 'FAILED')\n",
    "    skipped_count = sum(1 for r in silver_results if r['status'] == 'SKIPPED')\n",
    "    \n",
    "    total_inserts = sum(r.get('rows_inserted', 0) or 0 for r in silver_results)\n",
    "    total_updates = sum(r.get('rows_updated', 0) or 0 for r in silver_results)\n",
    "    total_deletes = sum(r.get('rows_deleted', 0) or 0 for r in silver_results)\n",
    "    total_unchanged = sum(r.get('rows_unchanged', 0) or 0 for r in silver_results)\n",
    "    \n",
    "    failed_tables = [r['table_name'] for r in silver_results if r['status'] == 'FAILED']\n",
    "    \n",
    "    # Log summary\n",
    "    silver_summary = {\n",
    "        \"run_id\": RUN_ID,\n",
    "        \"source\": source,\n",
    "        \"run_ts\": run_ts,\n",
    "        \"run_start\": silver_start,\n",
    "        \"run_end\": silver_end,\n",
    "        \"duration_seconds\": silver_duration,\n",
    "        \"total_tables\": len(silver_results),\n",
    "        \"tables_success\": success_count,\n",
    "        \"tables_failed\": failed_count,\n",
    "        \"tables_skipped\": skipped_count,\n",
    "        \"total_inserts\": total_inserts,\n",
    "        \"total_updates\": total_updates,\n",
    "        \"total_deletes\": total_deletes,\n",
    "        \"total_unchanged\": total_unchanged,\n",
    "        \"failed_tables\": failed_tables,\n",
    "    }\n",
    "    \n",
    "    log_summary(spark, summary=silver_summary, layer=\"silver\")\n",
    "    \n",
    "    # Print summary\n",
    "    logger.info(f\"\\n  Summary:\")\n",
    "    logger.info(f\"    Success: {success_count}\")\n",
    "    logger.info(f\"    Failed:  {failed_count}\")\n",
    "    logger.info(f\"    Skipped: {skipped_count}\")\n",
    "    if total_inserts or total_updates or total_deletes:\n",
    "        logger.info(f\"    CDC: +{total_inserts or 0} ~{total_updates or 0} -{total_deletes}\")\n",
    "    \n",
    "    if failed_tables:\n",
    "        logger.info(f\"\\n  ‚ö†Ô∏è  Failed tables: {failed_tables}\")\n",
    "else:\n",
    "    logger.info(f\"\\n  ‚ÑπÔ∏è  No Silver results to log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "## [10] Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 15:07:38,059 [INFO] - \n",
      "================================================================================\n",
      "2025-12-03 15:07:38,060 [INFO] - ORCHESTRATOR SUMMARY\n",
      "2025-12-03 15:07:38,060 [INFO] - ================================================================================\n",
      "2025-12-03 15:07:38,060 [INFO] - Run ID: 20251001T183103260_2dc7d8f3\n",
      "2025-12-03 15:07:38,061 [INFO] - Source: anva_meeus\n",
      "2025-12-03 15:07:38,061 [INFO] - Run TS: 20251001T183103260\n",
      "2025-12-03 15:07:38,061 [INFO] - \n",
      "Timing:\n",
      "2025-12-03 15:07:38,062 [INFO] -   Bronze: 25.792331s\n",
      "2025-12-03 15:07:38,062 [INFO] -   Total:  28s\n",
      "2025-12-03 15:07:38,063 [INFO] - \n",
      "Bronze Results:\n",
      "2025-12-03 15:07:38,063 [INFO] -   ‚úì Success: 49/58\n",
      "2025-12-03 15:07:38,063 [INFO] - \n",
      "Silver Results:\n",
      "2025-12-03 15:07:38,064 [INFO] -   (No processing)\n",
      "2025-12-03 15:07:38,064 [INFO] - \n",
      "Overall Status: SUCCESS\n",
      "2025-12-03 15:07:38,065 [INFO] - ================================================================================\n",
      "2025-12-03 15:07:38,065 [INFO] - \n",
      "‚úì All processing completed successfully!\n"
     ]
    }
   ],
   "source": [
    "total_end = datetime.now(timezone.utc)\n",
    "total_duration = int((total_end - bronze_start).total_seconds())\n",
    "\n",
    "logger.info(\"\\n\" + \"=\"*80)\n",
    "logger.info(\"ORCHESTRATOR SUMMARY\")\n",
    "logger.info(\"=\"*80)\n",
    "logger.info(f\"Run ID: {RUN_ID}\")\n",
    "logger.info(f\"Source: {source}\")\n",
    "logger.info(f\"Run TS: {run_ts}\")\n",
    "logger.info(f\"\\nTiming:\")\n",
    "logger.info(f\"  Bronze: {bronze_duration}s\")\n",
    "if silver_results:\n",
    "    logger.info(f\"  Silver: {silver_duration}s\")\n",
    "logger.info(f\"  Total:  {total_duration}s\")\n",
    "\n",
    "logger.info(f\"\\nBronze Results:\")\n",
    "if bronze_results:\n",
    "    bronze_success = sum(1 for r in bronze_results if r['status'] == 'SUCCESS')\n",
    "    bronze_failed = sum(1 for r in bronze_results if r['status'] == 'FAILED')\n",
    "    logger.info(f\"  ‚úì Success: {bronze_success}/{len(bronze_results)}\")\n",
    "    if bronze_failed > 0:\n",
    "        logger.info(f\"  ‚úó Failed:  {bronze_failed}\")\n",
    "else:\n",
    "    logger.info(f\"  (No processing)\")\n",
    "\n",
    "logger.info(f\"\\nSilver Results:\")\n",
    "if silver_results:\n",
    "    silver_success = sum(1 for r in silver_results if r['status'] == 'SUCCESS')\n",
    "    silver_failed = sum(1 for r in silver_results if r['status'] == 'FAILED')\n",
    "    logger.info(f\"  ‚úì Success: {silver_success}/{len(silver_results)}\")\n",
    "    if silver_failed > 0:\n",
    "        logger.info(f\"  ‚úó Failed:  {silver_failed}\")\n",
    "else:\n",
    "    logger.info(f\"  (No processing)\")\n",
    "\n",
    "# Overall status\n",
    "if bronze_results:\n",
    "    all_bronze_ok = all(r['status'] in ('SUCCESS', 'EMPTY', 'SKIPPED') for r in bronze_results)\n",
    "else:\n",
    "    all_bronze_ok = True\n",
    "\n",
    "if silver_results:\n",
    "    all_silver_ok = all(r['status'] in ('SUCCESS', 'SKIPPED') for r in silver_results)\n",
    "else:\n",
    "    all_silver_ok = True\n",
    "\n",
    "overall_status = \"SUCCESS\" if (all_bronze_ok and all_silver_ok) else \"PARTIAL\" if bronze_results or silver_results else \"NO_WORK\"\n",
    "\n",
    "logger.info(f\"\\nOverall Status: {overall_status}\")\n",
    "logger.info(\"=\"*80)\n",
    "\n",
    "if overall_status != \"SUCCESS\":\n",
    "    logger.info(f\"\\n‚ö†Ô∏è  Some tables failed. Check logs for details.\")\n",
    "    logger.info(f\"   Use retry_tables parameter to retry specific tables.\")\n",
    "else:\n",
    "    logger.info(f\"\\n‚úì All processing completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dwh-spark-processing (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
