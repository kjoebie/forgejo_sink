{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# 00 ‚Äî Master Orchestrator: Bronze ‚Üí Silver Processing\n",
    "\n",
    "Main orchestration notebook for processing parquet files through Bronze and Silver layers.\n",
    "\n",
    "## Architecture Overview\n",
    "\n",
    "```\n",
    "Parquet Files (Files/{source}/{run_ts}/)\n",
    "    ‚Üì\n",
    "Bronze Layer (append with run_ts for CDC)\n",
    "    ‚Üì\n",
    "Silver Layer (CDC merge: INSERT/UPDATE/DELETE)\n",
    "    ‚Üì\n",
    "Watermark Update (incremental tables only)\n",
    "```\n",
    "\n",
    "## Process Flow\n",
    "\n",
    "1. **Load Configuration** (DAG, enabled tables, retry filter)\n",
    "2. **Check Incremental** ‚Üí Run watermark merge if needed\n",
    "3. **Bronze Processing** ‚Üí Parallel table loading (10 workers)\n",
    "4. **Bronze Logging** ‚Üí Batch log all results\n",
    "5. **Silver Processing** ‚Üí Parallel CDC merge (tables with business_keys)\n",
    "6. **Silver Logging** ‚Üí Batch log all results\n",
    "7. **Summary Statistics** ‚Üí Performance metrics, efficiency\n",
    "\n",
    "## Key Features\n",
    "\n",
    "- **Parallel Processing**: ThreadPoolExecutor for 5-10x speedup\n",
    "- **Idempotency**: Check logs before reprocessing\n",
    "- **Retry Support**: Process only specific tables\n",
    "- **Error Resilience**: Continue on failure, comprehensive logging\n",
    "- **Performance Tracking**: Efficiency metrics (theoretical vs actual time)\n",
    "\n",
    "## Parameters\n",
    "\n",
    "- `source`: Source system name (e.g., \"vizier\")\n",
    "- `run_ts`: Run timestamp (e.g., \"20251105T142752505\")\n",
    "- `dag_path`: Path to DAG configuration JSON\n",
    "- `retry_tables`: Optional list of tables to retry\n",
    "- `force_reload`: Ignore log and reload all\n",
    "- `max_workers`: Parallel workers (default: 10)\n",
    "- `debug`: Enable debug output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters (Papermill compatible)\n",
    "source = \"anva_meeus\"                               # Source system name\n",
    "run_ts = \"20251001T183103260\"                       # Run timestamp\n",
    "dag_path = \"config/dag_anva_meeus_week.json\"        # DAG configuration path\n",
    "retry_tables = None                                 # Optional: list of table names to retry\n",
    "force_reload = True                                # If True, ignore logs and reload all\n",
    "max_workers = 10                                    # Parallel workers for table processing\n",
    "debug = True                                        # Enable debug output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## [1] Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timezone\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from typing import List, Dict, Any\n",
    "import json\n",
    "from uuid import uuid4\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"MASTER ORCHESTRATOR STARTING\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Source: {source}\")\n",
    "print(f\"Run TS: {run_ts}\")\n",
    "print(f\"DAG: {dag_path}\")\n",
    "print(f\"Retry tables: {retry_tables}\")\n",
    "print(f\"Force reload: {force_reload}\")\n",
    "print(f\"Max workers: {max_workers}\")\n",
    "print(f\"Debug: {debug}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## [2] Load Utility Notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "# Load logging utilities\n",
    "# Load config utilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug: Check working directory\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "print(f\"Path.cwd(): {Path.cwd()}\")\n",
    "print(f\"\\nFiles in current dir:\")\n",
    "print(list(Path.cwd().iterdir())[:10])\n",
    "\n",
    "print(f\"\\nDoes notebooks/ exist from here? {Path('notebooks').exists()}\")\n",
    "print(f\"Does ../notebooks/ exist? {Path('../notebooks').exists()}\")\n",
    "print(f\"Does 01_process_data.ipynb exist? {Path('01_process_data.ipynb').exists()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## [1.5] Initialize Spark Session\n",
    "\n",
    "# Check if Spark session exists (Fabric/Databricks has it by default)\n",
    "# For local environments, create it\n",
    "try:\n",
    "    spark\n",
    "    print(\"‚úì Spark session already available\")\n",
    "except NameError:\n",
    "    print(\"Creating Spark session for local environment...\")\n",
    "    from pyspark.sql import SparkSession\n",
    "    \n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"DWH_Bronze_Silver_Processing\") \\\n",
    "        .enableHiveSupport() \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    print(\"‚úì Spark session created\")\n",
    "\n",
    "spark.conf.set(\"spark.sql.parquet.datetimeRebaseModeInRead\", \"CORRECTED\")\n",
    "spark.conf.set(\"spark.sql.parquet.int96RebaseModeInRead\", \"CORRECTED\")\n",
    "spark.conf.set(\"spark.sql.parquet.datetimeRebaseModeInWrite\", \"CORRECTED\")\n",
    "spark.conf.set(\"spark.sql.parquet.int96RebaseModeInWrite\", \"CORRECTED\")\n",
    "\n",
    "# Verify Spark session\n",
    "print(f\"  Spark version: {spark.version}\")\n",
    "print(f\"  Application ID: {spark.sparkContext.applicationId}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"./notebooks/01_utils_logging.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"./notebooks/02_utils_config.ipynb\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## [3] Load DAG Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and validate DAG\n",
    "print(f\"\\nüìã Loading DAG configuration...\")\n",
    "dag = load_dag(dag_path)\n",
    "print(f\"‚úì DAG loaded: {dag.get('source')}\")\n",
    "\n",
    "# Get metadata\n",
    "dag_metadata = get_dag_metadata(dag)\n",
    "base_files = dag_metadata['base_files']\n",
    "\n",
    "print(f\"  Base files: {base_files}\")\n",
    "\n",
    "# Get tables to process\n",
    "tables_to_process = get_tables_to_process(\n",
    "    dag=dag,\n",
    "    retry_tables=retry_tables,\n",
    "    only_enabled=True\n",
    ")\n",
    "\n",
    "# Ensure schemas exist        \n",
    "schemas = set()\n",
    "\n",
    "for t in tables_to_process:\n",
    "    delta_table = t.get(\"delta_table\")\n",
    "    delta_schema = t.get(\"delta_schema\")\n",
    "\n",
    "    if delta_table and \".\" in delta_table:\n",
    "        # Vorm: schema.tabel in delta_table\n",
    "        schema = delta_table.split(\".\")[0]\n",
    "    else:\n",
    "        # Anders: gebruik delta_schema of standaard 'bronze'\n",
    "        schema = (delta_schema or \"bronze\")\n",
    "\n",
    "    schemas.add(schema)\n",
    "\n",
    "for schema in sorted(schemas):\n",
    "    spark.sql(f\"CREATE SCHEMA IF NOT EXISTS `{schema}`\")\n",
    "\n",
    "print(\"Schemas ensured:\", \", \".join(sorted(schemas)))\n",
    "\n",
    "print(f\"\\nüìä Tables to process: {len(tables_to_process)}\")\n",
    "\n",
    "# Show summary\n",
    "dag_summary = summarize_dag(dag)\n",
    "print(f\"  Total enabled: {dag_summary['enabled_tables']}\")\n",
    "print(f\"  Load modes: {dag_summary['load_mode_counts']}\")\n",
    "\n",
    "if not tables_to_process:\n",
    "    print(\"\\n‚ö†Ô∏è  No tables to process. Exiting.\")\n",
    "    raise SystemExit(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## [4] Generate Run ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate unique run ID\n",
    "RUN_ID = f\"{run_ts}_{uuid4().hex[:8]}\"\n",
    "run_date = build_run_date(run_ts)\n",
    "print(f\"\\nüÜî Run ID: {RUN_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## [5] Check for Incremental Tables (Watermark Merge)\n",
    "\n",
    "If incremental tables are present, run watermark merge notebook.\n",
    "This must happen BEFORE Bronze loading starts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nüíß Checking for incremental tables...\")\n",
    "\n",
    "# Filter incremental tables\n",
    "incremental_tables = get_tables_by_load_mode(tables_to_process, \"incremental\")\n",
    "\n",
    "if len(incremental_tables) > 0:\n",
    "    print(f\"  Found {len(incremental_tables)} incremental tables\")\n",
    "    print(f\"  Tables: {[t['name'] for t in incremental_tables[:5]]}\")\n",
    "    \n",
    "    # Get watermarks path from DAG\n",
    "    wm_configpath = dag_metadata.get('watermarks_path', 'config/watermarks.json')\n",
    "    \n",
    "    # Build watermark folder path (where extraction pipeline writes watermarks)\n",
    "    wm_folder = f\"runtime/{source}/{run_ts}/\"\n",
    "    \n",
    "    print(f\"  Config: {wm_configpath}\")\n",
    "    print(f\"  Runtime folder: {wm_folder}\")\n",
    "    \n",
    "    # Note: In Fabric, this would use mssparkutils.notebook.run()\n",
    "    # For local testing, we skip watermark merge (not critical for Bronze/Silver testing)\n",
    "    print(f\"\\n  ‚ö†Ô∏è  Watermark merge would run here (11_bronze_watermark_merge.ipynb)\")\n",
    "    print(f\"     Skipping for now - watermarks managed by extraction pipeline\")\n",
    "else:\n",
    "    print(f\"  ‚óØ No incremental tables - skipping watermark merge\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## [6] Bronze Processing (Parallel)\n",
    "\n",
    "Load all tables from parquet to Bronze Delta tables in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Bronze worker notebook\n",
    "%run \"./notebooks/10_bronze_load.ipynb\"\n",
    "\n",
    "print(\"‚úì Bronze worker loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Catalogs:\")\n",
    "# spark.sql(\"SHOW CATALOGS\").show(truncate=False)\n",
    "\n",
    "# print(\"\\nSchemas (databases) in spark_catalog:\")\n",
    "# spark.sql(\"SHOW DATABASES\").show(truncate=False)\n",
    "\n",
    "# # print(\"\\nTables per schema:\")\n",
    "# # for row in spark.sql(\"SHOW DATABASES\").collect():\n",
    "# #     db = row[\"namespace\"] if \"namespace\" in row else row[\"databaseName\"]\n",
    "# #     print(f\"\\n=== {db} ===\")\n",
    "# #     spark.sql(f\"SHOW TABLES IN {db}\").show(truncate=False)\n",
    "\n",
    "# for db in [\"demo\"]:\n",
    "#     print(f\"Dropping schema `{db}` (CASCADE)...\")\n",
    "#     spark.sql(f\"DROP DATABASE IF EXISTS `{db}` CASCADE\")\n",
    "\n",
    "# print(\"\\nSchemas na drop:\")\n",
    "# spark.sql(\"SHOW DATABASES\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nüîµ BRONZE: Loading parquet to Delta tables...\")\n",
    "print(f\"  Workers: {max_workers}\")\n",
    "print(f\"  Tables: {len(tables_to_process)}\")\n",
    "\n",
    "bronze_start = datetime.now(timezone.utc)\n",
    "bronze_results = []\n",
    "\n",
    "# Filter tables if not force_reload (check logs)\n",
    "if not force_reload:\n",
    "    print(f\"\\n  üìã Checking logs for already processed tables...\")\n",
    "    \n",
    "    # Get successfully processed tables from log\n",
    "    processed_tables = get_successful_tables(run_ts, layer=\"bronze\")\n",
    "    \n",
    "    if processed_tables:\n",
    "        print(f\"    Found {len(processed_tables)} already processed tables\")\n",
    "        \n",
    "        # Filter out already processed\n",
    "        tables_to_process_bronze = [\n",
    "            t for t in tables_to_process \n",
    "            if t['name'] not in processed_tables\n",
    "        ]\n",
    "\n",
    "        print(f\"    Remaining: {len(tables_to_process_bronze)} tables\")\n",
    "    else:\n",
    "        tables_to_process_bronze = tables_to_process\n",
    "else:\n",
    "    tables_to_process_bronze = tables_to_process\n",
    "    print(f\"  ‚ö†Ô∏è  Force reload enabled - processing all tables\")\n",
    "\n",
    "if not tables_to_process_bronze:\n",
    "    print(f\"\\n  ‚úì All tables already processed for this run_ts\")\n",
    "else:\n",
    "    print(f\"\\n  üöÄ Processing {len(tables_to_process_bronze)} tables in parallel...\\n\")\n",
    "    \n",
    "    # Wrapper function for parallel execution\n",
    "    def process_table_wrapper(table_def):\n",
    "        \"\"\"Wrapper to catch exceptions and always return a result.\"\"\"\n",
    "        try:\n",
    "            return process_bronze_table(\n",
    "                table_def=table_def,\n",
    "                source_name=source,\n",
    "                run_ts=run_ts,\n",
    "                base_files=base_files,\n",
    "                debug=False  # Disable per-table debug in parallel mode\n",
    "            )\n",
    "        except Exception as e:\n",
    "            # If worker throws unhandled exception, create error result\n",
    "            return {\n",
    "                \"log_id\": f\"{source}:{table_def['name']}:{run_ts}:error\",\n",
    "                \"run_id\": RUN_ID,\n",
    "                \"run_ts\": run_ts,\n",
    "                \"source\": source,\n",
    "                \"table_name\": table_def['name'],\n",
    "                \"load_mode\": table_def.get('load_mode'),\n",
    "                \"status\": \"FAILED\",\n",
    "                \"rows_read\": None,\n",
    "                \"rows_processed\": None,\n",
    "                \"start_time\": datetime.now(timezone.utc),\n",
    "                \"end_time\": datetime.now(timezone.utc),\n",
    "                \"duration_seconds\": 0,\n",
    "                \"error_message\": f\"Unhandled exception: {str(e)[:500]}\",\n",
    "                \"parquet_path\": None,\n",
    "                \"delta_table\": None,\n",
    "            }\n",
    "    \n",
    "    # Set RUN_ID\n",
    "    RUN_ID = f\"{run_ts}\"\n",
    "    print(f\"RUN_ID set to {RUN_ID}\")\n",
    "    \n",
    "    # Parallel execution\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {\n",
    "            executor.submit(process_table_wrapper, table): table \n",
    "            for table in tables_to_process_bronze\n",
    "        }\n",
    "        \n",
    "        completed = 0\n",
    "        for future in as_completed(futures):\n",
    "            result = future.result()\n",
    "            bronze_results.append(result)\n",
    "            completed += 1\n",
    "            \n",
    "            # Progress indicator\n",
    "            status_icon = \"‚úì\" if result['status'] == 'SUCCESS' else \"‚úó\" if result['status'] == 'FAILED' else \"‚óØ\"\n",
    "            # Kort foutfragment erbij (max 120 chars, 1 regel)\n",
    "            error_snippet = (result.get(\"error_message\") or \"\")[:120].replace(\"\\n\", \" \")\n",
    "            \n",
    "            print(\n",
    "                f\"[{completed}/{len(tables_to_process_bronze)}]\"\n",
    "                f\"{status_icon} {result['table_name']:<30} {result['status']:<10} \"\n",
    "                f\"{(result.get('rows_processed') or 0):>10,} rows {error_snippet}\"\n",
    "                )\n",
    "\n",
    "bronze_end = datetime.now(timezone.utc)\n",
    "bronze_duration = float((bronze_end - bronze_start).total_seconds())\n",
    "\n",
    "print(f\"\\n‚úì Bronze processing completed in {bronze_duration}s\")\n",
    "\n",
    "#sys.exit(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## [7] Bronze Logging and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "if bronze_results:\n",
    "    print(f\"\\nüìä Logging Bronze results...\")\n",
    "        \n",
    "    # Calculate summary statistics\n",
    "    success_count = sum(1 for r in bronze_results if r['status'] == 'SUCCESS')\n",
    "    failed_count = sum(1 for r in bronze_results if r['status'] == 'FAILED')\n",
    "    empty_count = sum(1 for r in bronze_results if r['status'] == 'EMPTY')\n",
    "    skipped_count = sum(1 for r in bronze_results if r['status'] == 'SKIPPED')\n",
    "    \n",
    "    total_rows = sum(r.get('rows_processed', 0) or 0 for r in bronze_results)\n",
    "    \n",
    "    # Performance metrics\n",
    "    sum_task_seconds = float(sum(r.get('duration_seconds', 0) or 0 for r in bronze_results))\n",
    "    theoretical_min_sec = float(sum_task_seconds / max_workers if max_workers > 0 else sum_task_seconds)\n",
    "    actual_time_sec = bronze_duration # float\n",
    "    efficiency_pct = float((theoretical_min_sec / actual_time_sec * 100) if actual_time_sec > 0 else 0)\n",
    "    \n",
    "    # Failed tables list\n",
    "    failed_tables = [r['table_name'] for r in bronze_results if r['status'] == 'FAILED']\n",
    "    \n",
    "    # Log summary\n",
    "    bronze_summary = {\n",
    "    \"run_id\": RUN_ID,\n",
    "    \"run_date\": run_date,\n",
    "    \"run_ts\": run_ts,\n",
    "    \"source\": source,\n",
    "    \"total_tables\": len(bronze_results),\n",
    "    \"tables_success\": success_count,\n",
    "    \"tables_empty\": empty_count,\n",
    "    \"tables_failed\": failed_count,\n",
    "    \"tables_skipped\": skipped_count,\n",
    "    \"total_rows\": total_rows,\n",
    "    \"workers\": max_workers,\n",
    "    \"sum_task_seconds\": sum_task_seconds,\n",
    "    \"theoretical_min_sec\": theoretical_min_sec,\n",
    "    \"actual_time_sec\": actual_time_sec,\n",
    "    \"efficiency_pct\": efficiency_pct,\n",
    "    \"run_start\": bronze_start,\n",
    "    \"run_end\": bronze_end,\n",
    "    \"duration_seconds\": bronze_duration,\n",
    "    \"error_message\": None,\n",
    "    \"failed_tables\": failed_tables,\n",
    "    }\n",
    "\n",
    "    run_log_id = log_bronze_summary(summary=bronze_summary)\n",
    "\n",
    "    # Batch log\n",
    "    log_bronze_batch(bronze_results=bronze_results, run_log_id=run_log_id)\n",
    "\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\n  Summary:\")\n",
    "    print(f\"    Success: {success_count}\")\n",
    "    print(f\"    Failed:  {failed_count}\")\n",
    "    print(f\"    Empty:   {empty_count}\")\n",
    "    print(f\"    Skipped: {skipped_count}\")\n",
    "    print(f\"    Total rows: {total_rows:,}\")\n",
    "    print(f\"    Efficiency: {efficiency_pct:.1f}%\")\n",
    "    \n",
    "    if failed_tables:\n",
    "        print(f\"\\n  ‚ö†Ô∏è  Failed tables: {failed_tables}\")\n",
    "else:\n",
    "    print(f\"\\n  ‚ÑπÔ∏è  No Bronze results to log\")\n",
    "\n",
    "#sys.exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SHOW TABLES IN logs\").show(truncate=False)\n",
    "#spark.table(\"logs.bronze_processing_log\").printSchema()\n",
    "#spark.table(\"logs.bronze_run_summary\").printSchema()\n",
    "\n",
    "#spark.sql(\"drop table if exists logs.silver_run_summary\").show()\n",
    "#spark.sql(\"select * from logs.bronze_run_summary order by run_end desc limit 5\").show(truncate=False)\n",
    "\n",
    "#spark.sql(\"drop table if exists logs.bronze_processing_log\").show()\n",
    "#spark.sql(\"drop table if exists logs.bronze_run_summary\").show()\n",
    "\n",
    "# spark.sql(\"drop table if exists logs.silver_processing_log\").show()\n",
    "# spark.sql(\"drop table if exists logs.silver_run_summary\").show()\n",
    "# spark.table(\"logs.bronze_run_summary\") \\\n",
    "#       .orderBy(\"run_end\", \"source\") \\\n",
    "#       .show(20, truncate=False)\n",
    "\n",
    "# spark.table(\"logs.bronze_processing_log\") \\\n",
    "#       .orderBy(\"run_ts\", \"table_name\") \\\n",
    "#       .show(200, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## [8] Silver Processing (Parallel CDC Merge)\n",
    "\n",
    "Process tables that have business_keys defined for CDC merge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Silver worker notebook\n",
    "%run \"./notebooks/20_silver_cdc_merge.ipynb\"\n",
    "\n",
    "print(\"‚úì Silver CDC merge worker loaded\")\n",
    "sys.exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nüî∑ SILVER: CDC merge from Bronze...\")\n",
    "\n",
    "# Filter tables for Silver processing:\n",
    "# 1. Must have business_keys defined\n",
    "# 2. Must have been successfully loaded to Bronze\n",
    "\n",
    "successful_bronze_tables = [r['table_name'] for r in bronze_results if r['status'] == 'SUCCESS']\n",
    "\n",
    "tables_for_silver = [\n",
    "    t for t in tables_to_process \n",
    "    if t.get('business_keys') and t['name'] in successful_bronze_tables\n",
    "]\n",
    "\n",
    "print(f\"  Tables with business_keys: {len([t for t in tables_to_process if t.get('business_keys')])}\")\n",
    "print(f\"  Successful Bronze loads: {len(successful_bronze_tables)}\")\n",
    "print(f\"  Tables to process in Silver: {len(tables_for_silver)}\")\n",
    "\n",
    "silver_results = []\n",
    "\n",
    "if not tables_for_silver:\n",
    "    print(f\"\\n  ‚ÑπÔ∏è  No tables to process in Silver\")\n",
    "else:\n",
    "    silver_start = datetime.now(timezone.utc)\n",
    "    \n",
    "    print(f\"\\n  üöÄ Processing {len(tables_for_silver)} tables in parallel...\\n\")\n",
    "    \n",
    "    # Wrapper function for parallel execution\n",
    "    def process_silver_wrapper(table_def):\n",
    "        \"\"\"Wrapper to catch exceptions and always return a result.\"\"\"\n",
    "        try:\n",
    "            return process_silver_cdc_merge(\n",
    "                table_def=table_def,\n",
    "                source_name=source,\n",
    "                run_ts=run_ts,\n",
    "                debug=False\n",
    "            )\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"log_id\": f\"{source}:{table_def['name']}:{run_ts}:silver:error\",\n",
    "                \"run_id\": RUN_ID,\n",
    "                \"run_ts\": run_ts,\n",
    "                \"source\": source,\n",
    "                \"table_name\": table_def['name'],\n",
    "                \"load_mode\": table_def.get('load_mode'),\n",
    "                \"status\": \"FAILED\",\n",
    "                \"rows_inserted\": None,\n",
    "                \"rows_updated\": None,\n",
    "                \"rows_deleted\": None,\n",
    "                \"rows_unchanged\": None,\n",
    "                \"total_silver_rows\": None,\n",
    "                \"bronze_rows\": None,\n",
    "                \"bronze_table\": None,\n",
    "                \"silver_table\": None,\n",
    "                \"start_time\": datetime.now(timezone.utc),\n",
    "                \"end_time\": datetime.now(timezone.utc),\n",
    "                \"duration_seconds\": 0,\n",
    "                \"error_message\": f\"Unhandled exception: {str(e)[:500]}\",\n",
    "            }\n",
    "    \n",
    "    # Parallel execution\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {\n",
    "            executor.submit(process_silver_wrapper, table): table \n",
    "            for table in tables_for_silver\n",
    "        }\n",
    "        \n",
    "        completed = 0\n",
    "        for future in as_completed(futures):\n",
    "            result = future.result()\n",
    "            silver_results.append(result)\n",
    "            completed += 1\n",
    "            \n",
    "            status_icon = \"‚úì\" if result['status'] == 'SUCCESS' else \"‚úó\"\n",
    "            deletes = result.get('rows_deleted', 0) or 0\n",
    "            delete_info = f\" ({deletes} deleted)\" if deletes > 0 else \"\"\n",
    "            print(f\"    [{completed}/{len(tables_for_silver)}] {status_icon} {result['table_name']:<30} {result['status']:<10}{delete_info}\")\n",
    "            \n",
    "    \n",
    "    silver_end = datetime.now(timezone.utc)\n",
    "    silver_duration = int((silver_end - silver_start).total_seconds())\n",
    "    \n",
    "    print(f\"\\n‚úì Silver processing completed in {silver_duration}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "## [9] Silver Logging and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "if silver_results:\n",
    "    print(f\"\\nüìä Logging Silver results...\")\n",
    "    \n",
    "    # Batch log\n",
    "    log_silver_batch(silver_results)\n",
    "    \n",
    "    # Calculate summary\n",
    "    success_count = sum(1 for r in silver_results if r['status'] == 'SUCCESS')\n",
    "    failed_count = sum(1 for r in silver_results if r['status'] == 'FAILED')\n",
    "    skipped_count = sum(1 for r in silver_results if r['status'] == 'SKIPPED')\n",
    "    \n",
    "    total_inserts = sum(r.get('rows_inserted', 0) or 0 for r in silver_results)\n",
    "    total_updates = sum(r.get('rows_updated', 0) or 0 for r in silver_results)\n",
    "    total_deletes = sum(r.get('rows_deleted', 0) or 0 for r in silver_results)\n",
    "    total_unchanged = sum(r.get('rows_unchanged', 0) or 0 for r in silver_results)\n",
    "    \n",
    "    failed_tables = [r['table_name'] for r in silver_results if r['status'] == 'FAILED']\n",
    "    \n",
    "    # Log summary\n",
    "    silver_summary = {\n",
    "        \"run_id\": RUN_ID,\n",
    "        \"source\": source,\n",
    "        \"run_ts\": run_ts,\n",
    "        \"run_start\": silver_start,\n",
    "        \"run_end\": silver_end,\n",
    "        \"duration_seconds\": silver_duration,\n",
    "        \"total_tables\": len(silver_results),\n",
    "        \"tables_success\": success_count,\n",
    "        \"tables_failed\": failed_count,\n",
    "        \"tables_skipped\": skipped_count,\n",
    "        \"total_inserts\": total_inserts,\n",
    "        \"total_updates\": total_updates,\n",
    "        \"total_deletes\": total_deletes,\n",
    "        \"total_unchanged\": total_unchanged,\n",
    "        \"failed_tables\": failed_tables,\n",
    "    }\n",
    "    \n",
    "    log_silver_summary(silver_summary)\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\n  Summary:\")\n",
    "    print(f\"    Success: {success_count}\")\n",
    "    print(f\"    Failed:  {failed_count}\")\n",
    "    print(f\"    Skipped: {skipped_count}\")\n",
    "    if total_inserts or total_updates or total_deletes:\n",
    "        print(f\"    CDC: +{total_inserts or 0} ~{total_updates or 0} -{total_deletes}\")\n",
    "    \n",
    "    if failed_tables:\n",
    "        print(f\"\\n  ‚ö†Ô∏è  Failed tables: {failed_tables}\")\n",
    "else:\n",
    "    print(f\"\\n  ‚ÑπÔ∏è  No Silver results to log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "## [10] Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_end = datetime.now(timezone.utc)\n",
    "total_duration = int((total_end - bronze_start).total_seconds())\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ORCHESTRATOR SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Run ID: {RUN_ID}\")\n",
    "print(f\"Source: {source}\")\n",
    "print(f\"Run TS: {run_ts}\")\n",
    "print(f\"\\nTiming:\")\n",
    "print(f\"  Bronze: {bronze_duration}s\")\n",
    "if silver_results:\n",
    "    print(f\"  Silver: {silver_duration}s\")\n",
    "print(f\"  Total:  {total_duration}s\")\n",
    "\n",
    "print(f\"\\nBronze Results:\")\n",
    "if bronze_results:\n",
    "    bronze_success = sum(1 for r in bronze_results if r['status'] == 'SUCCESS')\n",
    "    bronze_failed = sum(1 for r in bronze_results if r['status'] == 'FAILED')\n",
    "    print(f\"  ‚úì Success: {bronze_success}/{len(bronze_results)}\")\n",
    "    if bronze_failed > 0:\n",
    "        print(f\"  ‚úó Failed:  {bronze_failed}\")\n",
    "else:\n",
    "    print(f\"  (No processing)\")\n",
    "\n",
    "print(f\"\\nSilver Results:\")\n",
    "if silver_results:\n",
    "    silver_success = sum(1 for r in silver_results if r['status'] == 'SUCCESS')\n",
    "    silver_failed = sum(1 for r in silver_results if r['status'] == 'FAILED')\n",
    "    print(f\"  ‚úì Success: {silver_success}/{len(silver_results)}\")\n",
    "    if silver_failed > 0:\n",
    "        print(f\"  ‚úó Failed:  {silver_failed}\")\n",
    "else:\n",
    "    print(f\"  (No processing)\")\n",
    "\n",
    "# Overall status\n",
    "if bronze_results:\n",
    "    all_bronze_ok = all(r['status'] in ('SUCCESS', 'EMPTY', 'SKIPPED') for r in bronze_results)\n",
    "else:\n",
    "    all_bronze_ok = True\n",
    "\n",
    "if silver_results:\n",
    "    all_silver_ok = all(r['status'] in ('SUCCESS', 'SKIPPED') for r in silver_results)\n",
    "else:\n",
    "    all_silver_ok = True\n",
    "\n",
    "overall_status = \"SUCCESS\" if (all_bronze_ok and all_silver_ok) else \"PARTIAL\" if bronze_results or silver_results else \"NO_WORK\"\n",
    "\n",
    "print(f\"\\nOverall Status: {overall_status}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if overall_status != \"SUCCESS\":\n",
    "    print(f\"\\n‚ö†Ô∏è  Some tables failed. Check logs for details.\")\n",
    "    print(f\"   Use retry_tables parameter to retry specific tables.\")\n",
    "else:\n",
    "    print(f\"\\n‚úì All processing completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dwh-spark-processing (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
