{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ab783656",
      "metadata": {},
      "source": [
        "# 00 ‚Äî Master Orchestrator: Bronze ‚Üí Silver Processing\n",
        "\n",
        "Main orchestration notebook for processing parquet files through Bronze and Silver layers.\n",
        "\n",
        "## Architecture Overview\n",
        "\n",
        "```\n",
        "Parquet Files (Files/{source}/{run_ts}/)\n",
        "    ‚Üì\n",
        "Bronze Layer (append with run_ts for CDC)\n",
        "    ‚Üì\n",
        "Silver Layer (CDC merge: INSERT/UPDATE/DELETE)\n",
        "    ‚Üì\n",
        "Watermark Update (incremental tables only)\n",
        "```\n",
        "\n",
        "## Process Flow\n",
        "\n",
        "1. **Load Configuration** (DAG, enabled tables, retry filter)\n",
        "2. **Check Incremental** ‚Üí Run watermark merge if needed\n",
        "3. **Bronze Processing** ‚Üí Parallel table loading (10 workers)\n",
        "4. **Bronze Logging** ‚Üí Batch log all results\n",
        "5. **Silver Processing** ‚Üí Parallel CDC merge (tables with business_keys)\n",
        "6. **Silver Logging** ‚Üí Batch log all results\n",
        "7. **Summary Statistics** ‚Üí Performance metrics, efficiency\n",
        "\n",
        "## Key Features\n",
        "\n",
        "- **Parallel Processing**: ThreadPoolExecutor for 5-10x speedup\n",
        "- **Idempotency**: Check logs before reprocessing\n",
        "- **Retry Support**: Process only specific tables\n",
        "- **Error Resilience**: Continue on failure, comprehensive logging\n",
        "- **Performance Tracking**: Efficiency metrics (theoretical vs actual time)\n",
        "\n",
        "## Parameters\n",
        "\n",
        "- `source`: Source system name (e.g., \"vizier\")\n",
        "- `run_ts`: Run timestamp (e.g., \"20251105T142752505\")\n",
        "- `dag_path`: Path to DAG configuration JSON\n",
        "- `retry_tables`: Optional list of tables to retry\n",
        "- `force_reload`: Ignore log and reload all\n",
        "- `max_workers`: Parallel workers (default: 10)\n",
        "- `debug`: Enable debug output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "df6df089",
      "metadata": {
        "tags": [
          "parameters"
        ]
      },
      "outputs": [],
      "source": [
        "# Parameters (Papermill compatible)\n",
        "source = \"anva_meeus\"                               # Source system name\n",
        "run_ts = \"20251001T183103260\"                       # Run timestamp\n",
        "dag_path = \"config/dag_anva_meeus_week.json\"        # DAG configuration path\n",
        "retry_tables = None                                 # Optional: list of table names to retry\n",
        "force_reload = False                                # If True, ignore logs and reload all\n",
        "max_workers = 10                                    # Parallel workers for table processing\n",
        "debug = True                                        # Enable debug output"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc50f4ca",
      "metadata": {},
      "source": [
        "## [1] Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "bda4f626",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "MASTER ORCHESTRATOR STARTING\n",
            "================================================================================\n",
            "Source: anva_meeus\n",
            "Run TS: 20251001T183103260\n",
            "DAG: config/dag_anva_meeus_week.json\n",
            "Retry tables: None\n",
            "Force reload: False\n",
            "Max workers: 10\n",
            "Debug: True\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "from datetime import datetime\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from typing import List, Dict, Any\n",
        "import json\n",
        "from uuid import uuid4\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"MASTER ORCHESTRATOR STARTING\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Source: {source}\")\n",
        "print(f\"Run TS: {run_ts}\")\n",
        "print(f\"DAG: {dag_path}\")\n",
        "print(f\"Retry tables: {retry_tables}\")\n",
        "print(f\"Force reload: {force_reload}\")\n",
        "print(f\"Max workers: {max_workers}\")\n",
        "print(f\"Debug: {debug}\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf54a124",
      "metadata": {},
      "source": [
        "## [2] Load Utility Notebooks"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49b46d36",
      "metadata": {},
      "source": [
        "# Load logging utilities\n",
        "# Load config utilities\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "e31d5ae6",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current working directory: /home/sparkadmin/source/repos/dwh_spark_processing\n",
            "Path.cwd(): /home/sparkadmin/source/repos/dwh_spark_processing\n",
            "\n",
            "Files in current dir:\n",
            "[PosixPath('/home/sparkadmin/source/repos/dwh_spark_processing/notebook_outputs'), PosixPath('/home/sparkadmin/source/repos/dwh_spark_processing/pyproject.toml'), PosixPath('/home/sparkadmin/source/repos/dwh_spark_processing/.gitignore'), PosixPath('/home/sparkadmin/source/repos/dwh_spark_processing/uv.lock'), PosixPath('/home/sparkadmin/source/repos/dwh_spark_processing/.venv'), PosixPath('/home/sparkadmin/source/repos/dwh_spark_processing/.python-version'), PosixPath('/home/sparkadmin/source/repos/dwh_spark_processing/metastore_db'), PosixPath('/home/sparkadmin/source/repos/dwh_spark_processing/backup'), PosixPath('/home/sparkadmin/source/repos/dwh_spark_processing/modules'), PosixPath('/home/sparkadmin/source/repos/dwh_spark_processing/.vscode')]\n",
            "\n",
            "Does notebooks/ exist from here? True\n",
            "Does ../notebooks/ exist? False\n",
            "Does 01_process_data.ipynb exist? False\n"
          ]
        }
      ],
      "source": [
        "# Debug: Check working directory\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "print(f\"Current working directory: {os.getcwd()}\")\n",
        "print(f\"Path.cwd(): {Path.cwd()}\")\n",
        "print(f\"\\nFiles in current dir:\")\n",
        "print(list(Path.cwd().iterdir())[:10])\n",
        "\n",
        "print(f\"\\nDoes notebooks/ exist from here? {Path('notebooks').exists()}\")\n",
        "print(f\"Does ../notebooks/ exist? {Path('../notebooks').exists()}\")\n",
        "print(f\"Does 01_process_data.ipynb exist? {Path('01_process_data.ipynb').exists()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "06e90325",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating Spark session for local environment...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
            "25/11/26 16:48:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "25/11/26 16:48:38 WARN StandaloneSchedulerBackend: Dynamic allocation enabled without spark.executor.cores explicitly set, you may get more executors allocated than expected. It's recommended to set spark.executor.cores explicitly. Please check SPARK-30299 for more details.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Spark session created\n",
            "  Spark version: 3.5.7\n",
            "  Application ID: app-20251126164838-0013\n"
          ]
        }
      ],
      "source": [
        "## [1.5] Initialize Spark Session\n",
        "\n",
        "# Check if Spark session exists (Fabric/Databricks has it by default)\n",
        "# For local environments, create it\n",
        "try:\n",
        "    spark\n",
        "    print(\"‚úì Spark session already available\")\n",
        "except NameError:\n",
        "    print(\"Creating Spark session for local environment...\")\n",
        "    from pyspark.sql import SparkSession\n",
        "    \n",
        "    spark = SparkSession.builder \\\n",
        "        .appName(\"DWH_Bronze_Silver_Processing\") \\\n",
        "        .enableHiveSupport() \\\n",
        "        .getOrCreate()\n",
        "    \n",
        "    print(\"‚úì Spark session created\")\n",
        "\n",
        "# Verify Spark session\n",
        "print(f\"  Spark version: {spark.version}\")\n",
        "print(f\"  Application ID: {spark.sparkContext.applicationId}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "ae7b3daf",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bronze log schema defined: logs.bronze_processing_log\n",
            "Bronze summary schema defined: logs.bronze_run_summary\n",
            "Silver log schema defined: logs.silver_processing_log\n",
            "Silver summary schema defined: logs.silver_run_summary\n",
            "‚úì Schema 'logs' ready\n",
            "‚úì Exists: logs.bronze_processing_log\n",
            "‚úì Exists: logs.bronze_run_summary\n",
            "‚úì Exists: logs.silver_processing_log\n",
            "‚úì Exists: logs.silver_run_summary\n",
            "\n",
            "‚úì All log tables ready\n",
            "‚úì Helper functions defined\n",
            "‚úì Bronze logging functions defined\n",
            "‚úì Silver logging functions defined\n",
            "‚úì Query helper functions defined\n",
            "================================================================================\n",
            "LOGGING INFRASTRUCTURE VERIFICATION\n",
            "================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/11/26 16:48:43 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì logs.bronze_processing_log                        0 rows\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì logs.bronze_run_summary                           0 rows\n",
            "‚úì logs.silver_processing_log                        0 rows\n",
            "‚úì logs.silver_run_summary                           0 rows\n",
            "\n",
            "‚úì Logging infrastructure ready for Bronze and Silver processing\n"
          ]
        }
      ],
      "source": [
        "%run \"./notebooks/01_utils_logging.ipynb\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "c58f42f2",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Imports loaded\n",
            "Checking for custom cluster Files directory /data/lakehouse... True\n",
            "Detected matches for custom cluster Files directories: ['/data/lakehouse/gh_b_avd/lh_gh_bronze/Files']\n",
            "‚úì Base path: /data/lakehouse/gh_b_avd/lh_gh_bronze/Files\n",
            "‚úì Environment: Custom Cluster\n",
            "‚úì Config directory: /data/lakehouse/gh_b_avd/lh_gh_bronze/Files/config\n",
            "‚úì Watermarks path: /data/lakehouse/gh_b_avd/lh_gh_bronze/Files/config/watermarks.json\n",
            "‚úì Runplan path: /data/lakehouse/gh_b_avd/lh_gh_bronze/Files/config/runplan.json\n",
            "‚úì Data base path: /data/lakehouse/gh_b_avd/lh_gh_bronze/Files/greenhouse_sources\n",
            "‚úì DAG loading functions defined\n",
            "‚úì Table filtering functions defined\n",
            "‚úì Watermark functions defined (READ-ONLY)\n",
            "‚ö†Ô∏è  NOTE: Watermarks are managed by extraction pipeline, not by notebooks\n",
            "‚úì Path building functions defined\n",
            "‚úì DAG query helper functions defined\n",
            "‚úì Runplan functions defined\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/sparkadmin/source/repos/dwh_spark_processing/.venv/lib/python3.11/site-packages/nbformat/__init__.py:96: MissingIDFieldWarning: Cell is missing an id field, this will become a hard error in future nbformat versions. You may want to use `normalize()` on your notebooks before validations (available since nbformat 5.1.4). Previous versions of nbformat are fixing this issue transparently, and will stop doing so in the future.\n",
            "  validate(nb)\n"
          ]
        }
      ],
      "source": [
        "%run \"./notebooks/02_utils_config.ipynb\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9cf0dad",
      "metadata": {},
      "source": [
        "## [3] Load DAG Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "a206e350",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìã Loading DAG configuration...\n",
            "‚úì DAG loaded: anva_meeus\n",
            "  Base files: greenhouse_sources\n",
            "\n",
            "üìä Tables to process: 58\n",
            "  Total enabled: 58\n",
            "  Load modes: {'snapshot': 57, 'window': 1}\n"
          ]
        }
      ],
      "source": [
        "# Load and validate DAG\n",
        "print(f\"\\nüìã Loading DAG configuration...\")\n",
        "dag = load_dag(dag_path)\n",
        "print(f\"‚úì DAG loaded: {dag.get('source')}\")\n",
        "\n",
        "# Get metadata\n",
        "dag_metadata = get_dag_metadata(dag)\n",
        "base_files = dag_metadata['base_files']\n",
        "\n",
        "print(f\"  Base files: {base_files}\")\n",
        "\n",
        "# Get tables to process\n",
        "tables_to_process = get_tables_to_process(\n",
        "    dag=dag,\n",
        "    retry_tables=retry_tables,\n",
        "    only_enabled=True\n",
        ")\n",
        "\n",
        "print(f\"\\nüìä Tables to process: {len(tables_to_process)}\")\n",
        "\n",
        "# Show summary\n",
        "dag_summary = summarize_dag(dag)\n",
        "print(f\"  Total enabled: {dag_summary['enabled_tables']}\")\n",
        "print(f\"  Load modes: {dag_summary['load_mode_counts']}\")\n",
        "\n",
        "if not tables_to_process:\n",
        "    print(\"\\n‚ö†Ô∏è  No tables to process. Exiting.\")\n",
        "    raise SystemExit(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59c71392",
      "metadata": {},
      "source": [
        "## [4] Generate Run ID"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "1b0720e1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üÜî Run ID: 20251001T183103260_215304f0\n"
          ]
        }
      ],
      "source": [
        "# Generate unique run ID\n",
        "RUN_ID = f\"{run_ts}_{uuid4().hex[:8]}\"\n",
        "print(f\"\\nüÜî Run ID: {RUN_ID}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "871b0032",
      "metadata": {},
      "source": [
        "## [5] Check for Incremental Tables (Watermark Merge)\n",
        "\n",
        "If incremental tables are present, run watermark merge notebook.\n",
        "This must happen BEFORE Bronze loading starts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "07070c2a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üíß Checking for incremental tables...\n",
            "  ‚óØ No incremental tables - skipping watermark merge\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "print(f\"\\nüíß Checking for incremental tables...\")\n",
        "\n",
        "# Filter incremental tables\n",
        "incremental_tables = get_tables_by_load_mode(tables_to_process, \"incremental\")\n",
        "\n",
        "if len(incremental_tables) > 0:\n",
        "    print(f\"  Found {len(incremental_tables)} incremental tables\")\n",
        "    print(f\"  Tables: {[t['name'] for t in incremental_tables[:5]]}\")\n",
        "    \n",
        "    # Get watermarks path from DAG\n",
        "    wm_configpath = dag_metadata.get('watermarks_path', 'config/watermarks.json')\n",
        "    \n",
        "    # Build watermark folder path (where extraction pipeline writes watermarks)\n",
        "    wm_folder = f\"runtime/{source}/{run_ts}/\"\n",
        "    \n",
        "    print(f\"  Config: {wm_configpath}\")\n",
        "    print(f\"  Runtime folder: {wm_folder}\")\n",
        "    \n",
        "    # Note: In Fabric, this would use mssparkutils.notebook.run()\n",
        "    # For local testing, we skip watermark merge (not critical for Bronze/Silver testing)\n",
        "    print(f\"\\n  ‚ö†Ô∏è  Watermark merge would run here (11_bronze_watermark_merge.ipynb)\")\n",
        "    print(f\"     Skipping for now - watermarks managed by extraction pipeline\")\n",
        "else:\n",
        "    print(f\"  ‚óØ No incremental tables - skipping watermark merge\")\n",
        "\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd40407c",
      "metadata": {},
      "source": [
        "## [6] Bronze Processing (Parallel)\n",
        "\n",
        "Load all tables from parquet to Bronze Delta tables in parallel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "dac8a720",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Imports loaded\n",
            "‚úì Base path: Files\n",
            "‚úì Environment: Local\n",
            "‚úì Helper functions defined\n",
            "‚úì Bronze worker function defined\n",
            "\n",
            "================================================================================\n",
            "BRONZE WORKER READY\n",
            "================================================================================\n",
            "Base path: Files\n",
            "Environment: Local\n",
            "\n",
            "Function available: process_bronze_table(table_def, source_name, run_ts, ...)\n",
            "\n",
            "‚ö†Ô∏è  Remember to set RUN_ID before calling process_bronze_table()\n",
            "‚úì Bronze worker notebook loaded successfully\n",
            "‚úì Bronze worker loaded\n"
          ]
        }
      ],
      "source": [
        "# Load Bronze worker notebook\n",
        "%run \"./notebooks/10_bronze_load.ipynb\"\n",
        "\n",
        "print(\"‚úì Bronze worker loaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "3832aaa1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üîµ BRONZE: Loading parquet to Delta tables...\n",
            "  Workers: 10\n",
            "  Tables: 58\n",
            "\n",
            "  üìã Checking logs for already processed tables...\n",
            "\n",
            "  üöÄ Processing 58 tables in parallel...\n",
            "\n",
            "    [1/58] ‚úó Fact_MedewerkerPermissies      FAILED              0 rows\n",
            "    [2/58] ‚úó Fact_Agent_RekeningCourantDetail FAILED              0 rows\n",
            "    [3/58] ‚úó Fact_Dekking                   FAILED              0 rows\n",
            "    [4/58] ‚úó Dim_PolisVariabel              FAILED              0 rows\n",
            "    [5/58] ‚úó Dim_SchadeSoort                FAILED              0 rows\n",
            "    [6/58] ‚úó Dim_Schade                     FAILED              0 rows\n",
            "    [7/58] ‚úó Dim_RelatieVariabel_VrijeLabels FAILED              0 rows\n",
            "    [8/58] ‚úó Dim_RelatieProducent           FAILED              0 rows\n",
            "    [9/58] ‚úó Dim_Medewerker                 FAILED              0 rows\n",
            "    [10/58] ‚úó Fact_Productie                 FAILED              0 rows\n",
            "    [11/58] ‚úó Fact_Pakket                    FAILED              0 rows\n",
            "    [12/58] ‚úó Dim_Kantoor                    FAILED              0 rows\n",
            "    [13/58] ‚úó Dim_DekkingVariabel            FAILED              0 rows\n",
            "    [14/58] ‚úó Dim_DekkingCode                FAILED              0 rows\n",
            "    [15/58] ‚úó Fact_Dekking_Detail            FAILED              0 rows\n",
            "    [16/58] ‚úó Dim_PolisVariabel_VrijeLabels  FAILED              0 rows\n",
            "    [17/58] ‚úó Fact_KentekensRDW              FAILED              0 rows\n",
            "    [18/58] ‚úó Fact_PremieFacturen            FAILED              0 rows\n",
            "    [19/58] ‚úó Fact_Polis                     FAILED              0 rows\n",
            "    [20/58] ‚úó Fact_Cashbroker                FAILED              0 rows\n",
            "    [21/58] ‚úó Dim_HoofdBranche               FAILED              0 rows\n",
            "    [22/58] ‚úó Fact_Agent_RekeningCourant     FAILED              0 rows\n",
            "    [23/58] ‚úó Dim_Agent                      FAILED              0 rows\n",
            "    [24/58] ‚úó Fact_WachtPolis                FAILED              0 rows\n",
            "    [25/58] ‚úó Fact_Agent_Borderel            FAILED              0 rows\n",
            "    [26/58] ‚úó Dim_SchadeCode                 FAILED              0 rows\n",
            "    [27/58] ‚úó Dim_PolisProducent             FAILED              0 rows\n",
            "    [28/58] ‚úó Fact_Agenda                    FAILED              0 rows\n",
            "    [29/58] ‚úó Dim_Wijzigingsreden            FAILED              0 rows\n",
            "    [30/58] ‚úó Dim_Relatie                    FAILED              0 rows\n",
            "    [31/58] ‚úó Fact_Portefeuillestand_hist    FAILED              0 rows\n",
            "    [32/58] ‚úó Fact_PremieReserveBeurs        FAILED              0 rows\n",
            "    [33/58] ‚úó Fact_Portefeuillestand         FAILED              0 rows\n",
            "    [34/58] ‚úó Fact_WachtDekking              FAILED              0 rows\n",
            "    [35/58] ‚úó Fact_PoolInrichting            FAILED              0 rows\n",
            "    [36/58] ‚úó Dim_Verzekeringsvorm           FAILED              0 rows\n",
            "    [37/58] ‚úó Dim_SBICode                    FAILED              0 rows\n",
            "    [38/58] ‚úó Dim_Branche                    FAILED              0 rows\n",
            "    [39/58] ‚úó Dim_Tekenjaar                  FAILED              0 rows\n",
            "    [40/58] ‚úó Dim_Calamiteit                 FAILED              0 rows\n",
            "    [41/58] ‚úó Fact_Betalingen                FAILED              0 rows\n",
            "    [42/58] ‚úó Dim_RelatieVariabel            FAILED              0 rows\n",
            "    [43/58] ‚úó Dim_MeldingRDW                 FAILED              0 rows\n",
            "    [44/58] ‚úó Dim_Maatschappij               FAILED              0 rows\n",
            "    [45/58] ‚úó Fact_OpenstaandeFacturen_Cashbrokers FAILED              0 rows\n",
            "    [46/58] ‚úó Dim_Polisvoorwaarden           FAILED              0 rows\n",
            "    [47/58] ‚úó Fact_Schadeboekingen           FAILED              0 rows\n",
            "    [48/58] ‚úó Fact_PremieReserve             FAILED              0 rows\n",
            "    [49/58] ‚úó Fact_OpenstaandeFacturen       FAILED              0 rows\n",
            "    [50/58] ‚úó Dim_FactuurSoort               FAILED              0 rows\n",
            "    [51/58] ‚úó Dim_Collectiviteit             FAILED              0 rows\n",
            "    [52/58] ‚úó Dim_DetailMaatschappij         FAILED              0 rows\n",
            "    [53/58] ‚úó Fact_SchadeFacturen            FAILED              0 rows\n",
            "    [54/58] ‚úó Dim_Incassowijze               FAILED              0 rows\n",
            "    [55/58] ‚úó Dim_RelatieSoort               FAILED              0 rows\n",
            "    [56/58] ‚úó Fact_Werklijst                 FAILED              0 rows\n",
            "    [57/58] ‚úó Facturatiegroep                FAILED              0 rows\n",
            "    [58/58] ‚úó Landcode                       FAILED              0 rows\n",
            "\n",
            "‚úì Bronze processing completed in 0s\n"
          ]
        },
        {
          "ename": "SystemExit",
          "evalue": "0",
          "output_type": "error",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/sparkadmin/source/repos/dwh_spark_processing/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3707: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ]
        }
      ],
      "source": [
        "print(f\"\\nüîµ BRONZE: Loading parquet to Delta tables...\")\n",
        "print(f\"  Workers: {max_workers}\")\n",
        "print(f\"  Tables: {len(tables_to_process)}\")\n",
        "\n",
        "bronze_start = datetime.utcnow()\n",
        "bronze_results = []\n",
        "\n",
        "# Filter tables if not force_reload (check logs)\n",
        "if not force_reload:\n",
        "    print(f\"\\n  üìã Checking logs for already processed tables...\")\n",
        "    \n",
        "    # Get successfully processed tables from log\n",
        "    processed_tables = get_successful_tables(run_ts, layer=\"bronze\")\n",
        "    \n",
        "    if processed_tables:\n",
        "        print(f\"    Found {len(processed_tables)} already processed tables\")\n",
        "        \n",
        "        # Filter out already processed\n",
        "        tables_to_process_bronze = [\n",
        "            t for t in tables_to_process \n",
        "            if t['name'] not in processed_tables\n",
        "        ]\n",
        "        \n",
        "        print(f\"    Remaining: {len(tables_to_process_bronze)} tables\")\n",
        "    else:\n",
        "        tables_to_process_bronze = tables_to_process\n",
        "else:\n",
        "    tables_to_process_bronze = tables_to_process\n",
        "    print(f\"  ‚ö†Ô∏è  Force reload enabled - processing all tables\")\n",
        "\n",
        "if not tables_to_process_bronze:\n",
        "    print(f\"\\n  ‚úì All tables already processed for this run_ts\")\n",
        "else:\n",
        "    print(f\"\\n  üöÄ Processing {len(tables_to_process_bronze)} tables in parallel...\\n\")\n",
        "    \n",
        "    # Wrapper function for parallel execution\n",
        "    def process_table_wrapper(table_def):\n",
        "        \"\"\"Wrapper to catch exceptions and always return a result.\"\"\"\n",
        "        try:\n",
        "            return process_bronze_table(\n",
        "                table_def=table_def,\n",
        "                source_name=source,\n",
        "                run_ts=run_ts,\n",
        "                base_files=base_files,\n",
        "                debug=True  # Disable per-table debug in parallel mode\n",
        "            )\n",
        "        except Exception as e:\n",
        "            # If worker throws unhandled exception, create error result\n",
        "            return {\n",
        "                \"log_id\": f\"{source}:{table_def['name']}:{run_ts}:error\",\n",
        "                \"run_id\": RUN_ID,\n",
        "                \"run_ts\": run_ts,\n",
        "                \"source\": source,\n",
        "                \"table_name\": table_def['name'],\n",
        "                \"load_mode\": table_def.get('load_mode'),\n",
        "                \"status\": \"FAILED\",\n",
        "                \"rows_read\": None,\n",
        "                \"rows_written\": None,\n",
        "                \"start_time\": datetime.utcnow(),\n",
        "                \"end_time\": datetime.utcnow(),\n",
        "                \"duration_seconds\": 0,\n",
        "                \"error_message\": f\"Unhandled exception: {str(e)[:500]}\",\n",
        "                \"parquet_path\": None,\n",
        "                \"delta_table\": None,\n",
        "            }\n",
        "    \n",
        "    # Parallel execution\n",
        "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "        futures = {\n",
        "            executor.submit(process_table_wrapper, table): table \n",
        "            for table in tables_to_process_bronze\n",
        "        }\n",
        "        \n",
        "        completed = 0\n",
        "        for future in as_completed(futures):\n",
        "            result = future.result()\n",
        "            bronze_results.append(result)\n",
        "            completed += 1\n",
        "            \n",
        "            # Progress indicator\n",
        "            status_icon = \"‚úì\" if result['status'] == 'SUCCESS' else \"‚úó\" if result['status'] == 'FAILED' else \"‚óØ\"\n",
        "            print(f\"    [{completed}/{len(tables_to_process_bronze)}] {status_icon} {result['table_name']:<30} {result['status']:<10} {(result.get('rows_written') or 0):>10,} rows\")\n",
        "\n",
        "bronze_end = datetime.utcnow()\n",
        "bronze_duration = int((bronze_end - bronze_start).total_seconds())\n",
        "\n",
        "print(f\"\\n‚úì Bronze processing completed in {bronze_duration}s\")\n",
        "sys.exit(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "343c7bba",
      "metadata": {},
      "source": [
        "## [7] Bronze Logging and Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "314a9f5b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìä Logging Bronze results...\n"
          ]
        },
        {
          "ename": "PySparkValueError",
          "evalue": "[CANNOT_BE_NONE] Argument `obj` can not be None.",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mPySparkValueError\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müìä Logging Bronze results...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Batch log\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mlog_bronze_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbronze_results\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Calculate summary statistics\u001b[39;00m\n\u001b[32m      8\u001b[39m success_count = \u001b[38;5;28msum\u001b[39m(\u001b[32m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m bronze_results \u001b[38;5;28;01mif\u001b[39;00m r[\u001b[33m'\u001b[39m\u001b[33mstatus\u001b[39m\u001b[33m'\u001b[39m] == \u001b[33m'\u001b[39m\u001b[33mSUCCESS\u001b[39m\u001b[33m'\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/tmp/ipykernel_53511/2634631722.py:50\u001b[39m, in \u001b[36mlog_bronze_batch\u001b[39m\u001b[34m(records)\u001b[39m\n\u001b[32m     29\u001b[39m     error_msg = truncate_error_message(r.get(\u001b[33m\"\u001b[39m\u001b[33merror_message\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m     31\u001b[39m     rows.append(Row(\n\u001b[32m     32\u001b[39m         log_id           = r.get(\u001b[33m\"\u001b[39m\u001b[33mlog_id\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     33\u001b[39m         run_id           = r.get(\u001b[33m\"\u001b[39m\u001b[33mrun_id\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   (...)\u001b[39m\u001b[32m     47\u001b[39m         delta_table      = r.get(\u001b[33m\"\u001b[39m\u001b[33mdelta_table\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     48\u001b[39m     ))\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m df = \u001b[43mspark\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbronze_log_schema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     52\u001b[39m (df.write\n\u001b[32m     53\u001b[39m    .format(\u001b[33m\"\u001b[39m\u001b[33mdelta\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     54\u001b[39m    .mode(\u001b[33m\"\u001b[39m\u001b[33mappend\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     55\u001b[39m    .saveAsTable(BRONZE_LOG_TABLE_FULLNAME))\n\u001b[32m     57\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚úì Logged \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(records)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Bronze records to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mBRONZE_LOG_TABLE_FULLNAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/source/repos/dwh_spark_processing/.venv/lib/python3.11/site-packages/pyspark/sql/session.py:1443\u001b[39m, in \u001b[36mSparkSession.createDataFrame\u001b[39m\u001b[34m(self, data, schema, samplingRatio, verifySchema)\u001b[39m\n\u001b[32m   1438\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_pandas \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pd.DataFrame):\n\u001b[32m   1439\u001b[39m     \u001b[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[32m   1440\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(SparkSession, \u001b[38;5;28mself\u001b[39m).createDataFrame(  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[32m   1441\u001b[39m         data, schema, samplingRatio, verifySchema\n\u001b[32m   1442\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1443\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_dataframe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1444\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverifySchema\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   1445\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/source/repos/dwh_spark_processing/.venv/lib/python3.11/site-packages/pyspark/sql/session.py:1485\u001b[39m, in \u001b[36mSparkSession._create_dataframe\u001b[39m\u001b[34m(self, data, schema, samplingRatio, verifySchema)\u001b[39m\n\u001b[32m   1483\u001b[39m     rdd, struct = \u001b[38;5;28mself\u001b[39m._createFromRDD(data.map(prepare), schema, samplingRatio)\n\u001b[32m   1484\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1485\u001b[39m     rdd, struct = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_createFromLocal\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprepare\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1486\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1487\u001b[39m jrdd = \u001b[38;5;28mself\u001b[39m._jvm.SerDeUtil.toJavaArray(rdd._to_java_object_rdd())\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/source/repos/dwh_spark_processing/.venv/lib/python3.11/site-packages/pyspark/sql/session.py:1090\u001b[39m, in \u001b[36mSparkSession._createFromLocal\u001b[39m\u001b[34m(self, data, schema)\u001b[39m\n\u001b[32m   1088\u001b[39m \u001b[38;5;66;03m# make sure data could consumed multiple times\u001b[39;00m\n\u001b[32m   1089\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mlist\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1090\u001b[39m     data = \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1092\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(schema, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[32m   1093\u001b[39m     struct = \u001b[38;5;28mself\u001b[39m._inferSchemaFromList(data, names=schema)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/source/repos/dwh_spark_processing/.venv/lib/python3.11/site-packages/pyspark/sql/session.py:1459\u001b[39m, in \u001b[36mSparkSession._create_dataframe.<locals>.prepare\u001b[39m\u001b[34m(obj)\u001b[39m\n\u001b[32m   1457\u001b[39m \u001b[38;5;129m@no_type_check\u001b[39m\n\u001b[32m   1458\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mprepare\u001b[39m(obj):\n\u001b[32m-> \u001b[39m\u001b[32m1459\u001b[39m     \u001b[43mverify_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1460\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/source/repos/dwh_spark_processing/.venv/lib/python3.11/site-packages/pyspark/sql/types.py:2201\u001b[39m, in \u001b[36m_make_type_verifier.<locals>.verify\u001b[39m\u001b[34m(obj)\u001b[39m\n\u001b[32m   2199\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mverify\u001b[39m(obj: Any) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2200\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m verify_nullability(obj):\n\u001b[32m-> \u001b[39m\u001b[32m2201\u001b[39m         \u001b[43mverify_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/source/repos/dwh_spark_processing/.venv/lib/python3.11/site-packages/pyspark/sql/types.py:2174\u001b[39m, in \u001b[36m_make_type_verifier.<locals>.verify_struct\u001b[39m\u001b[34m(obj)\u001b[39m\n\u001b[32m   2164\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m PySparkValueError(\n\u001b[32m   2165\u001b[39m             error_class=\u001b[33m\"\u001b[39m\u001b[33mLENGTH_SHOULD_BE_THE_SAME\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2166\u001b[39m             message_parameters={\n\u001b[32m   (...)\u001b[39m\u001b[32m   2171\u001b[39m             },\n\u001b[32m   2172\u001b[39m         )\n\u001b[32m   2173\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m v, (_, verifier) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(obj, verifiers):\n\u001b[32m-> \u001b[39m\u001b[32m2174\u001b[39m         \u001b[43mverifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2175\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(obj, \u001b[33m\"\u001b[39m\u001b[33m__dict__\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   2176\u001b[39m     d = obj.\u001b[34m__dict__\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/source/repos/dwh_spark_processing/.venv/lib/python3.11/site-packages/pyspark/sql/types.py:2200\u001b[39m, in \u001b[36m_make_type_verifier.<locals>.verify\u001b[39m\u001b[34m(obj)\u001b[39m\n\u001b[32m   2199\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mverify\u001b[39m(obj: Any) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2200\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mverify_nullability\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m   2201\u001b[39m         verify_value(obj)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/source/repos/dwh_spark_processing/.venv/lib/python3.11/site-packages/pyspark/sql/types.py:2003\u001b[39m, in \u001b[36m_make_type_verifier.<locals>.verify_nullability\u001b[39m\u001b[34m(obj)\u001b[39m\n\u001b[32m   2001\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   2002\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2003\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m PySparkValueError(\n\u001b[32m   2004\u001b[39m             error_class=\u001b[33m\"\u001b[39m\u001b[33mCANNOT_BE_NONE\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2005\u001b[39m             message_parameters={\u001b[33m\"\u001b[39m\u001b[33marg_name\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mobj\u001b[39m\u001b[33m\"\u001b[39m},\n\u001b[32m   2006\u001b[39m         )\n\u001b[32m   2007\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2008\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
            "\u001b[31mPySparkValueError\u001b[39m: [CANNOT_BE_NONE] Argument `obj` can not be None."
          ]
        }
      ],
      "source": [
        "if bronze_results:\n",
        "    print(f\"\\nüìä Logging Bronze results...\")\n",
        "    \n",
        "    # Batch log\n",
        "    log_bronze_batch(bronze_results)\n",
        "    \n",
        "    # Calculate summary statistics\n",
        "    success_count = sum(1 for r in bronze_results if r['status'] == 'SUCCESS')\n",
        "    failed_count = sum(1 for r in bronze_results if r['status'] == 'FAILED')\n",
        "    empty_count = sum(1 for r in bronze_results if r['status'] == 'EMPTY')\n",
        "    skipped_count = sum(1 for r in bronze_results if r['status'] == 'SKIPPED')\n",
        "    \n",
        "    total_rows = sum(r.get('rows_written', 0) or 0 for r in bronze_results)\n",
        "    \n",
        "    # Performance metrics\n",
        "    sum_task_seconds = sum(r.get('duration_seconds', 0) or 0 for r in bronze_results)\n",
        "    theoretical_min_sec = sum_task_seconds / max_workers if max_workers > 0 else sum_task_seconds\n",
        "    actual_time_sec = bronze_duration\n",
        "    efficiency_pct = (theoretical_min_sec / actual_time_sec * 100) if actual_time_sec > 0 else 0\n",
        "    \n",
        "    # Failed tables list\n",
        "    failed_tables = [r['table_name'] for r in bronze_results if r['status'] == 'FAILED']\n",
        "    \n",
        "    # Log summary\n",
        "    bronze_summary = {\n",
        "        \"run_id\": RUN_ID,\n",
        "        \"source\": source,\n",
        "        \"run_ts\": run_ts,\n",
        "        \"run_start\": bronze_start,\n",
        "        \"run_end\": bronze_end,\n",
        "        \"duration_seconds\": bronze_duration,\n",
        "        \"total_tables\": len(bronze_results),\n",
        "        \"tables_success\": success_count,\n",
        "        \"tables_empty\": empty_count,\n",
        "        \"tables_failed\": failed_count,\n",
        "        \"tables_skipped\": skipped_count,\n",
        "        \"total_rows\": total_rows,\n",
        "        \"workers\": max_workers,\n",
        "        \"sum_task_seconds\": sum_task_seconds,\n",
        "        \"theoretical_min_sec\": theoretical_min_sec,\n",
        "        \"actual_time_sec\": actual_time_sec,\n",
        "        \"efficiency_pct\": efficiency_pct,\n",
        "        \"failed_tables\": failed_tables,\n",
        "    }\n",
        "    \n",
        "    log_bronze_summary(bronze_summary)\n",
        "    \n",
        "    # Print summary\n",
        "    print(f\"\\n  Summary:\")\n",
        "    print(f\"    Success: {success_count}\")\n",
        "    print(f\"    Failed:  {failed_count}\")\n",
        "    print(f\"    Empty:   {empty_count}\")\n",
        "    print(f\"    Skipped: {skipped_count}\")\n",
        "    print(f\"    Total rows: {total_rows:,}\")\n",
        "    print(f\"    Efficiency: {efficiency_pct:.1f}%\")\n",
        "    \n",
        "    if failed_tables:\n",
        "        print(f\"\\n  ‚ö†Ô∏è  Failed tables: {failed_tables}\")\n",
        "else:\n",
        "    print(f\"\\n  ‚ÑπÔ∏è  No Bronze results to log\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9e35a7b",
      "metadata": {},
      "source": [
        "## [8] Silver Processing (Parallel CDC Merge)\n",
        "\n",
        "Process tables that have business_keys defined for CDC merge."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01622c2b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Silver worker notebook\n",
        "%run \"./notebooks/20_silver_cdc_merge.ipynb\"\n",
        "\n",
        "print(\"‚úì Silver CDC merge worker loaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2193187c",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"\\nüî∑ SILVER: CDC merge from Bronze...\")\n",
        "\n",
        "# Filter tables for Silver processing:\n",
        "# 1. Must have business_keys defined\n",
        "# 2. Must have been successfully loaded to Bronze\n",
        "\n",
        "successful_bronze_tables = [r['table_name'] for r in bronze_results if r['status'] == 'SUCCESS']\n",
        "\n",
        "tables_for_silver = [\n",
        "    t for t in tables_to_process \n",
        "    if t.get('business_keys') and t['name'] in successful_bronze_tables\n",
        "]\n",
        "\n",
        "print(f\"  Tables with business_keys: {len([t for t in tables_to_process if t.get('business_keys')])}\")\n",
        "print(f\"  Successful Bronze loads: {len(successful_bronze_tables)}\")\n",
        "print(f\"  Tables to process in Silver: {len(tables_for_silver)}\")\n",
        "\n",
        "silver_results = []\n",
        "\n",
        "if not tables_for_silver:\n",
        "    print(f\"\\n  ‚ÑπÔ∏è  No tables to process in Silver\")\n",
        "else:\n",
        "    silver_start = datetime.utcnow()\n",
        "    \n",
        "    print(f\"\\n  üöÄ Processing {len(tables_for_silver)} tables in parallel...\\n\")\n",
        "    \n",
        "    # Wrapper function for parallel execution\n",
        "    def process_silver_wrapper(table_def):\n",
        "        \"\"\"Wrapper to catch exceptions and always return a result.\"\"\"\n",
        "        try:\n",
        "            return process_silver_cdc_merge(\n",
        "                table_def=table_def,\n",
        "                source_name=source,\n",
        "                run_ts=run_ts,\n",
        "                debug=False\n",
        "            )\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                \"log_id\": f\"{source}:{table_def['name']}:{run_ts}:silver:error\",\n",
        "                \"run_id\": RUN_ID,\n",
        "                \"run_ts\": run_ts,\n",
        "                \"source\": source,\n",
        "                \"table_name\": table_def['name'],\n",
        "                \"load_mode\": table_def.get('load_mode'),\n",
        "                \"status\": \"FAILED\",\n",
        "                \"rows_inserted\": None,\n",
        "                \"rows_updated\": None,\n",
        "                \"rows_deleted\": None,\n",
        "                \"rows_unchanged\": None,\n",
        "                \"total_silver_rows\": None,\n",
        "                \"bronze_rows\": None,\n",
        "                \"bronze_table\": None,\n",
        "                \"silver_table\": None,\n",
        "                \"start_time\": datetime.utcnow(),\n",
        "                \"end_time\": datetime.utcnow(),\n",
        "                \"duration_seconds\": 0,\n",
        "                \"error_message\": f\"Unhandled exception: {str(e)[:500]}\",\n",
        "            }\n",
        "    \n",
        "    # Parallel execution\n",
        "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "        futures = {\n",
        "            executor.submit(process_silver_wrapper, table): table \n",
        "            for table in tables_for_silver\n",
        "        }\n",
        "        \n",
        "        completed = 0\n",
        "        for future in as_completed(futures):\n",
        "            result = future.result()\n",
        "            silver_results.append(result)\n",
        "            completed += 1\n",
        "            \n",
        "            status_icon = \"‚úì\" if result['status'] == 'SUCCESS' else \"‚úó\"\n",
        "            deletes = result.get('rows_deleted', 0) or 0\n",
        "            delete_info = f\" ({deletes} deleted)\" if deletes > 0 else \"\"\n",
        "            print(f\"    [{completed}/{len(tables_for_silver)}] {status_icon} {result['table_name']:<30} {result['status']:<10}{delete_info}\")\n",
        "            \n",
        "    \n",
        "    silver_end = datetime.utcnow()\n",
        "    silver_duration = int((silver_end - silver_start).total_seconds())\n",
        "    \n",
        "    print(f\"\\n‚úì Silver processing completed in {silver_duration}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2492e44b",
      "metadata": {},
      "source": [
        "## [9] Silver Logging and Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d02a090",
      "metadata": {},
      "outputs": [],
      "source": [
        "if silver_results:\n",
        "    print(f\"\\nüìä Logging Silver results...\")\n",
        "    \n",
        "    # Batch log\n",
        "    log_silver_batch(silver_results)\n",
        "    \n",
        "    # Calculate summary\n",
        "    success_count = sum(1 for r in silver_results if r['status'] == 'SUCCESS')\n",
        "    failed_count = sum(1 for r in silver_results if r['status'] == 'FAILED')\n",
        "    skipped_count = sum(1 for r in silver_results if r['status'] == 'SKIPPED')\n",
        "    \n",
        "    total_inserts = sum(r.get('rows_inserted', 0) or 0 for r in silver_results)\n",
        "    total_updates = sum(r.get('rows_updated', 0) or 0 for r in silver_results)\n",
        "    total_deletes = sum(r.get('rows_deleted', 0) or 0 for r in silver_results)\n",
        "    total_unchanged = sum(r.get('rows_unchanged', 0) or 0 for r in silver_results)\n",
        "    \n",
        "    failed_tables = [r['table_name'] for r in silver_results if r['status'] == 'FAILED']\n",
        "    \n",
        "    # Log summary\n",
        "    silver_summary = {\n",
        "        \"run_id\": RUN_ID,\n",
        "        \"source\": source,\n",
        "        \"run_ts\": run_ts,\n",
        "        \"run_start\": silver_start,\n",
        "        \"run_end\": silver_end,\n",
        "        \"duration_seconds\": silver_duration,\n",
        "        \"total_tables\": len(silver_results),\n",
        "        \"tables_success\": success_count,\n",
        "        \"tables_failed\": failed_count,\n",
        "        \"tables_skipped\": skipped_count,\n",
        "        \"total_inserts\": total_inserts,\n",
        "        \"total_updates\": total_updates,\n",
        "        \"total_deletes\": total_deletes,\n",
        "        \"total_unchanged\": total_unchanged,\n",
        "        \"failed_tables\": failed_tables,\n",
        "    }\n",
        "    \n",
        "    log_silver_summary(silver_summary)\n",
        "    \n",
        "    # Print summary\n",
        "    print(f\"\\n  Summary:\")\n",
        "    print(f\"    Success: {success_count}\")\n",
        "    print(f\"    Failed:  {failed_count}\")\n",
        "    print(f\"    Skipped: {skipped_count}\")\n",
        "    if total_inserts or total_updates or total_deletes:\n",
        "        print(f\"    CDC: +{total_inserts or 0} ~{total_updates or 0} -{total_deletes}\")\n",
        "    \n",
        "    if failed_tables:\n",
        "        print(f\"\\n  ‚ö†Ô∏è  Failed tables: {failed_tables}\")\n",
        "else:\n",
        "    print(f\"\\n  ‚ÑπÔ∏è  No Silver results to log\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea6ece15",
      "metadata": {},
      "source": [
        "## [10] Final Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb5afcc0",
      "metadata": {},
      "outputs": [],
      "source": [
        "total_end = datetime.utcnow()\n",
        "total_duration = int((total_end - bronze_start).total_seconds())\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ORCHESTRATOR SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Run ID: {RUN_ID}\")\n",
        "print(f\"Source: {source}\")\n",
        "print(f\"Run TS: {run_ts}\")\n",
        "print(f\"\\nTiming:\")\n",
        "print(f\"  Bronze: {bronze_duration}s\")\n",
        "if silver_results:\n",
        "    print(f\"  Silver: {silver_duration}s\")\n",
        "print(f\"  Total:  {total_duration}s\")\n",
        "\n",
        "print(f\"\\nBronze Results:\")\n",
        "if bronze_results:\n",
        "    bronze_success = sum(1 for r in bronze_results if r['status'] == 'SUCCESS')\n",
        "    bronze_failed = sum(1 for r in bronze_results if r['status'] == 'FAILED')\n",
        "    print(f\"  ‚úì Success: {bronze_success}/{len(bronze_results)}\")\n",
        "    if bronze_failed > 0:\n",
        "        print(f\"  ‚úó Failed:  {bronze_failed}\")\n",
        "else:\n",
        "    print(f\"  (No processing)\")\n",
        "\n",
        "print(f\"\\nSilver Results:\")\n",
        "if silver_results:\n",
        "    silver_success = sum(1 for r in silver_results if r['status'] == 'SUCCESS')\n",
        "    silver_failed = sum(1 for r in silver_results if r['status'] == 'FAILED')\n",
        "    print(f\"  ‚úì Success: {silver_success}/{len(silver_results)}\")\n",
        "    if silver_failed > 0:\n",
        "        print(f\"  ‚úó Failed:  {silver_failed}\")\n",
        "else:\n",
        "    print(f\"  (No processing)\")\n",
        "\n",
        "# Overall status\n",
        "if bronze_results:\n",
        "    all_bronze_ok = all(r['status'] in ('SUCCESS', 'EMPTY', 'SKIPPED') for r in bronze_results)\n",
        "else:\n",
        "    all_bronze_ok = True\n",
        "\n",
        "if silver_results:\n",
        "    all_silver_ok = all(r['status'] in ('SUCCESS', 'SKIPPED') for r in silver_results)\n",
        "else:\n",
        "    all_silver_ok = True\n",
        "\n",
        "overall_status = \"SUCCESS\" if (all_bronze_ok and all_silver_ok) else \"PARTIAL\" if bronze_results or silver_results else \"NO_WORK\"\n",
        "\n",
        "print(f\"\\nOverall Status: {overall_status}\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "if overall_status != \"SUCCESS\":\n",
        "    print(f\"\\n‚ö†Ô∏è  Some tables failed. Check logs for details.\")\n",
        "    print(f\"   Use retry_tables parameter to retry specific tables.\")\n",
        "else:\n",
        "    print(f\"\\n‚úì All processing completed successfully!\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "dwh-spark-processing (3.11.14)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
