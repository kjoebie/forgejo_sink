{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# 00 ‚Äî Master Orchestrator: Bronze ‚Üí Silver Processing\n",
    "\n",
    "Main orchestration notebook for processing parquet files through Bronze and Silver layers.\n",
    "\n",
    "## Architecture Overview\n",
    "\n",
    "```\n",
    "Parquet Files (Files/{source}/{run_ts}/)\n",
    "    ‚Üì\n",
    "Bronze Layer (append with run_ts for CDC)\n",
    "    ‚Üì\n",
    "Silver Layer (CDC merge: INSERT/UPDATE/DELETE)\n",
    "    ‚Üì\n",
    "Watermark Update (incremental tables only)\n",
    "```\n",
    "\n",
    "## Process Flow\n",
    "\n",
    "1. **Load Configuration** (DAG, enabled tables, retry filter)\n",
    "2. **Check Incremental** ‚Üí Run watermark merge if needed\n",
    "3. **Bronze Processing** ‚Üí Parallel table loading (10 workers)\n",
    "4. **Bronze Logging** ‚Üí Batch log all results\n",
    "5. **Silver Processing** ‚Üí Parallel CDC merge (tables with business_keys)\n",
    "6. **Silver Logging** ‚Üí Batch log all results\n",
    "7. **Summary Statistics** ‚Üí Performance metrics, efficiency\n",
    "\n",
    "## Key Features\n",
    "\n",
    "- **Parallel Processing**: ThreadPoolExecutor for 5-10x speedup\n",
    "- **Idempotency**: Check logs before reprocessing\n",
    "- **Retry Support**: Process only specific tables\n",
    "- **Error Resilience**: Continue on failure, comprehensive logging\n",
    "- **Performance Tracking**: Efficiency metrics (theoretical vs actual time)\n",
    "\n",
    "## Parameters\n",
    "\n",
    "- `source`: Source system name (e.g., \"vizier\")\n",
    "- `run_ts`: Run timestamp (e.g., \"20251105T142752505\")\n",
    "- `dag_path`: Path to DAG configuration JSON\n",
    "- `retry_tables`: Optional list of tables to retry\n",
    "- `force_reload`: Ignore log and reload all\n",
    "- `max_workers`: Parallel workers (default: 10)\n",
    "- `debug`: Enable debug output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters (Papermill compatible)\n",
    "source = \"anva_meeus\"                               # Source system name\n",
    "run_ts = \"20251001T183103260\"                       # Run timestamp\n",
    "dag_path = \"config/dag_anva_meeus_week.json\"        # DAG configuration path\n",
    "retry_tables = None                                 # Optional: list of table names to retry\n",
    "force_reload = True                                 # If True, ignore logs and reload all\n",
    "debug = True                                        # Enable debug output\n",
    "log_to_console = True                               # Also stream logs to stdout/stderr\n",
    "optimize_for = \"throughput\"                         # Worker profile optimization goal, choose throughput or efficiency\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## [1] Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 12:46:29,948 [INFO] - Logfile: /data/lakehouse/gh_b_avd/lh_gh_bronze/Files/notebook_outputs/logs/master_orchestrator_20251203_123312.log\n",
      "2025-12-03 12:46:29,949 [INFO] - ================================================================================\n",
      "2025-12-03 12:46:29,950 [INFO] - MASTER ORCHESTRATOR STARTING\n",
      "2025-12-03 12:46:29,951 [INFO] - ================================================================================\n",
      "2025-12-03 12:46:29,951 [INFO] - Source: anva_meeus\n",
      "2025-12-03 12:46:29,951 [INFO] - Run TS: 20251001T183103260\n",
      "2025-12-03 12:46:29,952 [INFO] - DAG: config/dag_anva_meeus_week.json\n",
      "2025-12-03 12:46:29,952 [INFO] - Retry tables: None\n",
      "2025-12-03 12:46:29,952 [INFO] - Force reload: True\n",
      "2025-12-03 12:46:29,953 [INFO] - Debug: True\n",
      "2025-12-03 12:46:29,954 [INFO] - ================================================================================\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timezone\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from uuid import uuid4\n",
    "\n",
    "from modules.logging_utils import configure_logging\n",
    "import logging\n",
    "from modules.worker_utils import choose_worker_profile_from_history\n",
    "\n",
    "log_file = configure_logging(run_name=\"master_orchestrator\", enable_console_logging=log_to_console)\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.info(\"Logfile: %s\", log_file)\n",
    "\n",
    "logger.info(\"=\"*80)\n",
    "logger.info(\"MASTER ORCHESTRATOR STARTING\")\n",
    "logger.info(\"=\"*80)\n",
    "logger.info(f\"Source: {source}\")\n",
    "logger.info(f\"Run TS: {run_ts}\")\n",
    "logger.info(f\"DAG: {dag_path}\")\n",
    "logger.info(f\"Retry tables: {retry_tables}\")\n",
    "logger.info(f\"Force reload: {force_reload}\")\n",
    "logger.info(f\"Debug: {debug}\")\n",
    "logger.info(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## [2] Load Utility Notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": "# Import all required utilities from modules\nfrom modules.config_utils import (\n    load_dag, get_enabled_tables, get_tables_to_process,\n    get_tables_by_load_mode, get_dag_metadata, summarize_dag,\n    get_business_keys\n)\n\nfrom modules.logging_utils import (\n    build_run_date,\n    get_successful_tables,\n    log_batch,\n    log_summary\n)\n\nfrom modules.path_utils import get_base_path\n\n# Import worker functions directly\nfrom modules.bronze_processor import process_bronze_table\nfrom modules.silver_processor import process_silver_cdc_merge\n\nlogger.info(\"‚úì Utility functions imported from modules\")"
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 12:46:29,972 [INFO] - ‚úì Using existing Spark session\n",
      "2025-12-03 12:46:29,973 [INFO] -   Spark version: 3.5.5\n",
      "2025-12-03 12:46:29,973 [INFO] -   Application ID: app-20251203123314-0861\n",
      "2025-12-03 12:46:29,974 [INFO] -   Application name: DWH_Bronze_Silver_Processing\n"
     ]
    }
   ],
   "source": [
    "from modules.spark_session import get_or_create_spark_session\n",
    "\n",
    "spark = get_or_create_spark_session(\n",
    "    app_name=\"DWH_Bronze_Silver_Processing\",\n",
    "    enable_hive=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": "# Notebook 01 no longer needed - all functions imported from modules\nlogger.info(\"‚úì Logging utilities imported from modules (notebook 01 no longer needed)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": "# This cell is no longer needed - all functions are imported from modules\nlogger.info(\"‚úì All utilities imported from modules (notebook 02 no longer needed)\")"
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## [3] Load DAG Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": "# Load and validate DAG\nlogger.info(f\"\\nüìã Loading DAG configuration...\")\n\n# Get base path for Files directory (environment-aware)\nbase_files_path = get_base_path(spark)\nlogger.info(f\"  Base Files path: {base_files_path}\")\n\n# Load DAG (handles both absolute and relative paths)\ndag = load_dag(dag_path, base_path=base_files_path)\nlogger.info(f\"‚úì DAG loaded: {dag.get('source')}\")\n\n# Get metadata\ndag_metadata = get_dag_metadata(dag)\nbase_files = dag_metadata['base_files']\n\nlogger.info(f\"  Base files: {base_files}\")\n\n# Get tables to process\ntables_to_process = get_tables_to_process(\n    dag=dag,\n    retry_tables=retry_tables,\n    only_enabled=True\n)\n\n# Ensure schemas exist        \nschemas = set()\n\nfor t in tables_to_process:\n    delta_table = t.get(\"delta_table\")\n    delta_schema = t.get(\"delta_schema\")\n\n    if delta_table and \".\" in delta_table:\n        # Vorm: schema.tabel in delta_table\n        schema = delta_table.split(\".\")[0]\n    else:\n        # Anders: gebruik delta_schema of standaard 'bronze'\n        schema = (delta_schema or \"bronze\")\n\n    schemas.add(schema)\n\nfor schema in sorted(schemas):\n    spark.sql(f\"CREATE SCHEMA IF NOT EXISTS `{schema}`\")\n\nlogger.info(\"Schemas ensured: %s\", \", \".join(sorted(schemas)))\n\nlogger.info(f\"\\nüìä Tables to process: {len(tables_to_process)}\")\n\n# Show summary\ndag_summary = summarize_dag(dag)\nlogger.info(f\"  Total enabled: {dag_summary['enabled_tables']}\")\nlogger.info(f\"  Load modes: {dag_summary['load_mode_counts']}\")\n\nif not tables_to_process:\n    logger.info(\"\\n‚ö†Ô∏è  No tables to process. Exiting.\")\n    raise SystemExit(0)"
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## [4] Generate Run ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 12:46:31,128 [INFO] - \n",
      "üÜî Run ID: 20251001T183103260_b0449d09\n"
     ]
    }
   ],
   "source": [
    "# Generate unique run ID\n",
    "RUN_ID = f\"{run_ts}_{uuid4().hex[:8]}\"\n",
    "run_date = build_run_date(run_ts)\n",
    "logger.info(f\"\\nüÜî Run ID: {RUN_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## [5] Check for Incremental Tables (Watermark Merge)\n",
    "\n",
    "If incremental tables are present, run watermark merge notebook.\n",
    "This must happen BEFORE Bronze loading starts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 12:46:31,150 [INFO] - \n",
      "üíß Checking for incremental tables...\n",
      "2025-12-03 12:46:31,151 [INFO] -   ‚óØ No incremental tables - skipping watermark merge\n",
      "2025-12-03 12:46:31,152 [INFO] - ================================================================================\n"
     ]
    }
   ],
   "source": [
    "logger.info(f\"\\nüíß Checking for incremental tables...\")\n",
    "\n",
    "# Filter incremental tables\n",
    "incremental_tables = get_tables_by_load_mode(tables_to_process, \"incremental\")\n",
    "\n",
    "if len(incremental_tables) > 0:\n",
    "    logger.info(f\"  Found {len(incremental_tables)} incremental tables\")\n",
    "    logger.info(f\"  Tables: {[t['name'] for t in incremental_tables[:5]]}\")\n",
    "    \n",
    "    # Get watermarks path from DAG\n",
    "    wm_configpath = dag_metadata.get('watermarks_path', 'config/watermarks.json')\n",
    "    \n",
    "    # Build watermark folder path (where extraction pipeline writes watermarks)\n",
    "    wm_folder = f\"runtime/{source}/{run_ts}/\"\n",
    "    \n",
    "    logger.info(f\"  Config: {wm_configpath}\")\n",
    "    logger.info(f\"  Runtime folder: {wm_folder}\")\n",
    "    \n",
    "    # Note: In Fabric, this would use mssparkutils.notebook.run()\n",
    "    # For local testing, we skip watermark merge (not critical for Bronze/Silver testing)\n",
    "    logger.info(f\"\\n  ‚ö†Ô∏è  Watermark merge would run here (11_bronze_watermark_merge.ipynb)\")\n",
    "    logger.info(f\"     Skipping for now - watermarks managed by extraction pipeline\")\n",
    "else:\n",
    "    logger.info(f\"  ‚óØ No incremental tables - skipping watermark merge\")\n",
    "\n",
    "logger.info(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## [6] Bronze Processing (Parallel)\n",
    "\n",
    "Load all tables from parquet to Bronze Delta tables in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": "# Notebook 10 no longer needed - process_bronze_table imported from modules\nlogger.info(\"‚úì Bronze worker imported from modules (notebook 10 no longer needed)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": "logger.info(f\"\\nüîµ BRONZE: Loading parquet to Delta tables...\")\nlogger.info(f\"  Tables: {len(tables_to_process)}\")\n\nbronze_start = datetime.now(timezone.utc)\nbronze_results = []\n\n# Filter tables if not force_reload (check logs)\nif not force_reload:\n    logger.info(f\"\\n  üìã Checking logs for already processed tables...\")\n    \n    # Get successfully processed tables from log\n    processed_tables = get_successful_tables(spark, run_ts, layer=\"bronze\")\n    \n    if processed_tables:\n        logger.info(f\"    Found {len(processed_tables)} already processed tables\")\n        \n        # Filter out already processed\n        tables_to_process_bronze = [\n            t for t in tables_to_process \n            if t['name'] not in processed_tables\n        ]\n\n        logger.info(f\"    Remaining: {len(tables_to_process_bronze)} tables\")\n    else:\n        tables_to_process_bronze = tables_to_process\nelse:\n    tables_to_process_bronze = tables_to_process\n    logger.info(f\"  ‚ö†Ô∏è  Force reload enabled - processing all tables\")\n\nif not tables_to_process_bronze:\n    logger.info(f\"\\n  ‚úì All tables already processed for this run_ts\")\nelse:\n    logger.info(f\"\\n  üöÄ Processing {len(tables_to_process_bronze)} tables in parallel...\\n\")\n    \n    # Wrapper function for parallel execution\n    def process_table_wrapper(table_def):\n        \"\"\"Wrapper to catch exceptions and always return a result.\"\"\"\n        try:\n            return process_bronze_table(\n                spark=spark,\n                table_def=table_def,\n                source_name=source,\n                run_id=RUN_ID,\n                run_ts=run_ts,\n                run_date=run_date,\n                base_files=base_files,\n                debug=False  # Disable per-table debug in parallel mode\n            )\n        except Exception as e:\n            # If worker throws unhandled exception, create error result\n            return {\n                \"log_id\": f\"{source}:{table_def['name']}:{run_ts}:error\",\n                \"run_id\": RUN_ID,\n                \"run_date\": run_date,\n                \"run_ts\": run_ts,\n                \"source\": source,\n                \"table_name\": table_def['name'],\n                \"load_mode\": table_def.get('load_mode'),\n                \"status\": \"FAILED\",\n                \"rows_read\": None,\n                \"rows_processed\": None,\n                \"start_time\": datetime.now(timezone.utc),\n                \"end_time\": datetime.now(timezone.utc),\n                \"duration_seconds\": 0,\n                \"error_message\": f\"Unhandled exception: {str(e)[:500]}\",\n                \"parquet_path\": None,\n                \"delta_table\": None,\n            }\n    \n    # Optimize voor throughput (snelheid)\n    MAX_WORKERS = choose_worker_profile_from_history(\n        spark=spark,\n        source_name=source,\n        summary_table=\"logs.bronze_run_summary\",\n        default_workers=10,\n        min_workers=2,\n        max_workers_cap=12,\n        lookback_runs=5,\n        optimize_for=optimize_for,  # Focus on rows/second\n        debug=debug\n    )\n    # Cap on number of tables\n    MAX_WORKERS = min(MAX_WORKERS, len(tables_to_process_bronze))\n    logger.info(f\"Using MAX_WORKERS={MAX_WORKERS} for bronze processing\")\n\n\n    # Parallel execution\n    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n        futures = {\n            executor.submit(process_table_wrapper, table): table \n            for table in tables_to_process_bronze\n        }\n        \n        completed = 0\n        for future in as_completed(futures):\n            result = future.result()\n            bronze_results.append(result)\n            completed += 1\n            \n            # Progress indicator\n            status_icon = \"‚úì\" if result['status'] == 'SUCCESS' else \"‚úó\" if result['status'] == 'FAILED' else \"‚óØ\"\n            # Kort foutfragment erbij (max 120 chars, 1 regel)\n            error_snippet = (result.get(\"error_message\") or \"\")[:120].replace(\"\\n\", \" \")\n            \n            logger.info(\n                f\"[{completed}/{len(tables_to_process_bronze)}]\"\n                f\"{status_icon} {result['table_name']:<30} {result['status']:<10} \"\n                f\"{(result.get('rows_processed') or 0):>10,} rows {error_snippet}\"\n                )\n\nbronze_end = datetime.now(timezone.utc)\nbronze_duration = float((bronze_end - bronze_start).total_seconds())\n\nlogger.info(f\"\\n‚úì Bronze processing completed in {bronze_duration}s\")\n\n#sys.exit(0)"
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## [7] Bronze Logging and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": "if bronze_results:\n    logger.info(f\"\\nüìä Logging Bronze results...\")\n        \n    # Calculate summary statistics\n    success_count = sum(1 for r in bronze_results if r['status'] == 'SUCCESS')\n    failed_count = sum(1 for r in bronze_results if r['status'] == 'FAILED')\n    empty_count = sum(1 for r in bronze_results if r['status'] == 'EMPTY')\n    skipped_count = sum(1 for r in bronze_results if r['status'] == 'SKIPPED')\n    \n    total_rows = sum(r.get('rows_processed', 0) or 0 for r in bronze_results)\n    \n    # Performance metrics\n    sum_task_seconds = float(sum(r.get('duration_seconds', 0) or 0 for r in bronze_results))\n    theoretical_min_sec = float(sum_task_seconds / MAX_WORKERS if MAX_WORKERS > 0 else sum_task_seconds)\n    actual_time_sec = bronze_duration # float\n    efficiency_pct = float((theoretical_min_sec / actual_time_sec * 100) if actual_time_sec > 0 else 0)\n    \n    # Failed tables list\n    failed_tables = [r['table_name'] for r in bronze_results if r['status'] == 'FAILED']\n    \n    # Log summary\n    bronze_summary = {\n    \"run_id\": RUN_ID,\n    \"run_date\": run_date,\n    \"run_ts\": run_ts,\n    \"source\": source,\n    \"total_tables\": len(bronze_results),\n    \"tables_success\": success_count,\n    \"tables_empty\": empty_count,\n    \"tables_failed\": failed_count,\n    \"tables_skipped\": skipped_count,\n    \"total_rows\": total_rows,\n    \"workers\": MAX_WORKERS,\n    \"sum_task_seconds\": sum_task_seconds,\n    \"theoretical_min_sec\": theoretical_min_sec,\n    \"actual_time_sec\": actual_time_sec,\n    \"efficiency_pct\": efficiency_pct,\n    \"run_start\": bronze_start,\n    \"run_end\": bronze_end,\n    \"duration_seconds\": bronze_duration,\n    \"error_message\": None,\n    \"failed_tables\": failed_tables,\n    }\n\n    run_log_id = log_summary(spark, bronze_summary, layer=\"bronze\")\n\n    log_batch(spark, records=bronze_results, layer=\"bronze\", run_log_id=run_log_id)\n\n    \n    # Print summary\n    logger.info(f\"\\n  Summary:\")\n    logger.info(f\"    Success: {success_count}\")\n    logger.info(f\"    Failed:  {failed_count}\")\n    logger.info(f\"    Empty:   {empty_count}\")\n    logger.info(f\"    Skipped: {skipped_count}\")\n    logger.info(f\"    Total rows: {total_rows:,}\")\n    logger.info(f\"    Efficiency: {efficiency_pct:.1f}%\")\n    \n    if failed_tables:\n        logger.info(f\"\\n  ‚ö†Ô∏è  Failed tables: {failed_tables}\")\nelse:\n    logger.info(f\"\\n  ‚ÑπÔ∏è  No Bronze results to log\")\n\n#sys.exit(0)"
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.sql(\"SHOW TABLES IN logs\").show(truncate=False)\n",
    "#spark.table(\"logs.bronze_processing_log\").printSchema()\n",
    "#spark.table(\"logs.bronze_run_summary\").printSchema()\n",
    "\n",
    "#spark.sql(\"drop table if exists logs.silver_run_summary\").show()\n",
    "#spark.sql(\"select * from logs.bronze_run_summary order by run_end desc limit 5\").show(truncate=False)\n",
    "\n",
    "#spark.sql(\"drop table if exists logs.bronze_processing_log\").show()\n",
    "#spark.sql(\"drop table if exists logs.bronze_run_summary\").show()\n",
    "\n",
    "# spark.sql(\"drop table if exists logs.silver_processing_log\").show()\n",
    "# spark.sql(\"drop table if exists logs.silver_run_summary\").show()\n",
    "# spark.table(\"logs.bronze_run_summary\") \\\n",
    "#       .orderBy(\"run_end\", \"source\") \\\n",
    "#       .show(20, truncate=False)\n",
    "\n",
    "# spark.table(\"logs.bronze_processing_log\") \\\n",
    "#       .orderBy(\"run_ts\", \"table_name\") \\\n",
    "#       .show(200, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## [8] Silver Processing (Parallel CDC Merge)\n",
    "\n",
    "Process tables that have business_keys defined for CDC merge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": "# Notebook 20 no longer needed - process_silver_cdc_merge imported from modules\nlogger.info(\"‚úì Silver CDC merge worker imported from modules (notebook 20 no longer needed)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": "logger.info(f\"\\nüî∑ SILVER: CDC merge from Bronze...\")\n\n# Filter tables for Silver processing:\n# 1. Must have business_keys defined\n# 2. Must have been successfully loaded to Bronze\n\nsuccessful_bronze_tables = [r['table_name'] for r in bronze_results if r['status'] == 'SUCCESS']\n\ntables_for_silver = [\n    t for t in tables_to_process \n    if t.get('business_keys') and t['name'] in successful_bronze_tables\n]\n\nlogger.info(f\"  Tables with business_keys: {len([t for t in tables_to_process if t.get('business_keys')])}\")\nlogger.info(f\"  Successful Bronze loads: {len(successful_bronze_tables)}\")\nlogger.info(f\"  Tables to process in Silver: {len(tables_for_silver)}\")\n\nsilver_results = []\n\nif not tables_for_silver:\n    logger.info(f\"\\n  ‚ÑπÔ∏è  No tables to process in Silver\")\nelse:\n    silver_start = datetime.now(timezone.utc)\n    \n    logger.info(f\"\\n  üöÄ Processing {len(tables_for_silver)} tables in parallel...\\n\")\n    \n    # Wrapper function for parallel execution\n    def process_silver_wrapper(table_def):\n        \"\"\"Wrapper to catch exceptions and always return a result.\"\"\"\n        try:\n            return process_silver_cdc_merge(\n                spark=spark,\n                table_def=table_def,\n                source_name=source,\n                run_id=RUN_ID,\n                run_ts=run_ts,\n                debug=False\n            )\n        except Exception as e:\n            return {\n                \"log_id\": f\"{source}:{table_def['name']}:{run_ts}:silver:error\",\n                \"run_id\": RUN_ID,\n                \"run_ts\": run_ts,\n                \"source\": source,\n                \"table_name\": table_def['name'],\n                \"load_mode\": table_def.get('load_mode'),\n                \"status\": \"FAILED\",\n                \"rows_inserted\": None,\n                \"rows_updated\": None,\n                \"rows_deleted\": None,\n                \"rows_unchanged\": None,\n                \"total_silver_rows\": None,\n                \"bronze_rows\": None,\n                \"bronze_table\": None,\n                \"silver_table\": None,\n                \"start_time\": datetime.now(timezone.utc),\n                \"end_time\": datetime.now(timezone.utc),\n                \"duration_seconds\": 0,\n                \"error_message\": f\"Unhandled exception: {str(e)[:500]}\",\n            }\n    \n    # Parallel execution\n    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n        futures = {\n            executor.submit(process_silver_wrapper, table): table \n            for table in tables_for_silver\n        }\n        \n        completed = 0\n        for future in as_completed(futures):\n            result = future.result()\n            silver_results.append(result)\n            completed += 1\n            \n            status_icon = \"‚úì\" if result['status'] == 'SUCCESS' else \"‚úó\"\n            deletes = result.get('rows_deleted', 0) or 0\n            delete_info = f\" ({deletes} deleted)\" if deletes > 0 else \"\"\n            logger.info(f\"    [{completed}/{len(tables_for_silver)}] {status_icon} {result['table_name']:<30} {result['status']:<10}{delete_info}\")\n            \n    \n    silver_end = datetime.now(timezone.utc)\n    silver_duration = int((silver_end - silver_start).total_seconds())\n    \n    logger.info(f\"\\n‚úì Silver processing completed in {silver_duration}s\")\n    #sys.exit(0)"
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "## [9] Silver Logging and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": "if silver_results:\n    logger.info(f\"\\nüìä Logging Silver results...\")\n    \n    # Batch log\n    log_batch(spark, records=silver_results, layer=\"silver\")\n    \n    # Calculate summary\n    success_count = sum(1 for r in silver_results if r['status'] == 'SUCCESS')\n    failed_count = sum(1 for r in silver_results if r['status'] == 'FAILED')\n    skipped_count = sum(1 for r in silver_results if r['status'] == 'SKIPPED')\n    \n    total_inserts = sum(r.get('rows_inserted', 0) or 0 for r in silver_results)\n    total_updates = sum(r.get('rows_updated', 0) or 0 for r in silver_results)\n    total_deletes = sum(r.get('rows_deleted', 0) or 0 for r in silver_results)\n    total_unchanged = sum(r.get('rows_unchanged', 0) or 0 for r in silver_results)\n    \n    failed_tables = [r['table_name'] for r in silver_results if r['status'] == 'FAILED']\n    \n    # Log summary\n    silver_summary = {\n        \"run_id\": RUN_ID,\n        \"source\": source,\n        \"run_ts\": run_ts,\n        \"run_start\": silver_start,\n        \"run_end\": silver_end,\n        \"duration_seconds\": silver_duration,\n        \"total_tables\": len(silver_results),\n        \"tables_success\": success_count,\n        \"tables_failed\": failed_count,\n        \"tables_skipped\": skipped_count,\n        \"total_inserts\": total_inserts,\n        \"total_updates\": total_updates,\n        \"total_deletes\": total_deletes,\n        \"total_unchanged\": total_unchanged,\n        \"failed_tables\": failed_tables,\n    }\n    \n    log_summary(spark, summary=silver_summary, layer=\"silver\")\n    \n    # Print summary\n    logger.info(f\"\\n  Summary:\")\n    logger.info(f\"    Success: {success_count}\")\n    logger.info(f\"    Failed:  {failed_count}\")\n    logger.info(f\"    Skipped: {skipped_count}\")\n    if total_inserts or total_updates or total_deletes:\n        logger.info(f\"    CDC: +{total_inserts or 0} ~{total_updates or 0} -{total_deletes}\")\n    \n    if failed_tables:\n        logger.info(f\"\\n  ‚ö†Ô∏è  Failed tables: {failed_tables}\")\nelse:\n    logger.info(f\"\\n  ‚ÑπÔ∏è  No Silver results to log\")"
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "## [10] Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_end = datetime.now(timezone.utc)\n",
    "total_duration = int((total_end - bronze_start).total_seconds())\n",
    "\n",
    "logger.info(\"\\n\" + \"=\"*80)\n",
    "logger.info(\"ORCHESTRATOR SUMMARY\")\n",
    "logger.info(\"=\"*80)\n",
    "logger.info(f\"Run ID: {RUN_ID}\")\n",
    "logger.info(f\"Source: {source}\")\n",
    "logger.info(f\"Run TS: {run_ts}\")\n",
    "logger.info(f\"\\nTiming:\")\n",
    "logger.info(f\"  Bronze: {bronze_duration}s\")\n",
    "if silver_results:\n",
    "    logger.info(f\"  Silver: {silver_duration}s\")\n",
    "logger.info(f\"  Total:  {total_duration}s\")\n",
    "\n",
    "logger.info(f\"\\nBronze Results:\")\n",
    "if bronze_results:\n",
    "    bronze_success = sum(1 for r in bronze_results if r['status'] == 'SUCCESS')\n",
    "    bronze_failed = sum(1 for r in bronze_results if r['status'] == 'FAILED')\n",
    "    logger.info(f\"  ‚úì Success: {bronze_success}/{len(bronze_results)}\")\n",
    "    if bronze_failed > 0:\n",
    "        logger.info(f\"  ‚úó Failed:  {bronze_failed}\")\n",
    "else:\n",
    "    logger.info(f\"  (No processing)\")\n",
    "\n",
    "logger.info(f\"\\nSilver Results:\")\n",
    "if silver_results:\n",
    "    silver_success = sum(1 for r in silver_results if r['status'] == 'SUCCESS')\n",
    "    silver_failed = sum(1 for r in silver_results if r['status'] == 'FAILED')\n",
    "    logger.info(f\"  ‚úì Success: {silver_success}/{len(silver_results)}\")\n",
    "    if silver_failed > 0:\n",
    "        logger.info(f\"  ‚úó Failed:  {silver_failed}\")\n",
    "else:\n",
    "    logger.info(f\"  (No processing)\")\n",
    "\n",
    "# Overall status\n",
    "if bronze_results:\n",
    "    all_bronze_ok = all(r['status'] in ('SUCCESS', 'EMPTY', 'SKIPPED') for r in bronze_results)\n",
    "else:\n",
    "    all_bronze_ok = True\n",
    "\n",
    "if silver_results:\n",
    "    all_silver_ok = all(r['status'] in ('SUCCESS', 'SKIPPED') for r in silver_results)\n",
    "else:\n",
    "    all_silver_ok = True\n",
    "\n",
    "overall_status = \"SUCCESS\" if (all_bronze_ok and all_silver_ok) else \"PARTIAL\" if bronze_results or silver_results else \"NO_WORK\"\n",
    "\n",
    "logger.info(f\"\\nOverall Status: {overall_status}\")\n",
    "logger.info(\"=\"*80)\n",
    "\n",
    "if overall_status != \"SUCCESS\":\n",
    "    logger.info(f\"\\n‚ö†Ô∏è  Some tables failed. Check logs for details.\")\n",
    "    logger.info(f\"   Use retry_tables parameter to retry specific tables.\")\n",
    "else:\n",
    "    logger.info(f\"\\n‚úì All processing completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dwh-spark-processing (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}