{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# 10 — Bronze Load Worker\n",
    "\n",
    "This notebook defines the worker function that loads a **single table** from parquet into a Bronze Delta table.\n",
    "\n",
    "## Architecture: Bronze History Pattern\n",
    "\n",
    "Bronze uses **APPEND with run_ts partitioning** to enable full CDC capability:\n",
    "- **Snapshot/Window tables**: Overwrite entire table\n",
    "- **Incremental tables**: Append with `_bronze_load_ts` partition\n",
    "- Silver can reconstruct current state and detect deletes\n",
    "\n",
    "## Key Features\n",
    "- Does **not** write to log table (returns metrics dict)\n",
    "- Reads parquet from auto-detected path (Fabric or Local)\n",
    "- Adds metadata columns: `_bronze_load_ts`, `_bronze_filename`\n",
    "- Handles missing files, empty data, corrupt Delta tables\n",
    "- Returns comprehensive metrics for logging\n",
    "\n",
    "## Load Modes\n",
    "- **snapshot**: Complete table refresh (overwrite)\n",
    "- **incremental**: Delta append with run_ts (CDC history)\n",
    "- **window**: Overwrite with partitioning support\n",
    "\n",
    "This notebook is imported via `%run` from the master orchestrator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters (set by orchestrator, not by Papermill in this case)\n",
    "# These are here for documentation and can be overridden when %run is called\n",
    "RUN_ID = None  # Will be set by orchestrator\n",
    "#DEBUG = False  # Enable debug output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## [1] Imports and Path Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import (\n",
    "    lit, input_file_name, col, year, month\n",
    ")\n",
    "from datetime import datetime, timezone\n",
    "from typing import Dict, Any\n",
    "from uuid import uuid4\n",
    "import os\n",
    "from modules.path_utils import build_parquet_dir\n",
    "from modules.path_utils import get_base_path\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "logger.info(\"✓ Imports loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-detect base path (Fabric vs Cluster)\n",
    "# Base path voor Files/Tables gebruiken we centraal vanuit modules.path_utils\n",
    "\n",
    "try:\n",
    "    # Als 02_utils_config al gedraaid heeft, staat BASE_PATH al in de globals\n",
    "    BASE_PATH  # type: ignore[name-defined]\n",
    "    logger.info(f\"✓ Base path (bronze worker): {BASE_PATH}\")\n",
    "    env_type = (\n",
    "        \"Fabric\" if \"/lakehouse/default/Files\" in BASE_PATH\n",
    "        else \"Custom Cluster\" if \"/data/lakehouse/\" in BASE_PATH\n",
    "        else \"Cluster/Relative\"\n",
    "    )\n",
    "    logger.info(f\"✓ Environment (bronze worker): {env_type}\")\n",
    "except NameError:\n",
    "    # Fallback zodat je 10_bronze_load ook stand-alone kunt draaien\n",
    "    BASE_PATH = get_base_path()\n",
    "    env_type = (\n",
    "        \"Fabric\" if \"/lakehouse/default/Files\" in BASE_PATH\n",
    "        else \"Custom Cluster\" if \"/data/lakehouse/\" in BASE_PATH\n",
    "        else \"Local/Relative\"\n",
    "    )\n",
    "    logger.info(f\"✓ Base path (bronze worker – fallback): {BASE_PATH}\")\n",
    "    logger.info(f\"✓ Environment (bronze worker – fallback): {env_type}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## [2] Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": "# Import Bronze processing function from module\nfrom modules.bronze_processor import process_bronze_table\n\nlogger.info(\"✓ Bronze processor function imported from modules.bronze_processor\")\nlogger.info(\"✓ Helper functions (error detection, Delta utils) included in module\")"
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## [3] Core Worker Function\n",
    "\n",
    "This is the main function that processes a single table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": "# Bronze processor function is now imported from modules.bronze_processor\n# \n# The process_bronze_table() function handles:\n# - Reading parquet files\n# - Adding metadata columns (_bronze_load_ts, _bronze_filename)\n# - Writing to Delta tables (snapshot/incremental/window modes)\n# - Error recovery for corrupt Delta tables\n# - Partitioning support\n#\n# Usage in orchestrator:\n# result = process_bronze_table(\n#     spark=spark,\n#     table_def=table,\n#     source_name=source,\n#     run_id=RUN_ID,\n#     run_ts=run_ts,\n#     run_date=run_date,\n#     base_files=base_files,\n#     debug=True\n# )\n\nlogger.info(\"✓ Bronze worker function ready (imported from module)\")"
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## [4] Function Ready\n",
    "\n",
    "The `process_bronze_table()` function is now available for use by the orchestrator.\n",
    "\n",
    "**Usage pattern:**\n",
    "\n",
    "```python\n",
    "# In orchestrator notebook:\n",
    "%run \"10_bronze_load\"\n",
    "\n",
    "# Set RUN_ID\n",
    "RUN_ID = f\"run_{run_ts}\"\n",
    "\n",
    "# Process tables\n",
    "results = []\n",
    "for table in tables:\n",
    "    result = process_bronze_table(\n",
    "        table_def=table,\n",
    "        source_name=source,\n",
    "        run_ts=run_ts,\n",
    "        base_files=base_files,\n",
    "        debug=True\n",
    "    )\n",
    "    results.append(result)\n",
    "\n",
    "# Log results in batch\n",
    "bronze_summary = {...}  # build from results when ready\n",
    "run_log_id = log_summary(bronze_summary, layer=\"bronze\")\n",
    "log_batch(results, layer=\"bronze\", run_log_id=run_log_id)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"\\n\" + \"=\" * 80)\n",
    "logger.info(\"BRONZE WORKER READY\")\n",
    "logger.info(\"=\" * 80)\n",
    "logger.info(f\"Base path: {BASE_PATH}\")\n",
    "logger.info(f\"Environment: {'Fabric' if '/lakehouse' in BASE_PATH else 'Local'}\")\n",
    "logger.info(\"\\nFunction available: process_bronze_table(table_def, source_name, run_ts, ...)\")\n",
    "logger.info(\"\\n⚠️  Remember to set RUN_ID before calling process_bronze_table()\")\n",
    "logger.info(\"✓ Bronze worker notebook loaded successfully\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}