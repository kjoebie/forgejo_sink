{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# 10 — Bronze Load Worker\n",
    "\n",
    "This notebook defines the worker function that loads a **single table** from parquet into a Bronze Delta table.\n",
    "\n",
    "## Architecture: Bronze History Pattern\n",
    "\n",
    "Bronze uses **APPEND with run_ts partitioning** to enable full CDC capability:\n",
    "- **Snapshot/Window tables**: Overwrite entire table\n",
    "- **Incremental tables**: Append with `_bronze_load_ts` partition\n",
    "- Silver can reconstruct current state and detect deletes\n",
    "\n",
    "## Key Features\n",
    "- Does **not** write to log table (returns metrics dict)\n",
    "- Reads parquet from auto-detected path (Fabric or Local)\n",
    "- Adds metadata columns: `_bronze_load_ts`, `_bronze_filename`\n",
    "- Handles missing files, empty data, corrupt Delta tables\n",
    "- Returns comprehensive metrics for logging\n",
    "\n",
    "## Load Modes\n",
    "- **snapshot**: Complete table refresh (overwrite)\n",
    "- **incremental**: Delta append with run_ts (CDC history)\n",
    "- **window**: Overwrite with partitioning support\n",
    "\n",
    "This notebook is imported via `%run` from the master orchestrator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters (set by orchestrator, not by Papermill in this case)\n",
    "# These are here for documentation and can be overridden when %run is called\n",
    "RUN_ID = None  # Will be set by orchestrator\n",
    "#DEBUG = False  # Enable debug output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## [1] Imports and Path Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import (\n",
    "    lit, current_timestamp, input_file_name, col, year, month\n",
    ")\n",
    "from datetime import datetime\n",
    "from typing import Dict, Any\n",
    "from uuid import uuid4\n",
    "import os\n",
    "from modules.path_utils import build_parquet_dir\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "print(\"✓ Imports loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-detect base path (Fabric vs Cluster)\n",
    "# Base path voor Files/Tables gebruiken we centraal vanuit 02_utils_config\n",
    "\n",
    "try:\n",
    "    # Als 02_utils_config al gedraaid heeft, staat BASE_PATH al in de globals\n",
    "    BASE_PATH  # type: ignore[name-defined]\n",
    "    print(f\"✓ Base path (bronze worker): {BASE_PATH}\")\n",
    "    env_type = (\n",
    "        \"Fabric\" if \"/lakehouse/default/Files\" in BASE_PATH\n",
    "        else \"Custom Cluster\" if \"/data/lakehouse/\" in BASE_PATH\n",
    "        else \"Cluster/Relative\"\n",
    "    )\n",
    "    print(f\"✓ Environment (bronze worker): {env_type}\")\n",
    "except NameError:\n",
    "    # Fallback zodat je 10_bronze_load ook stand-alone kunt draaien\n",
    "    import glob\n",
    "    import os\n",
    "\n",
    "    def detect_base_path() -> str:\n",
    "        \"\"\"\n",
    "        Zelfde logica als in 02_utils_config:\n",
    "        1. Fabric: /lakehouse/default/Files\n",
    "        2. Custom cluster: /data/lakehouse/**/Files\n",
    "        3. Anders: 'Files' (relatief, bijvoorbeeld in je repo)\n",
    "        \"\"\"\n",
    "        # 1. Fabric\n",
    "        fabric_path = \"/lakehouse/default/Files\"\n",
    "        if os.path.exists(fabric_path):\n",
    "            return fabric_path\n",
    "\n",
    "        # 2. Custom cluster\n",
    "        if os.path.exists(\"/data/lakehouse\"):\n",
    "            pattern = \"/data/lakehouse/**/Files\"\n",
    "            matches = glob.glob(pattern, recursive=True)\n",
    "            if matches:\n",
    "                return sorted(matches)[0]\n",
    "\n",
    "        # 3. Local / relative\n",
    "        return \"Files\"\n",
    "\n",
    "    BASE_PATH = detect_base_path()\n",
    "    env_type = (\n",
    "        \"Fabric\" if \"/lakehouse/default/Files\" in BASE_PATH\n",
    "        else \"Custom Cluster\" if \"/data/lakehouse/\" in BASE_PATH\n",
    "        else \"Local/Relative\"\n",
    "    )\n",
    "    print(f\"✓ Base path (bronze worker – fallback): {BASE_PATH}\")\n",
    "    print(f\"✓ Environment (bronze worker – fallback): {env_type}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## [2] Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_missing_path_error(exc: Exception) -> bool:\n",
    "    \"\"\"\n",
    "    Heuristic to detect 'no parquet files found' situations.\n",
    "    \"\"\"\n",
    "    msg = str(exc).lower()\n",
    "    return (\n",
    "        \"path does not exist\" in msg\n",
    "        or \"no such file or directory\" in msg\n",
    "        or \"file not found\" in msg\n",
    "        or \"cannot find path\" in msg\n",
    "    )\n",
    "\n",
    "\n",
    "def is_probably_corrupt_delta(exc: Exception) -> bool:\n",
    "    \"\"\"\n",
    "    Heuristic to detect a broken Delta table that may need to be recreated.\n",
    "    \"\"\"\n",
    "    msg = str(exc).lower()\n",
    "    return (\n",
    "        \"is not a delta table\" in msg\n",
    "        or \"failed to merge fields\" in msg\n",
    "        or \"incompatible format\" in msg\n",
    "        or \"protocol\" in msg and \"unsupported\" in msg\n",
    "        or (\"delta log\" in msg and \"error\" in msg)\n",
    "    )\n",
    "\n",
    "\n",
    "def get_last_num_output_rows(table_fullname: str) -> int | None:\n",
    "    \"\"\"\n",
    "    Get the number of written rows from the last Delta write\n",
    "    for this table using DESCRIBE HISTORY.\n",
    "    \"\"\"\n",
    "    history = spark.sql(f\"DESCRIBE HISTORY {table_fullname}\")\n",
    "    row = (\n",
    "        history\n",
    "        .orderBy(F.col(\"version\").desc())\n",
    "        .select(F.col(\"operationMetrics\")[\"numOutputRows\"].alias(\"rows\"))\n",
    "        .first()\n",
    "    )\n",
    "    return int(row[\"rows\"]) if row and row[\"rows\"] is not None else None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"✓ Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## [3] Core Worker Function\n",
    "\n",
    "This is the main function that processes a single table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_bronze_table(\n",
    "    table_def: Dict[str, Any],\n",
    "    source_name: str,\n",
    "    run_ts: str,\n",
    "    base_files: str, #= \"greenhouse_sources\",\n",
    "    debug: bool = False\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Load a single table's parquet files for a given run_ts into Bronze Delta table.\n",
    "    \n",
    "    Architecture:\n",
    "    - Snapshot/Window: Overwrite entire table\n",
    "    - Incremental: Append with _bronze_load_ts partition (for CDC)\n",
    "    \n",
    "    Args:\n",
    "        table_def: Table definition from DAG\n",
    "        source_name: Source system name (e.g., \"vizier\")\n",
    "        run_ts: Run timestamp (e.g., \"20251105T142752505\")\n",
    "        base_files: Base directory for files (default: \"greenhouse_sources\")\n",
    "        debug: Enable debug output\n",
    "    \n",
    "    Returns:\n",
    "        Dict with processing results:\n",
    "        - log_id, run_id, run_ts, source, table_name, load_mode\n",
    "        - status (SUCCESS, FAILED, SKIPPED, EMPTY)\n",
    "        - rows_processed\n",
    "        - start_time, end_time, duration_seconds\n",
    "        - error_message, parquet_path, delta_table\n",
    "    \"\"\"\n",
    "    \n",
    "    # Validate table_def\n",
    "    table_name = table_def.get(\"name\")\n",
    "    if not table_name:\n",
    "        raise ValueError(\"table_def is missing 'name'\")\n",
    "    \n",
    "    # Validate RUN_ID is set\n",
    "    if RUN_ID is None:\n",
    "        raise ValueError(\"RUN_ID must be set before calling process_bronze_table\")\n",
    "    \n",
    "    # Get load mode (default to snapshot)\n",
    "    load_mode = (table_def.get(\"load_mode\") or \"snapshot\").lower()\n",
    "    supported_modes = {\"snapshot\", \"window\", \"incremental\"}\n",
    "    \n",
    "    # Initialize metrics\n",
    "    log_id = f\"{source_name}:{table_name}:{run_ts}:{uuid4().hex[:8]}\"\n",
    "    start_time = datetime.utcnow()\n",
    "    end_time = None\n",
    "    status = \"RUNNING\"\n",
    "    error_message = None\n",
    "    rows_processed = None\n",
    "    \n",
    "    # Early exit for unsupported load modes\n",
    "    if load_mode not in supported_modes:\n",
    "        end_time = datetime.utcnow()\n",
    "        duration = int((end_time - start_time).total_seconds())\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"[{table_name}] SKIPPED: unsupported load_mode '{load_mode}'\")\n",
    "        \n",
    "        return {\n",
    "            \"log_id\": log_id,\n",
    "            \"run_id\": RUN_ID,\n",
    "            \"run_ts\": run_ts,\n",
    "            \"source\": source_name,\n",
    "            \"table_name\": table_name,\n",
    "            \"load_mode\": load_mode,\n",
    "            \"status\": \"SKIPPED\",\n",
    "            \"rows_processed\": None,\n",
    "            \"start_time\": start_time,\n",
    "            \"end_time\": end_time,\n",
    "            \"duration_seconds\": duration,\n",
    "            \"error_message\": f\"Unsupported load_mode '{load_mode}'\",\n",
    "            \"parquet_path\": None,\n",
    "            \"delta_table\": None,\n",
    "        }\n",
    "    \n",
    "    # Build target table name\n",
    "    target_table = table_def.get(\"delta_table\") or table_name\n",
    "    delta_schema = table_def.get(\"delta_schema\") or \"bronze\"\n",
    "    \n",
    "    # Handle schema.table format\n",
    "    if \".\" not in target_table:\n",
    "        delta_table_full = f\"{delta_schema}.{target_table}\"\n",
    "    else:\n",
    "        delta_table_full = target_table\n",
    "    \n",
    "    # Build parquet path\n",
    "    parquet_dir = build_parquet_dir(base_files, source_name, run_ts, table_name, spark)\n",
    "    parquet_glob = f\"{parquet_dir}/*.parquet\"\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"[{table_name}] Starting ({load_mode})\")\n",
    "        print(f\"  Parquet: {parquet_dir}\")\n",
    "        print(f\"  Target: {delta_table_full}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STEP 1: Read Parquet\n",
    "    # ========================================================================\n",
    "    \n",
    "    try:\n",
    "        df = spark.read.parquet(parquet_glob)\n",
    "                \n",
    "    except Exception as e:\n",
    "        if is_missing_path_error(e):\n",
    "            # No parquet files - table not exported in this run\n",
    "            end_time = datetime.utcnow()\n",
    "            duration = int((end_time - start_time).total_seconds())\n",
    "            \n",
    "            if debug:\n",
    "                print(f\"[{table_name}] SKIPPED: No parquet files in {parquet_dir}\")\n",
    "            \n",
    "            return {\n",
    "                \"log_id\": log_id,\n",
    "                \"run_id\": RUN_ID,\n",
    "                \"run_ts\": run_ts,\n",
    "                \"source\": source_name,\n",
    "                \"table_name\": table_name,\n",
    "                \"load_mode\": load_mode,\n",
    "                \"status\": \"SKIPPED\",\n",
    "                \"rows_processed\": 0,\n",
    "                \"start_time\": start_time,\n",
    "                \"end_time\": end_time,\n",
    "                \"duration_seconds\": duration,\n",
    "                \"error_message\": f\"No parquet files found in {parquet_dir}\",\n",
    "                \"parquet_path\": parquet_dir,\n",
    "                \"delta_table\": delta_table_full,\n",
    "            }\n",
    "        else:\n",
    "            # Other read error\n",
    "            end_time = datetime.utcnow()\n",
    "            duration = int((end_time - start_time).total_seconds())\n",
    "            \n",
    "            if debug:\n",
    "                print(f\"[{table_name}] FAILED reading parquet: {str(e)[:200]}\")\n",
    "            \n",
    "            return {\n",
    "                \"log_id\": log_id,\n",
    "                \"run_id\": RUN_ID,\n",
    "                \"run_ts\": run_ts,\n",
    "                \"source\": source_name,\n",
    "                \"table_name\": table_name,\n",
    "                \"load_mode\": load_mode,\n",
    "                \"status\": \"FAILED\",\n",
    "                \"rows_processed\": None,\n",
    "                \"start_time\": start_time,\n",
    "                \"end_time\": end_time,\n",
    "                \"duration_seconds\": duration,\n",
    "                \"error_message\": f\"Read parquet failed: {str(e)[:500]}\",\n",
    "                \"parquet_path\": parquet_dir,\n",
    "                \"delta_table\": delta_table_full,\n",
    "            }\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STEP 2: Add Metadata Columns\n",
    "    # ========================================================================\n",
    "    \n",
    "    # Add Bronze metadata columns\n",
    "    df_with_meta = df \\\n",
    "        .withColumn(\"_bronze_load_ts\", lit(run_ts)) \\\n",
    "        .withColumn(\"_bronze_filename\", input_file_name())\n",
    "    \n",
    "    # For window tables with partitioning config, add partition columns\n",
    "    partitioning_config = table_def.get(\"partitioning\")\n",
    "    if partitioning_config and load_mode in (\"window\"):\n",
    "        partition_type = partitioning_config.get(\"type\")\n",
    "        \n",
    "        if partition_type == \"year_month\":\n",
    "            year_col = partitioning_config.get(\"year_col\", \"p_year\")\n",
    "            month_col = partitioning_config.get(\"month_col\", \"p_month\")\n",
    "            \n",
    "            # Get window column to extract year/month from\n",
    "            window_config = table_def.get(\"window\", {})\n",
    "            window_col = window_config.get(\"column\", \"Boek_Datum\")  # Default\n",
    "            \n",
    "            if window_col in df.columns:\n",
    "                df_with_meta = df_with_meta \\\n",
    "                    .withColumn(year_col, year(col(window_col))) \\\n",
    "                    .withColumn(month_col, month(col(window_col)))\n",
    "                \n",
    "                if debug:\n",
    "                    print(f\"  Added partitioning: {year_col}, {month_col} from {window_col}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STEP 3: Write to Delta\n",
    "    # ========================================================================\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        # Determine write mode based on load_mode\n",
    "        if load_mode == \"incremental\":\n",
    "            # APPEND with partition by _bronze_load_ts (CDC history)\n",
    "            writer = df_with_meta.write \\\n",
    "                .format(\"delta\") \\\n",
    "                .mode(\"append\") \\\n",
    "                .partitionBy(\"_bronze_load_ts\")\n",
    "            \n",
    "            if debug:\n",
    "                print(f\"  Mode: APPEND with partition by _bronze_load_ts (CDC history)\")\n",
    "        \n",
    "        elif load_mode in (\"snapshot\", \"window\"):\n",
    "            # OVERWRITE entire table\n",
    "            # For window tables with partitioning, could use dynamic partition overwrite\n",
    "            # but for simplicity, we overwrite entire table\n",
    "            \n",
    "            if partitioning_config:\n",
    "                # Partitioned overwrite\n",
    "                partition_type = partitioning_config.get(\"type\")\n",
    "                if partition_type == \"year_month\":\n",
    "                    year_col = partitioning_config.get(\"year_col\", \"p_year\")\n",
    "                    month_col = partitioning_config.get(\"month_col\", \"p_month\")\n",
    "                    \n",
    "                    writer = df_with_meta.write \\\n",
    "                        .format(\"delta\") \\\n",
    "                        .mode(\"overwrite\") \\\n",
    "                        .option(\"overwriteSchema\", \"true\") \\\n",
    "                        .partitionBy(year_col, month_col)\n",
    "                    \n",
    "                    if debug:\n",
    "                        print(f\"  Mode: OVERWRITE with partitioning by {year_col}, {month_col}\")\n",
    "                else:\n",
    "                    # Unknown partition type, just overwrite\n",
    "                    writer = df_with_meta.write \\\n",
    "                        .format(\"delta\") \\\n",
    "                        .mode(\"overwrite\") \\\n",
    "                        .option(\"overwriteSchema\", \"true\")\n",
    "            else:\n",
    "                # No partitioning, simple overwrite\n",
    "                writer = df_with_meta.write \\\n",
    "                    .format(\"delta\") \\\n",
    "                    .mode(\"overwrite\") \\\n",
    "                    .option(\"overwriteSchema\", \"true\")\n",
    "                \n",
    "                if debug:\n",
    "                    print(f\"  Mode: OVERWRITE\")\n",
    "        \n",
    "        # Execute write\n",
    "        writer.saveAsTable(delta_table_full)\n",
    "        rows_processed = get_last_num_output_rows(delta_table_full)\n",
    "        \n",
    "        # Check for empty result\n",
    "        if rows_processed == 0:\n",
    "            end_time = datetime.utcnow()\n",
    "            duration = int((end_time - start_time).total_seconds())\n",
    "            \n",
    "            if debug:\n",
    "                print(f\"[{table_name}] EMPTY: Parquet exists but contains 0 rows\")\n",
    "            \n",
    "            return {\n",
    "                \"log_id\": log_id,\n",
    "                \"run_id\": RUN_ID,\n",
    "                \"run_ts\": run_ts,\n",
    "                \"source\": source_name,\n",
    "                \"table_name\": table_name,\n",
    "                \"load_mode\": load_mode,\n",
    "                \"status\": \"EMPTY\",\n",
    "                \"rows_processed\": 0,\n",
    "                \"start_time\": start_time,\n",
    "                \"end_time\": end_time,\n",
    "                \"duration_seconds\": duration,\n",
    "                \"error_message\": \"Parquet exists but contains 0 rows\",\n",
    "                \"parquet_path\": parquet_dir,\n",
    "                \"delta_table\": delta_table_full,\n",
    "            }\n",
    "        \n",
    "        # Success!\n",
    "        status = \"SUCCESS\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Try recovery if Delta table looks corrupt\n",
    "        if is_probably_corrupt_delta(e):\n",
    "            if debug:\n",
    "                print(f\"[{table_name}] Write failed, attempting DROP+RECREATE: {str(e)[:200]}\")\n",
    "            \n",
    "            try:\n",
    "                # Drop and recreate\n",
    "                spark.sql(f\"DROP TABLE IF EXISTS {delta_table_full}\")\n",
    "                \n",
    "                # Recreate with appropriate partitioning\n",
    "                if load_mode == \"incremental\":\n",
    "                    writer = df_with_meta.write \\\n",
    "                        .format(\"delta\") \\\n",
    "                        .mode(\"overwrite\") \\\n",
    "                        .option(\"overwriteSchema\", \"true\") \\\n",
    "                        .partitionBy(\"_bronze_load_ts\")\n",
    "                elif partitioning_config and partitioning_config.get(\"type\") == \"year_month\":\n",
    "                    year_col = partitioning_config.get(\"year_col\", \"p_year\")\n",
    "                    month_col = partitioning_config.get(\"month_col\", \"p_month\")\n",
    "                    writer = df_with_meta.write \\\n",
    "                        .format(\"delta\") \\\n",
    "                        .mode(\"overwrite\") \\\n",
    "                        .option(\"overwriteSchema\", \"true\") \\\n",
    "                        .partitionBy(year_col, month_col)\n",
    "                else:\n",
    "                    writer = df_with_meta.write \\\n",
    "                        .format(\"delta\") \\\n",
    "                        .mode(\"overwrite\") \\\n",
    "                        .option(\"overwriteSchema\", \"true\")\n",
    "                \n",
    "                writer.saveAsTable(delta_table_full)\n",
    "                rows_processed = get_last_num_output_rows(delta_table_full)\n",
    "\n",
    "                # Count rows\n",
    "                if load_mode == \"incremental\":\n",
    "                    rows_processed = spark.table(delta_table_full) \\\n",
    "                        .where(f\"_bronze_load_ts = '{run_ts}'\") \\\n",
    "                        .count()\n",
    "                else:\n",
    "                    rows_processed = spark.table(delta_table_full).count()\n",
    "                \n",
    "                status = \"SUCCESS\"\n",
    "                error_message = f\"Initial write failed but table was recreated. Original error: {str(e)[:300]}\"\n",
    "                \n",
    "                if debug:\n",
    "                    print(f\"[{table_name}] Recovery successful\")\n",
    "                \n",
    "            except Exception as e2:\n",
    "                status = \"FAILED\"\n",
    "                error_message = f\"Write failed and recovery failed: {str(e2)[:500]}\"\n",
    "                \n",
    "                if debug:\n",
    "                    print(f\"[{table_name}] Recovery FAILED: {str(e2)[:200]}\")\n",
    "        else:\n",
    "            status = \"FAILED\"\n",
    "            error_message = f\"Write failed: {str(e)[:500]}\"\n",
    "            \n",
    "            if debug:\n",
    "                print(f\"[{table_name}] FAILED: {str(e)[:200]}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STEP 4: Return Results\n",
    "    # ========================================================================\n",
    "    \n",
    "    end_time = datetime.utcnow()\n",
    "    duration = int((end_time - start_time).total_seconds())\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"[{table_name}] {status} in {duration}s ({rows_processed:,} rows)\")\n",
    "    \n",
    "    return {\n",
    "        \"log_id\": log_id,\n",
    "        \"run_id\": RUN_ID,\n",
    "        \"run_ts\": run_ts,\n",
    "        \"source\": source_name,\n",
    "        \"table_name\": table_name,\n",
    "        \"load_mode\": load_mode,\n",
    "        \"status\": status,\n",
    "        \"rows_processed\": rows_processed,\n",
    "        \"start_time\": start_time,\n",
    "        \"end_time\": end_time,\n",
    "        \"duration_seconds\": duration,\n",
    "        \"error_message\": error_message,\n",
    "        \"parquet_path\": parquet_dir,\n",
    "        \"delta_table\": delta_table_full,\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"✓ Bronze worker function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## [4] Function Ready\n",
    "\n",
    "The `process_bronze_table()` function is now available for use by the orchestrator.\n",
    "\n",
    "**Usage pattern:**\n",
    "\n",
    "```python\n",
    "# In orchestrator notebook:\n",
    "%run \"10_bronze_load\"\n",
    "\n",
    "# Set RUN_ID\n",
    "RUN_ID = f\"run_{run_ts}\"\n",
    "\n",
    "# Process tables\n",
    "results = []\n",
    "for table in tables:\n",
    "    result = process_bronze_table(\n",
    "        table_def=table,\n",
    "        source_name=source,\n",
    "        run_ts=run_ts,\n",
    "        base_files=base_files,\n",
    "        debug=True\n",
    "    )\n",
    "    results.append(result)\n",
    "\n",
    "# Log results in batch\n",
    "log_bronze_batch(results)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"BRONZE WORKER READY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Base path: {BASE_PATH}\")\n",
    "print(f\"Environment: {'Fabric' if '/lakehouse' in BASE_PATH else 'Local'}\")\n",
    "print(\"\\nFunction available: process_bronze_table(table_def, source_name, run_ts, ...)\")\n",
    "print(\"\\n⚠️  Remember to set RUN_ID before calling process_bronze_table()\")\n",
    "print(\"✓ Bronze worker notebook loaded successfully\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
