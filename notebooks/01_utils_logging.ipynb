{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# 01 \u2014 Logging Utilities for Bronze and Silver Processing\n",
    "\n",
    "This notebook defines the logging infrastructure for the data pipeline:\n",
    "\n",
    "## Bronze Logging\n",
    "- `logs.bronze_processing_log` - Per-table processing results\n",
    "- `logs.bronze_run_summary` - Aggregated run statistics\n",
    "\n",
    "## Silver Logging\n",
    "- `logs.silver_processing_log` - Per-table CDC merge results\n",
    "- `logs.silver_run_summary` - Aggregated CDC statistics\n",
    "\n",
    "## Key Features\n",
    "- Batch logging (one write per run, not per table)\n",
    "- Append-only (no MERGE, maximum concurrency)\n",
    "- Partitioned by `run_date` and `table_name`\n",
    "- Helper functions for log retrieval and analysis\n",
    "\n",
    "**Architecture:** Bronze uses append with run_ts history for full CDC capability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters (Papermill compatible)\n",
    "# These can be overridden when running via notebook orchestration\n",
    "LOG_SCHEMA = \"logs\"  # Database for all log tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## [1] Setup: Imports and Schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, LongType, IntegerType,\n",
    "    TimestampType, DateType, DoubleType\n",
    ")\n",
    "from uuid import uuid4\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import functions as F\n",
    "from datetime import date\n",
    "from typing import List, Dict, Any, Optional\n",
    "from pyspark.sql import DataFrame \n",
    "\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## [2] Bronze Processing Log Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bronze processing log - tracks individual table loads\n",
    "BRONZE_LOG_TABLE = \"bronze_processing_log\"\n",
    "BRONZE_LOG_TABLE_FULLNAME = f\"{LOG_SCHEMA}.{BRONZE_LOG_TABLE}\"\n",
    "\n",
    "bronze_processing_log_schema = StructType([\n",
    "    # Identifiers\n",
    "    StructField(\"log_id\",           StringType(),    False),  # Unique log entry ID\n",
    "    StructField(\"run_log_id\",       StringType(),    False),  # FK to bronze_run_summary.log_id\n",
    "    StructField(\"run_id\",           StringType(),    False),  # Run identifier\n",
    "    StructField(\"run_date\",         DateType(),      False),  # Partition key (derived from run_ts)\n",
    "    StructField(\"run_ts\",           StringType(),    False),  # Run timestamp (yyyyMMddTHHmmssSSS)\n",
    "    \n",
    "    # Source information\n",
    "    StructField(\"source\",           StringType(),    False),  # Source system name\n",
    "    StructField(\"table_name\",       StringType(),    False),  # Table name (partition key)\n",
    "    StructField(\"partition_key\",    StringType(),    True),   # new: optional partition key\n",
    "    StructField(\"load_mode\",        StringType(),    True),   # snapshot, incremental, window\n",
    "    \n",
    "    # Processing results\n",
    "    StructField(\"status\",           StringType(),    False),  # SUCCESS, FAILED, SKIPPED, EMPTY\n",
    "    StructField(\"rows_processed\",   LongType(),      True),   # Rows processed from parquet to Bronze (Delta)\n",
    "\n",
    "    # Timing\n",
    "    StructField(\"start_time\",       TimestampType(), True),\n",
    "    StructField(\"end_time\",         TimestampType(), True),\n",
    "    StructField(\"duration_seconds\", LongType(),      True),\n",
    "    \n",
    "    # Error handling\n",
    "    StructField(\"error_message\",    StringType(),    True),   # Truncated to 1000 chars\n",
    "    \n",
    "    # Source/target paths\n",
    "    StructField(\"parquet_path\",     StringType(),    True),   # Source parquet folder\n",
    "    StructField(\"delta_table\",      StringType(),    True),   # Target Bronze table\n",
    "])\n",
    "\n",
    "logger.info(f\"Bronze log schema defined: {BRONZE_LOG_TABLE_FULLNAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## [3] Bronze Run Summary Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bronze run summary - aggregated statistics per run\n",
    "BRONZE_SUMMARY_TABLE = \"bronze_run_summary\"\n",
    "BRONZE_SUMMARY_TABLE_FULLNAME = f\"{LOG_SCHEMA}.{BRONZE_SUMMARY_TABLE}\"\n",
    "\n",
    "bronze_run_summary_schema = StructType([\n",
    "        # Run identifiers\n",
    "        StructField(\"log_id\",                   StringType(), False),           # run-level GUID\n",
    "        StructField(\"run_id\",                   StringType(), False),           # e.g. \"{run_ts}_{guid8}\"\n",
    "        StructField(\"run_date\",                 DateType(), False),\n",
    "        StructField(\"run_ts\",                   StringType(), False),\n",
    "        StructField(\"source\",                   StringType(), True),\n",
    "\n",
    "        # Timing\n",
    "        StructField(\"status\",                   StringType(), False),           # SUCCESS / FAILED / PARTIAL\n",
    "        StructField(\"run_start\",                TimestampType(), False),\n",
    "        StructField(\"run_end\",                  TimestampType(), False),\n",
    "        StructField(\"duration_seconds\",         DoubleType(), True),\n",
    "\n",
    "        # Table counts\n",
    "        StructField(\"total_tables\",             LongType(), False),\n",
    "        StructField(\"tables_success\",           LongType(), False),\n",
    "        StructField(\"tables_empty\",             LongType(), False),\n",
    "        StructField(\"tables_failed\",            LongType(), False),\n",
    "        StructField(\"tables_skipped\",           LongType(), False),\n",
    "        StructField(\"total_rows\",               LongType(), False),\n",
    "\n",
    "        # Performance metrics\n",
    "        StructField(\"workers\",                  LongType(), True),               # Parallel workers used\n",
    "        StructField(\"sum_task_seconds\",         DoubleType(), True),    # Sum of all task durations\n",
    "        StructField(\"theoretical_min_sec\",      DoubleType(), True), # Sum / workers\n",
    "        StructField(\"actual_time_sec\",          DoubleType(), True),     # Wall clock time\n",
    "        StructField(\"efficiency_pct\",           DoubleType(), True),      # (theoretical / actual) * 100\n",
    "\n",
    "        # Failed tables list\n",
    "        StructField(\"failed_tables\",            StringType(), True),       # JSON array of failed table names\n",
    "        StructField(\"error_message\",            StringType(), True),        # batch-level error, if any\n",
    "        ]\n",
    ")\n",
    "\n",
    "logger.info(f\"Bronze summary schema defined: {BRONZE_SUMMARY_TABLE_FULLNAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## [4] Silver Processing Log Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silver processing log - tracks CDC merge operations\n",
    "SILVER_LOG_TABLE = \"silver_processing_log\"\n",
    "SILVER_LOG_TABLE_FULLNAME = f\"{LOG_SCHEMA}.{SILVER_LOG_TABLE}\"\n",
    "\n",
    "silver_processing_log_schema = StructType([\n",
    "    # Identifiers\n",
    "    StructField(\"log_id\",           StringType(),    False),\n",
    "    StructField(\"run_id\",           StringType(),    False),\n",
    "    StructField(\"run_date\",         DateType(),      False),\n",
    "    StructField(\"run_ts\",           StringType(),    False),\n",
    "    \n",
    "    # Source information\n",
    "    StructField(\"source\",           StringType(),    False),\n",
    "    StructField(\"table_name\",       StringType(),    False),\n",
    "    StructField(\"load_mode\",        StringType(),    True),\n",
    "    \n",
    "    # Processing results\n",
    "    StructField(\"status\",           StringType(),    False),  # SUCCESS, FAILED, SKIPPED\n",
    "    \n",
    "    # CDC statistics\n",
    "    StructField(\"rows_inserted\",    LongType(),      True),   # New rows added to Silver\n",
    "    StructField(\"rows_updated\",     LongType(),      True),   # Existing rows updated\n",
    "    StructField(\"rows_deleted\",     LongType(),      True),   # Rows marked as deleted\n",
    "    StructField(\"rows_unchanged\",   LongType(),      True),   # Rows with no changes\n",
    "    StructField(\"total_silver_rows\",LongType(),      True),   # Total rows in Silver after merge\n",
    "    \n",
    "    # Bronze source info\n",
    "    StructField(\"bronze_rows\",      LongType(),      True),   # Rows processed from Bronze\n",
    "    StructField(\"bronze_table\",     StringType(),    True),   # Source Bronze table\n",
    "    \n",
    "    # Timing\n",
    "    StructField(\"start_time\",       TimestampType(), True),\n",
    "    StructField(\"end_time\",         TimestampType(), True),\n",
    "    StructField(\"duration_seconds\", LongType(),      True),\n",
    "    \n",
    "    # Error handling\n",
    "    StructField(\"error_message\",    StringType(),    True),\n",
    "    \n",
    "    # Target\n",
    "    StructField(\"silver_table\",     StringType(),    True),   # Target Silver table\n",
    "])\n",
    "\n",
    "logger.info(f\"Silver log schema defined: {SILVER_LOG_TABLE_FULLNAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## [5] Silver Run Summary Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silver run summary - aggregated CDC statistics per run\n",
    "SILVER_SUMMARY_TABLE = \"silver_run_summary\"\n",
    "SILVER_SUMMARY_TABLE_FULLNAME = f\"{LOG_SCHEMA}.{SILVER_SUMMARY_TABLE}\"\n",
    "\n",
    "silver_run_summary_schema = StructType([\n",
    "    # Run identifiers\n",
    "    StructField(\"run_id\",              StringType(),    False),\n",
    "    StructField(\"source\",              StringType(),    False),\n",
    "    StructField(\"run_ts\",              StringType(),    False),\n",
    "    StructField(\"run_date\",            DateType(),      False),\n",
    "    \n",
    "    # Timing\n",
    "    StructField(\"run_start\",           TimestampType(), False),\n",
    "    StructField(\"run_end\",             TimestampType(), True),\n",
    "    StructField(\"duration_seconds\",    LongType(),      True),\n",
    "    \n",
    "    # Table counts\n",
    "    StructField(\"total_tables\",        LongType(),      False),\n",
    "    StructField(\"tables_success\",      LongType(),      True),\n",
    "    StructField(\"tables_failed\",       LongType(),      True),\n",
    "    StructField(\"tables_skipped\",      LongType(),      True),\n",
    "    \n",
    "    # Aggregate CDC statistics\n",
    "    StructField(\"total_inserts\",       LongType(),      True),\n",
    "    StructField(\"total_updates\",       LongType(),      True),\n",
    "    StructField(\"total_deletes\",       LongType(),      True),\n",
    "    StructField(\"total_unchanged\",     LongType(),      True),\n",
    "    \n",
    "    # Failed tables\n",
    "    StructField(\"failed_tables\",       StringType(),    True),\n",
    "])\n",
    "\n",
    "logger.info(f\"Silver summary schema defined: {SILVER_SUMMARY_TABLE_FULLNAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## [6] Create Log Tables (Idempotent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure logs schema exists\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {LOG_SCHEMA}\")\n",
    "logger.info(f\"\u2713 Schema '{LOG_SCHEMA}' ready\")\n",
    "\n",
    "if not spark.catalog.tableExists(BRONZE_LOG_TABLE_FULLNAME):\n",
    "    empty_df = spark.createDataFrame([], bronze_processing_log_schema)\n",
    "    (empty_df.write\n",
    "         .format(\"delta\")\n",
    "         .partitionBy(\"run_date\", \"table_name\")\n",
    "         .mode(\"append\")   # of .mode(\"ignore\")\n",
    "         .saveAsTable(BRONZE_LOG_TABLE_FULLNAME))\n",
    "\n",
    "if not spark.catalog.tableExists(BRONZE_SUMMARY_TABLE_FULLNAME):\n",
    "    empty_df = spark.createDataFrame([], bronze_run_summary_schema)\n",
    "    (empty_df.write\n",
    "         .format(\"delta\")\n",
    "         .mode(\"append\")   # of .mode(\"ignore\")\n",
    "         .saveAsTable(BRONZE_SUMMARY_TABLE_FULLNAME))\n",
    "\n",
    "if not spark.catalog.tableExists(SILVER_LOG_TABLE_FULLNAME):\n",
    "    empty_df = spark.createDataFrame([], silver_processing_log_schema)\n",
    "    (empty_df.write\n",
    "         .format(\"delta\")\n",
    "         .partitionBy(\"run_date\", \"table_name\")\n",
    "         .mode(\"append\")   # of .mode(\"ignore\")\n",
    "         .saveAsTable(SILVER_LOG_TABLE_FULLNAME))\n",
    "\n",
    "if not spark.catalog.tableExists(SILVER_SUMMARY_TABLE_FULLNAME):\n",
    "    empty_df = spark.createDataFrame([], silver_run_summary_schema)\n",
    "    (empty_df.write\n",
    "         .format(\"delta\")\n",
    "         .mode(\"append\")   # of .mode(\"ignore\")\n",
    "         .saveAsTable(SILVER_SUMMARY_TABLE_FULLNAME))\n",
    "\n",
    "logger.info(\"\\n\u2713 All log tables ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## [7] Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_run_date(run_ts: str) -> date:\n",
    "    \"\"\"\n",
    "    Convert a run_ts like '20251005T142752505' into a Python date(2025, 10, 5).\n",
    "    \n",
    "    This avoids Spark date parsing issues with ANSI mode.\n",
    "    \"\"\"\n",
    "    if not run_ts or len(run_ts) < 8:\n",
    "        raise ValueError(f\"run_ts '{run_ts}' is not in expected yyyymmddThhmmss format\")\n",
    "    \n",
    "    y = int(run_ts[0:4])\n",
    "    m = int(run_ts[4:6])\n",
    "    d = int(run_ts[6:8])\n",
    "    return date(y, m, d)\n",
    "\n",
    "\n",
    "def truncate_error_message(error_msg: Optional[str], max_length: int = 1000) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Truncate error messages to prevent bloating log tables.\n",
    "    \"\"\"\n",
    "    if not error_msg:\n",
    "        return None\n",
    "    \n",
    "    if len(error_msg) <= max_length:\n",
    "        return error_msg\n",
    "    \n",
    "    return error_msg[:max_length] + \"... [TRUNCATED]\"\n",
    "\n",
    "\n",
    "logger.info(\"\u2713 Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## [8] Bronze Logging Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_bronze_batch(bronze_results: List[Dict[str, Any]], run_log_id: str) -> None:\n",
    "    \"\"\"\n",
    "    Write many Bronze log records in a single batch append.\n",
    "    \n",
    "    Args:\n",
    "        bronze_results: List of dicts with Bronze processing results\n",
    "    \n",
    "    Each record should contain:\n",
    "        - log_id, run_id, run_ts, source, table_name, load_mode\n",
    "        - status, rows_read, rows_written\n",
    "        - start_time, end_time, duration_seconds\n",
    "        - error_message, parquet_path, delta_table\n",
    "    \"\"\"\n",
    "    if not bronze_results:\n",
    "        return\n",
    "\n",
    "    rows = []\n",
    "    for r in bronze_results:\n",
    "        # Ensure log_id and partition_key are set\n",
    "        log_id = r.get(\"log_id\") or f\"{run_log_id}_{uuid4().hex[:8]}\"\n",
    "        partition_key = r.get(\"partition_key\") or r.get(\"run_ts\")\n",
    "        \n",
    "        # Truncate error message\n",
    "        error_msg = truncate_error_message(r.get(\"error_message\"))\n",
    "        \n",
    "        rows.append(\n",
    "            (\n",
    "                log_id,\n",
    "                run_log_id,\n",
    "                r.get(\"run_id\"),\n",
    "                r.get(\"run_date\"),\n",
    "                r.get(\"run_ts\"),\n",
    "                r.get(\"source\"),\n",
    "                r.get(\"table_name\"),\n",
    "                partition_key,\n",
    "                r.get(\"load_mode\"),\n",
    "                r.get(\"status\"),\n",
    "                r.get(\"rows_processed\"),\n",
    "                r.get(\"start_time\"),\n",
    "                r.get(\"end_time\"),\n",
    "                r.get(\"duration_seconds\"),\n",
    "                error_msg,\n",
    "                r.get(\"parquet_path\"),\n",
    "                r.get(\"delta_table\"),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    df = spark.createDataFrame(rows, schema=bronze_processing_log_schema)\n",
    "    \n",
    "    (df.write\n",
    "       .format(\"delta\")\n",
    "       .mode(\"append\")\n",
    "       .saveAsTable(BRONZE_LOG_TABLE_FULLNAME))\n",
    "    \n",
    "    logger.info(f\"\u2713 Logged {len(bronze_results)} Bronze records to {BRONZE_LOG_TABLE_FULLNAME}\")\n",
    "\n",
    "def log_bronze_summary(summary: dict) -> str:\n",
    "    \"\"\"\n",
    "    Write Bronze run summary.\n",
    "\n",
    "    Args:\n",
    "        summary: Dict with run-level statistics.\n",
    "\n",
    "    Returns:\n",
    "        Run-level log_id used to link processing log rows.\n",
    "    \"\"\"\n",
    "    \n",
    "    log_id = summary.get(\"log_id\") or uuid4().hex\n",
    "\n",
    "    run_ts = summary[\"run_ts\"]\n",
    "    run_id = summary.get(\"run_id\") or f\"{run_ts}_{log_id[:8]}\"\n",
    "\n",
    "    row = {\n",
    "        \"log_id\":               log_id,\n",
    "        \"run_id\":               run_id,\n",
    "        \"run_date\":             summary[\"run_date\"],\n",
    "        \"run_ts\":               run_ts,\n",
    "        \"source\":               summary.get(\"source\"),\n",
    "\n",
    "        \"status\":               summary.get(\"status\", \"SUCCESS\"),\n",
    "        \"run_start\":            summary[\"run_start\"],\n",
    "        \"run_end\":              summary[\"run_end\"],\n",
    "        \"duration_seconds\":     summary.get(\"duration_seconds\"),\n",
    "\n",
    "        \"total_tables\":         summary[\"total_tables\"],\n",
    "        \"tables_success\":       summary[\"tables_success\"],\n",
    "        \"tables_empty\":         summary[\"tables_empty\"],\n",
    "        \"tables_failed\":        summary[\"tables_failed\"],\n",
    "        \"tables_skipped\":       summary[\"tables_skipped\"],\n",
    "        \"total_rows\":           summary[\"total_rows\"],\n",
    "        \"workers\":              summary[\"workers\"],\n",
    "\n",
    "        \"sum_task_seconds\":     summary.get(\"sum_task_seconds\"),\n",
    "        \"theoretical_min_sec\":  summary.get(\"theoretical_min_sec\"),\n",
    "        \"actual_time_sec\":      summary.get(\"actual_time_sec\"),\n",
    "        \"efficiency_pct\":       summary.get(\"efficiency_pct\"),\n",
    "\n",
    "        \"failed_tables\":        summary.get(\"failed_tables\"),\n",
    "        \"error_message\":        summary.get(\"error_message\"),\n",
    "    }\n",
    "\n",
    "    df = spark.createDataFrame([row], schema=bronze_run_summary_schema)\n",
    "\n",
    "    (\n",
    "        df.write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"append\")\n",
    "        .saveAsTable(BRONZE_SUMMARY_TABLE_FULLNAME)\n",
    "    )\n",
    "\n",
    "    return log_id\n",
    "\n",
    "\n",
    "logger.info(\"\u2713 Bronze logging functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## [9] Silver Logging Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_silver_batch(records: List[Dict[str, Any]]) -> None:\n",
    "    \"\"\"\n",
    "    Write many Silver log records in a single batch append.\n",
    "    \n",
    "    Args:\n",
    "        records: List of dicts with Silver CDC merge results\n",
    "    \n",
    "    Each record should contain:\n",
    "        - log_id, run_id, run_ts, source, table_name, load_mode\n",
    "        - status, rows_inserted, rows_updated, rows_deleted, rows_unchanged\n",
    "        - total_silver_rows, bronze_rows, bronze_table\n",
    "        - start_time, end_time, duration_seconds\n",
    "        - error_message, silver_table\n",
    "    \"\"\"\n",
    "    if not records:\n",
    "        return\n",
    "    \n",
    "    rows = []\n",
    "    for r in records:\n",
    "        run_ts = r.get(\"run_ts\")\n",
    "        if not run_ts:\n",
    "            raise ValueError(\"Silver log record is missing run_ts\")\n",
    "        \n",
    "        rd = r.get(\"run_date\")\n",
    "        if rd is None:\n",
    "            rd = build_run_date(run_ts)\n",
    "        \n",
    "        error_msg = truncate_error_message(r.get(\"error_message\"))\n",
    "        \n",
    "        rows.append(Row(\n",
    "            log_id           = r.get(\"log_id\"),\n",
    "            run_id           = r.get(\"run_id\"),\n",
    "            run_date         = rd,\n",
    "            run_ts           = run_ts,\n",
    "            source           = r.get(\"source\"),\n",
    "            table_name       = r.get(\"table_name\"),\n",
    "            load_mode        = r.get(\"load_mode\"),\n",
    "            status           = r.get(\"status\"),\n",
    "            rows_inserted    = r.get(\"rows_inserted\"),\n",
    "            rows_updated     = r.get(\"rows_updated\"),\n",
    "            rows_deleted     = r.get(\"rows_deleted\"),\n",
    "            rows_unchanged   = r.get(\"rows_unchanged\"),\n",
    "            total_silver_rows= r.get(\"total_silver_rows\"),\n",
    "            bronze_rows      = r.get(\"bronze_rows\"),\n",
    "            bronze_table     = r.get(\"bronze_table\"),\n",
    "            start_time       = r.get(\"start_time\"),\n",
    "            end_time         = r.get(\"end_time\"),\n",
    "            duration_seconds = r.get(\"duration_seconds\"),\n",
    "            error_message    = error_msg,\n",
    "            silver_table     = r.get(\"silver_table\"),\n",
    "        ))\n",
    "    \n",
    "    df = spark.createDataFrame(rows, schema=silver_log_schema)\n",
    "    \n",
    "    (df.write\n",
    "       .format(\"delta\")\n",
    "       .mode(\"append\")\n",
    "       .saveAsTable(SILVER_LOG_TABLE_FULLNAME))\n",
    "    \n",
    "    logger.info(f\"\u2713 Logged {len(records)} Silver records to {SILVER_LOG_TABLE_FULLNAME}\")\n",
    "\n",
    "\n",
    "def log_silver_summary(summary: Dict[str, Any]) -> None:\n",
    "    \"\"\"\n",
    "    Write Silver run summary.\n",
    "    \n",
    "    Args:\n",
    "        summary: Dict with run-level CDC statistics\n",
    "    \"\"\"\n",
    "    run_ts = summary.get(\"run_ts\")\n",
    "    if not run_ts:\n",
    "        raise ValueError(\"Summary missing run_ts\")\n",
    "    \n",
    "    run_date = summary.get(\"run_date\")\n",
    "    if run_date is None:\n",
    "        run_date = build_run_date(run_ts)\n",
    "    \n",
    "    import json\n",
    "    failed_tables = summary.get(\"failed_tables\", [])\n",
    "    failed_tables_json = json.dumps(failed_tables) if failed_tables else None\n",
    "    \n",
    "    row = Row(\n",
    "        run_id              = summary.get(\"run_id\"),\n",
    "        source              = summary.get(\"source\"),\n",
    "        run_ts              = run_ts,\n",
    "        run_date            = run_date,\n",
    "        run_start           = summary.get(\"run_start\"),\n",
    "        run_end             = summary.get(\"run_end\"),\n",
    "        duration_seconds    = summary.get(\"duration_seconds\"),\n",
    "        total_tables        = summary.get(\"total_tables\"),\n",
    "        tables_success      = summary.get(\"tables_success\"),\n",
    "        tables_failed       = summary.get(\"tables_failed\"),\n",
    "        tables_skipped      = summary.get(\"tables_skipped\"),\n",
    "        total_inserts       = summary.get(\"total_inserts\"),\n",
    "        total_updates       = summary.get(\"total_updates\"),\n",
    "        total_deletes       = summary.get(\"total_deletes\"),\n",
    "        total_unchanged     = summary.get(\"total_unchanged\"),\n",
    "        failed_tables       = failed_tables_json,\n",
    "    )\n",
    "    \n",
    "    df = spark.createDataFrame([row], schema=silver_summary_schema)\n",
    "    \n",
    "    (df.write\n",
    "       .format(\"delta\")\n",
    "       .mode(\"append\")\n",
    "       .saveAsTable(SILVER_SUMMARY_TABLE_FULLNAME))\n",
    "    \n",
    "    logger.info(f\"\u2713 Logged Silver summary to {SILVER_SUMMARY_TABLE_FULLNAME}\")\n",
    "\n",
    "\n",
    "logger.info(\"\u2713 Silver logging functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## [10] Query Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bronze_logs_for_run(run_ts: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Get all Bronze processing logs for a specific run_ts.\n",
    "    \"\"\"\n",
    "    return spark.table(BRONZE_LOG_TABLE_FULLNAME).where(F.col(\"run_ts\") == run_ts)\n",
    "\n",
    "\n",
    "def get_silver_logs_for_run(run_ts: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Get all Silver processing logs for a specific run_ts.\n",
    "    \"\"\"\n",
    "    return spark.table(SILVER_LOG_TABLE_FULLNAME).where(F.col(\"run_ts\") == run_ts)\n",
    "\n",
    "\n",
    "def get_failed_tables(run_ts: str, layer: str = \"bronze\") -> List[str]:\n",
    "    \"\"\"\n",
    "    Get list of failed table names for a run_ts.\n",
    "    \n",
    "    Args:\n",
    "        run_ts: Run timestamp\n",
    "        layer: \"bronze\" or \"silver\"\n",
    "    \n",
    "    Returns:\n",
    "        List of table names with status='FAILED'\n",
    "    \"\"\"\n",
    "    table = BRONZE_LOG_TABLE_FULLNAME if layer == \"bronze\" else SILVER_LOG_TABLE_FULLNAME\n",
    "    \n",
    "    failed = spark.table(table) \\\n",
    "        .where(f\"run_ts = '{run_ts}' AND status = 'FAILED'\") \\\n",
    "        .select(\"table_name\") \\\n",
    "        .distinct() \\\n",
    "        .collect()\n",
    "    \n",
    "    return [row.table_name for row in failed]\n",
    "\n",
    "\n",
    "def get_successful_tables(run_ts: str, layer: str = \"bronze\") -> List[str]:\n",
    "    \"\"\"\n",
    "    Get list of successful table names for a run_ts.\n",
    "    \n",
    "    Args:\n",
    "        run_ts: Run timestamp\n",
    "        layer: \"bronze\" or \"silver\"\n",
    "    \n",
    "    Returns:\n",
    "        List of table names with status='SUCCESS'\n",
    "    \"\"\"\n",
    "    table = BRONZE_LOG_TABLE_FULLNAME if layer == \"bronze\" else SILVER_LOG_TABLE_FULLNAME\n",
    "    \n",
    "    success = spark.table(table) \\\n",
    "        .where(f\"run_ts = '{run_ts}' AND status = 'SUCCESS'\") \\\n",
    "        .select(\"table_name\") \\\n",
    "        .distinct() \\\n",
    "        .collect()\n",
    "    \n",
    "    return [row.table_name for row in success]\n",
    "\n",
    "\n",
    "def is_table_processed(run_ts: str, table_name: str, layer: str = \"bronze\") -> bool:\n",
    "    \"\"\"\n",
    "    Check if a specific table was successfully processed for a run_ts.\n",
    "    \n",
    "    Returns:\n",
    "        True if table has status='SUCCESS' for this run_ts\n",
    "    \"\"\"\n",
    "    table = BRONZE_LOG_TABLE_FULLNAME if layer == \"bronze\" else SILVER_LOG_TABLE_FULLNAME\n",
    "    \n",
    "    count = spark.table(table) \\\n",
    "        .where(f\"run_ts = '{run_ts}' AND table_name = '{table_name}' AND status = 'SUCCESS'\") \\\n",
    "        .count()\n",
    "    \n",
    "    return count > 0\n",
    "\n",
    "\n",
    "def get_latest_run_summary(source: str, layer: str = \"bronze\") -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Get the most recent run summary for a source.\n",
    "    \n",
    "    Returns:\n",
    "        Dict with summary data, or None if no runs found\n",
    "    \"\"\"\n",
    "    table = BRONZE_SUMMARY_TABLE_FULLNAME if layer == \"bronze\" else SILVER_SUMMARY_TABLE_FULLNAME\n",
    "    \n",
    "    latest = spark.table(table) \\\n",
    "        .where(f\"source = '{source}'\") \\\n",
    "        .orderBy(F.col(\"run_ts\").desc()) \\\n",
    "        .limit(1) \\\n",
    "        .collect()\n",
    "    \n",
    "    if not latest:\n",
    "        return None\n",
    "    \n",
    "    return latest[0].asDict()\n",
    "\n",
    "\n",
    "logger.info(\"\u2713 Query helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## [11] Verification\n",
    "\n",
    "Quick verification that all tables exist and are queryable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logger.info(\"=\" * 80)\n",
    "# logger.info(\"LOGGING INFRASTRUCTURE VERIFICATION\")\n",
    "# logger.info(\"=\" * 80)\n",
    "\n",
    "# tables_to_check = [\n",
    "#     BRONZE_LOG_TABLE_FULLNAME,\n",
    "#     BRONZE_SUMMARY_TABLE_FULLNAME,\n",
    "#     SILVER_LOG_TABLE_FULLNAME,\n",
    "#     SILVER_SUMMARY_TABLE_FULLNAME,\n",
    "# ]\n",
    "\n",
    "# for table_name in tables_to_check:\n",
    "#     if not spark.catalog.tableExists(table_name):\n",
    "#         logger.info(f\"\u2717 {table_name:<40} NOT FOUND\")\n",
    "#         continue\n",
    "\n",
    "#     try:\n",
    "#         count = spark.table(table_name).count()\n",
    "#         logger.info(f\"\u2713 {table_name:<40} {count:>10,} rows\")\n",
    "#     except Exception as e:\n",
    "#         logger.info(f\"! {table_name:<40} ERROR: {type(e).__name__}: {e}\")\n",
    "\n",
    "# logger.info(\"\\n\u2713 Logging infrastructure ready for Bronze and Silver processing\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}