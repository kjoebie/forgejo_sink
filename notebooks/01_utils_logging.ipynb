{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# 01 — Logging Utilities for Bronze and Silver Processing\n",
    "\n",
    "This notebook defines the logging infrastructure for the data pipeline:\n",
    "\n",
    "## Bronze Logging\n",
    "- `logs.bronze_processing_log` - Per-table processing results\n",
    "- `logs.bronze_run_summary` - Aggregated run statistics\n",
    "\n",
    "## Silver Logging\n",
    "- `logs.silver_processing_log` - Per-table CDC merge results\n",
    "- `logs.silver_run_summary` - Aggregated CDC statistics\n",
    "\n",
    "## Key Features\n",
    "- Batch logging (one write per run, not per table)\n",
    "- Append-only (no MERGE, maximum concurrency)\n",
    "- Partitioned by `run_date` and `table_name`\n",
    "- Helper functions for log retrieval and analysis\n",
    "\n",
    "**Architecture:** Bronze uses append with run_ts history for full CDC capability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters (Papermill compatible)\n",
    "# These can be overridden when running via notebook orchestration\n",
    "LOG_SCHEMA = \"logs\"  # Database for all log tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## [1] Setup: Imports and Schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": "import logging\n\nlogger = logging.getLogger(__name__)\nlogger.setLevel(logging.INFO)\n\nlogger.info(\"✓ Imports loaded\")"
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## [2] Bronze Processing Log Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": "# Import Bronze log schema from module\nfrom modules.log_schemas import (\n    BRONZE_LOG_TABLE,\n    BRONZE_LOG_TABLE_FULLNAME,\n    bronze_processing_log_schema\n)\n\nlogger.info(f\"✓ Bronze log schema imported: {BRONZE_LOG_TABLE_FULLNAME}\")"
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## [3] Bronze Run Summary Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": "# Import Bronze summary schema from module\nfrom modules.log_schemas import (\n    BRONZE_SUMMARY_TABLE,\n    BRONZE_SUMMARY_TABLE_FULLNAME,\n    bronze_run_summary_schema\n)\n\nlogger.info(f\"✓ Bronze summary schema imported: {BRONZE_SUMMARY_TABLE_FULLNAME}\")"
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## [4] Silver Processing Log Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": "# Import Silver log schema from module\nfrom modules.log_schemas import (\n    SILVER_LOG_TABLE,\n    SILVER_LOG_TABLE_FULLNAME,\n    silver_processing_log_schema\n)\n\nlogger.info(f\"✓ Silver log schema imported: {SILVER_LOG_TABLE_FULLNAME}\")"
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## [5] Silver Run Summary Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": "# Import Silver summary schema from module\nfrom modules.log_schemas import (\n    SILVER_SUMMARY_TABLE,\n    SILVER_SUMMARY_TABLE_FULLNAME,\n    silver_run_summary_schema\n)\n\nlogger.info(f\"✓ Silver summary schema imported: {SILVER_SUMMARY_TABLE_FULLNAME}\")"
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## [6] Create Log Tables (Idempotent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": "# Ensure logs schema exists\nfrom modules.log_schemas import LOG_SCHEMA\n\nspark.sql(f\"CREATE SCHEMA IF NOT EXISTS {LOG_SCHEMA}\")\nlogger.info(f\"✓ Schema '{LOG_SCHEMA}' ready\")\n\n# Create Bronze processing log table\nif not spark.catalog.tableExists(BRONZE_LOG_TABLE_FULLNAME):\n    empty_df = spark.createDataFrame([], bronze_processing_log_schema)\n    (empty_df.write\n         .format(\"delta\")\n         .partitionBy(\"run_date\", \"table_name\")\n         .mode(\"append\")\n         .saveAsTable(BRONZE_LOG_TABLE_FULLNAME))\n    logger.info(f\"✓ Created table: {BRONZE_LOG_TABLE_FULLNAME}\")\n\n# Create Bronze summary table\nif not spark.catalog.tableExists(BRONZE_SUMMARY_TABLE_FULLNAME):\n    empty_df = spark.createDataFrame([], bronze_run_summary_schema)\n    (empty_df.write\n         .format(\"delta\")\n         .mode(\"append\")\n         .saveAsTable(BRONZE_SUMMARY_TABLE_FULLNAME))\n    logger.info(f\"✓ Created table: {BRONZE_SUMMARY_TABLE_FULLNAME}\")\n\n# Create Silver processing log table\nif not spark.catalog.tableExists(SILVER_LOG_TABLE_FULLNAME):\n    empty_df = spark.createDataFrame([], silver_processing_log_schema)\n    (empty_df.write\n         .format(\"delta\")\n         .partitionBy(\"run_date\", \"table_name\")\n         .mode(\"append\")\n         .saveAsTable(SILVER_LOG_TABLE_FULLNAME))\n    logger.info(f\"✓ Created table: {SILVER_LOG_TABLE_FULLNAME}\")\n\n# Create Silver summary table\nif not spark.catalog.tableExists(SILVER_SUMMARY_TABLE_FULLNAME):\n    empty_df = spark.createDataFrame([], silver_run_summary_schema)\n    (empty_df.write\n         .format(\"delta\")\n         .mode(\"append\")\n         .saveAsTable(SILVER_SUMMARY_TABLE_FULLNAME))\n    logger.info(f\"✓ Created table: {SILVER_SUMMARY_TABLE_FULLNAME}\")\n\nlogger.info(\"\\n✓ All log tables ready\")"
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## [7] Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": "# Import helper functions from module\nfrom modules.logging_utils import (\n    build_run_date,\n    truncate_error_message\n)\n\nlogger.info(\"✓ Helper functions imported from modules.logging_utils\")"
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## [8] Bronze Logging Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": "# Note: log_batch() and log_summary() functions are defined in this notebook\n# because they depend on the Spark session and schemas being available.\n# These are also available in modules.logging_utils but the notebook versions\n# allow for interactive setup and testing.\n\nfrom uuid import uuid4\nfrom pyspark.sql import Row\nfrom pyspark.sql import functions as F\nfrom typing import List, Dict, Any, Optional\n\ndef _prepare_bronze_rows(bronze_results: List[Dict[str, Any]], run_log_id: str):\n    rows = []\n    for r in bronze_results:\n        log_id = r.get(\"log_id\") or f\"{run_log_id}_{uuid4().hex[:8]}\"\n        partition_key = r.get(\"partition_key\") or r.get(\"run_ts\")\n        error_msg = truncate_error_message(r.get(\"error_message\"))\n\n        rows.append(\n            (\n                log_id,\n                run_log_id,\n                r.get(\"run_id\"),\n                r.get(\"run_date\"),\n                r.get(\"run_ts\"),\n                r.get(\"source\"),\n                r.get(\"table_name\"),\n                partition_key,\n                r.get(\"load_mode\"),\n                r.get(\"status\"),\n                r.get(\"rows_processed\"),\n                r.get(\"start_time\"),\n                r.get(\"end_time\"),\n                r.get(\"duration_seconds\"),\n                error_msg,\n                r.get(\"parquet_path\"),\n                r.get(\"delta_table\"),\n            )\n        )\n    return rows\n\n\ndef _prepare_silver_rows(records: List[Dict[str, Any]]):\n    rows = []\n    for r in records:\n        run_ts = r.get(\"run_ts\")\n        if not run_ts:\n            raise ValueError(\"Silver log record is missing run_ts\")\n\n        run_date = r.get(\"run_date\")\n        if run_date is None:\n            run_date = build_run_date(run_ts)\n\n        error_msg = truncate_error_message(r.get(\"error_message\"))\n\n        rows.append(Row(\n            log_id           = r.get(\"log_id\"),\n            run_id           = r.get(\"run_id\"),\n            run_date         = run_date,\n            run_ts           = run_ts,\n            source           = r.get(\"source\"),\n            table_name       = r.get(\"table_name\"),\n            load_mode        = r.get(\"load_mode\"),\n            status           = r.get(\"status\"),\n            rows_inserted    = r.get(\"rows_inserted\"),\n            rows_updated     = r.get(\"rows_updated\"),\n            rows_deleted     = r.get(\"rows_deleted\"),\n            rows_unchanged   = r.get(\"rows_unchanged\"),\n            total_silver_rows= r.get(\"total_silver_rows\"),\n            bronze_rows      = r.get(\"bronze_rows\"),\n            bronze_table     = r.get(\"bronze_table\"),\n            start_time       = r.get(\"start_time\"),\n            end_time         = r.get(\"end_time\"),\n            duration_seconds = r.get(\"duration_seconds\"),\n            error_message    = error_msg,\n            silver_table     = r.get(\"silver_table\"),\n        ))\n    return rows\n\n\ndef log_batch(records: List[Dict[str, Any]], layer: str, run_log_id: Optional[str] = None) -> None:\n    \"\"\"\n    Write many log records in a single batch append for the given layer.\n    \"\"\"\n    if not records:\n        return\n\n    layer = layer.lower()\n    if layer == \"bronze\":\n        if not run_log_id:\n            raise ValueError(\"run_log_id is required for Bronze batch logging\")\n        rows = _prepare_bronze_rows(records, run_log_id)\n        schema = bronze_processing_log_schema\n        table = BRONZE_LOG_TABLE_FULLNAME\n    elif layer == \"silver\":\n        rows = _prepare_silver_rows(records)\n        schema = silver_processing_log_schema\n        table = SILVER_LOG_TABLE_FULLNAME\n    else:\n        raise ValueError(\"layer must be 'bronze' or 'silver'\")\n\n    df = spark.createDataFrame(rows, schema=schema)\n\n    (df.write\n        .format(\"delta\")\n        .mode(\"append\")\n        .saveAsTable(table))\n\n    logger.info(f\"✓ Logged {len(records)} {layer.capitalize()} records to {table}\")\n\n\ndef log_summary(summary: Dict[str, Any], layer: str) -> Optional[str]:\n    \"\"\"\n    Write run summary for Bronze or Silver processing.\n\n    Returns:\n        The Bronze run log_id for linking batch rows, or None for Silver.\n    \"\"\"\n    import json\n    \n    layer = layer.lower()\n\n    if layer == \"bronze\":\n        log_id = summary.get(\"log_id\") or uuid4().hex\n        run_ts = summary[\"run_ts\"]\n        run_id = summary.get(\"run_id\") or f\"{run_ts}_{log_id[:8]}\"\n\n        row = {\n            \"log_id\":               log_id,\n            \"run_id\":               run_id,\n            \"run_date\":             summary[\"run_date\"],\n            \"run_ts\":               run_ts,\n            \"source\":               summary.get(\"source\"),\n            \"status\":               summary.get(\"status\", \"SUCCESS\"),\n            \"run_start\":            summary[\"run_start\"],\n            \"run_end\":              summary[\"run_end\"],\n            \"duration_seconds\":     summary.get(\"duration_seconds\"),\n            \"total_tables\":         summary[\"total_tables\"],\n            \"tables_success\":       summary[\"tables_success\"],\n            \"tables_empty\":         summary[\"tables_empty\"],\n            \"tables_failed\":        summary[\"tables_failed\"],\n            \"tables_skipped\":       summary[\"tables_skipped\"],\n            \"total_rows\":           summary[\"total_rows\"],\n            \"workers\":              summary[\"workers\"],\n            \"sum_task_seconds\":     summary.get(\"sum_task_seconds\"),\n            \"theoretical_min_sec\":  summary.get(\"theoretical_min_sec\"),\n            \"actual_time_sec\":      summary.get(\"actual_time_sec\"),\n            \"efficiency_pct\":       summary.get(\"efficiency_pct\"),\n            \"failed_tables\":        summary.get(\"failed_tables\"),\n            \"error_message\":        summary.get(\"error_message\"),\n        }\n\n        df = spark.createDataFrame([row], schema=bronze_run_summary_schema)\n        table = BRONZE_SUMMARY_TABLE_FULLNAME\n\n    elif layer == \"silver\":\n        run_ts = summary.get(\"run_ts\")\n        if not run_ts:\n            raise ValueError(\"Summary missing run_ts\")\n\n        run_date = summary.get(\"run_date\")\n        if run_date is None:\n            run_date = build_run_date(run_ts)\n\n        failed_tables = summary.get(\"failed_tables\", [])\n        failed_tables_json = json.dumps(failed_tables) if failed_tables else None\n\n        row = Row(\n            run_id              = summary.get(\"run_id\"),\n            source              = summary.get(\"source\"),\n            run_ts              = run_ts,\n            run_date            = run_date,\n            run_start           = summary.get(\"run_start\"),\n            run_end             = summary.get(\"run_end\"),\n            duration_seconds    = summary.get(\"duration_seconds\"),\n            total_tables        = summary.get(\"total_tables\"),\n            tables_success      = summary.get(\"tables_success\"),\n            tables_failed       = summary.get(\"tables_failed\"),\n            tables_skipped      = summary.get(\"tables_skipped\"),\n            total_inserts       = summary.get(\"total_inserts\"),\n            total_updates       = summary.get(\"total_updates\"),\n            total_deletes       = summary.get(\"total_deletes\"),\n            total_unchanged     = summary.get(\"total_unchanged\"),\n            failed_tables       = failed_tables_json,\n        )\n\n        df = spark.createDataFrame([row], schema=silver_run_summary_schema)\n        table = SILVER_SUMMARY_TABLE_FULLNAME\n        log_id = None\n\n    else:\n        raise ValueError(\"layer must be 'bronze' or 'silver'\")\n\n    (df.write\n        .format(\"delta\")\n        .mode(\"append\")\n        .saveAsTable(table))\n\n    logger.info(f\"✓ Logged {layer.capitalize()} summary to {table}\")\n    return log_id\n\n\nlogger.info(\"✓ Logging functions defined for Bronze and Silver layers\")"
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## [9] Silver Logging Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"✓ Generic log_batch/log_summary functions ready for use\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## [10] Query Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": "# Import query helper functions from module\nfrom modules.logging_utils import (\n    get_bronze_logs_for_run,\n    get_silver_logs_for_run,\n    get_failed_tables,\n    get_successful_tables,\n    is_table_processed,\n    get_latest_run_summary\n)\n\nlogger.info(\"✓ Query helper functions imported from modules.logging_utils\")"
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## [11] Verification\n",
    "\n",
    "Quick verification that all tables exist and are queryable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logger.info(\"=\" * 80)\n",
    "# logger.info(\"LOGGING INFRASTRUCTURE VERIFICATION\")\n",
    "# logger.info(\"=\" * 80)\n",
    "\n",
    "# tables_to_check = [\n",
    "#     BRONZE_LOG_TABLE_FULLNAME,\n",
    "#     BRONZE_SUMMARY_TABLE_FULLNAME,\n",
    "#     SILVER_LOG_TABLE_FULLNAME,\n",
    "#     SILVER_SUMMARY_TABLE_FULLNAME,\n",
    "# ]\n",
    "\n",
    "# for table_name in tables_to_check:\n",
    "#     if not spark.catalog.tableExists(table_name):\n",
    "#         logger.info(f\"✗ {table_name:<40} NOT FOUND\")\n",
    "#         continue\n",
    "\n",
    "#     try:\n",
    "#         count = spark.table(table_name).count()\n",
    "#         logger.info(f\"✓ {table_name:<40} {count:>10,} rows\")\n",
    "#     except Exception as e:\n",
    "#         logger.info(f\"! {table_name:<40} ERROR: {type(e).__name__}: {e}\")\n",
    "\n",
    "# logger.info(\"\\n✓ Logging infrastructure ready for Bronze and Silver processing\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}