{"cells":[{"cell_type":"markdown","source":["# 01 – Logging utilities for bronze processing\n","\n","This notebook defines the logging schema and helpers for the bronze processing\n","pipeline:\n","\n","- Creates an append-only Delta table `logs.bronze_processing_log`.\n","- Ensures the table is partitioned by `run_date` and `table_name`.\n","- Provides helper functions to:\n","  - Convert `run_ts` (e.g. `20251005T142752505`) into a `DATE` column (`run_date`).\n","  - Write many log records in one batch (`log_table_processing_batch`).\n","  - Write a single log record (`log_table_processing`).\n","  - Optionally read the latest log row per table (`get_latest_logs`).\n","\n","The design goal is:\n","- **One physical log row per table per run**, written in **one batch** from the master notebook.\n","- **No MERGE**, only `INSERT` (append) for maximum concurrency.\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"39fdf23e-617c-472d-9c8d-9aacec3c2067"},{"cell_type":"code","source":["# [1] Setup logging schema and Delta table\n","\n","from pyspark.sql.types import (\n","    StructType, StructField, StringType, LongType, TimestampType, DateType, DoubleType\n",")\n","from pyspark.sql import Row\n","from pyspark.sql import functions as F\n","from datetime import datetime, date\n","\n","# run log summary\n","SUMMARY_LOG_DB = \"logs\"\n","SUMMARY_LOG_TABLE = \"bronze_run_summary\"\n","SUMMARY_LOG_TABLE_FULLNAME = f\"{SUMMARY_LOG_DB}.{SUMMARY_LOG_TABLE}\"\n","\n","summary_log_schema = StructType([\n","    StructField(\"run_id\",              StringType(), False),\n","    StructField(\"source\",              StringType(),  False),\n","    StructField(\"run_ts\",              StringType(),  False),\n","    StructField(\"run_start\",           TimestampType(),  False),\n","    StructField(\"run_end\",             TimestampType(),  True),\n","    StructField(\"total_tables\",        LongType(), False),\n","    StructField(\"tables_success\",      LongType(), True),\n","    StructField(\"tables_empty\",        LongType(), True),\n","    StructField(\"tables_failed\",       LongType(), True),\n","    StructField(\"tables_skipped\",      LongType(), True),\n","    StructField(\"total_rows\",          LongType(), True),\n","    StructField(\"duration_seconds\",    LongType(), True),\n","    StructField(\"workers\",             LongType(),    True),\n","    StructField(\"sum_task_seconds\",    DoubleType(),  True),\n","    StructField(\"theoretical_min_sec\", DoubleType(),  True),\n","    StructField(\"actual_time_sec\",     DoubleType(),  True),\n","    StructField(\"efficiency_pct\",      DoubleType(),  True),\n","])\n","\n","if not spark.catalog.tableExists(SUMMARY_LOG_TABLE_FULLNAME):\n","    empty_sdf = spark.createDataFrame([], summary_log_schema)\n","\n","    (empty_sdf.write\n","         .format(\"delta\")\n","         .mode(\"overwrite\")\n","         .saveAsTable(SUMMARY_LOG_TABLE_FULLNAME))\n","\n","# Database and table name for the processing log\n","LOG_DB = \"logs\"\n","LOG_TABLE = \"bronze_processing_log\"\n","LOG_TABLE_FULLNAME = f\"{LOG_DB}.{LOG_TABLE}\"\n","\n","# Log table schema (run_date is a DATE, not STRING)\n","log_schema = StructType([\n","    StructField(\"log_id\",           StringType(),  False),\n","    StructField(\"run_id\",           StringType(),  False),\n","    StructField(\"run_date\",         DateType(),    False),\n","    StructField(\"run_ts\",           StringType(),  False),\n","    StructField(\"source\",           StringType(),  False),\n","    StructField(\"table_name\",       StringType(),  False),\n","    StructField(\"load_mode\",        StringType(),  True),\n","    StructField(\"status\",           StringType(),  False),\n","    StructField(\"rows_read\",        LongType(),    True),\n","    StructField(\"rows_written\",     LongType(),    True),\n","    StructField(\"start_time\",       TimestampType(), True),\n","    StructField(\"end_time\",         TimestampType(), True),\n","    StructField(\"duration_seconds\", LongType(),    True),\n","    StructField(\"error_message\",    StringType(),  True),\n","    StructField(\"parquet_path\",     StringType(),  True),\n","    StructField(\"delta_table\",      StringType(),  True),\n","])\n","\n","# Ensure the logs schema exists\n","spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {LOG_DB}\")\n","\n","# Idempotent create of the Delta log table\n","if not spark.catalog.tableExists(LOG_TABLE_FULLNAME):\n","    empty_df = spark.createDataFrame([], log_schema)\n","\n","    (empty_df.write\n","         .format(\"delta\")\n","         .partitionBy(\"run_date\", \"table_name\")\n","         .mode(\"overwrite\")\n","         .saveAsTable(LOG_TABLE_FULLNAME))"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"tags":["parameters"]},"id":"31ed06d4-ba24-470b-9adb-a03177a3bbc1"},{"cell_type":"markdown","source":["## [2] Logging helper functions\n","\n","This cell defines:\n","\n","- `build_run_date(run_ts)` – converts `20251005T142752505` → `date(2025, 10, 5)` using Python.\n","- `log_table_processing_batch(records)` – writes a list of log dicts in **one** append operation.\n","- `log_table_processing(**kwargs)` – convenience wrapper for a single record.\n","- `get_latest_logs(run_ts)` – returns the latest log row per `(source, table_name)` for a given run.\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ae7fd485-9908-488f-a078-f7a0f9a1bbc3"},{"cell_type":"code","source":["# [2] Logging helper functions\n","\n","from pyspark.sql.window import Window\n","\n","def build_run_date(run_ts: str) -> date:\n","    \"\"\"\n","    Convert a run_ts like '20251005T142752505' into a Python date(2025, 10, 5).\n","\n","    We do this in Python to avoid Spark date parsing issues with ANSI mode.\n","    \"\"\"\n","    if not run_ts or len(run_ts) < 8:\n","        raise ValueError(f\"run_ts '{run_ts}' is not in expected yyyymmddThhmmss format\")\n","\n","    y = int(run_ts[0:4])\n","    m = int(run_ts[4:6])\n","    d = int(run_ts[6:8])\n","    return date(y, m, d)\n","\n","\n","def log_table_processing_batch(records, log_db: str = LOG_DB, log_table: str = LOG_TABLE):\n","    \"\"\"\n","    Write many log records in a single append to logs.bronze_processing_log.\n","\n","    records: iterable of dicts with at least:\n","      - log_id, run_ts, source, table_name, load_mode, status\n","      - rows_read, rows_written, start_time, end_time, duration_seconds\n","      - error_message, parquet_path, delta_table\n","    \"\"\"\n","    records = list(records)\n","    if not records:\n","        return\n","\n","    rows = []\n","    for r in records:\n","        run_ts = r.get(\"run_ts\")\n","        if not run_ts:\n","            raise ValueError(\"log record is missing run_ts\")\n","\n","        rd = r.get(\"run_date\")\n","        if rd is None:\n","            rd = build_run_date(run_ts)\n","\n","        rows.append(Row(\n","            log_id           = r.get(\"log_id\"),\n","            run_id           = r.get(\"run_id\"),\n","            run_date         = rd,\n","            run_ts           = run_ts,\n","            source           = r.get(\"source\"),\n","            table_name       = r.get(\"table_name\"),\n","            load_mode        = r.get(\"load_mode\"),\n","            status           = r.get(\"status\"),\n","            rows_read        = r.get(\"rows_read\"),\n","            rows_written     = r.get(\"rows_written\"),\n","            start_time       = r.get(\"start_time\"),\n","            end_time         = r.get(\"end_time\"),\n","            duration_seconds = r.get(\"duration_seconds\"),\n","            error_message    = r.get(\"error_message\"),\n","            parquet_path     = r.get(\"parquet_path\"),\n","            delta_table      = r.get(\"delta_table\"),\n","        ))\n","\n","    df = spark.createDataFrame(rows, schema=log_schema)\n","    full_name = f\"{log_db}.{log_table}\"\n","\n","    (df.write\n","       .format(\"delta\")\n","       .mode(\"append\")\n","       .saveAsTable(full_name))\n","\n","\n","def log_table_processing(**kwargs):\n","    \"\"\"\n","    Convenience wrapper around log_table_processing_batch for a single record.\n","\n","    Usage:\n","        log_table_processing(\n","            log_id=...,\n","            run_id=...,\n","            run_ts=...,\n","            source=...,\n","            table_name=...,\n","            status=\"SUCCESS\",\n","            ...\n","        )\n","    \"\"\"\n","    log_table_processing_batch([kwargs])\n","\n","\n","def get_latest_logs(run_ts: str):\n","    \"\"\"\n","    Return, for a given run_ts, the latest log row per (source, table_name).\n","\n","    With the current design we write exactly one row per table per run_ts,\n","    but this function still works if you ever decide to log multiple times.\n","    \"\"\"\n","    df = spark.table(LOG_TABLE_FULLNAME).where(F.col(\"run_ts\") == run_ts)\n","\n","    w = Window.partitionBy(\"source\", \"table_name\").orderBy(\n","        F.col(\"end_time\").desc_nulls_last(),\n","        F.col(\"start_time\").desc_nulls_last()\n","    )\n","\n","    return (df\n","            .withColumn(\"rn\", F.row_number().over(w))\n","            .where(F.col(\"rn\") == 1)\n","            .drop(\"rn\"))\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"f4ad4f6e-7945-4079-bdc7-58d36280d4f6"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"708925f5-6775-4eaa-abcb-d10995033b8c"}],"default_lakehouse":"708925f5-6775-4eaa-abcb-d10995033b8c","default_lakehouse_name":"lh_gh_bronze","default_lakehouse_workspace_id":"f29eeacf-64cd-431a-913b-2ed71174251e"}}},"nbformat":4,"nbformat_minor":5}