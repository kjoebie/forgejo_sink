{"cells":[{"cell_type":"markdown","source":["# 10 – Bronze load worker\n","\n","This notebook defines the *worker* function that loads a **single table** from\n","parquet into a bronze Delta table.\n","\n","Design:\n","\n","- Does **not** write to the log table.\n","- Reads parquet from OneLake using `file:/lakehouse/default/Files/.../*.parquet`.\n","- Supports three `load_mode` values, coming from the DAG:\n","  - `snapshot` – overwrite the entire Delta table.\n","  - `window` – treated like `snapshot` (the parquet already contains the window).\n","  - `incremental` – append-only.\n","- Returns a Python `dict` with all metrics needed for logging and summary.\n","- Handles:\n","  - Missing parquet files → status = `EMPTY`.\n","  - Unknown `load_mode` → status = `SKIPPED` (no Spark work).\n","  - Probable corrupt Delta table → optional drop+recreate on write error.\n","\n","This notebook is intended to be imported via `%run \"/10_bronze_load\"` from the\n","master notebook.\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b03c6ca2-8e63-40dc-99b0-b91422dc116f"},{"cell_type":"markdown","source":["## [1] Imports and helper functions\n","\n","This cell defines:\n","\n","- `build_parquet_dir(base_files, source_name, run_ts, table_name)` – builds the\n","  OneLake directory for parquet.\n","- `is_missing_path_error(exc)` – detects \"no parquet files found\".\n","- `is_probably_corrupt_delta(exc)` – detects a corrupt or incompatible Delta table.\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"97252fcb-84f9-44e9-8a64-13dee916d66d"},{"cell_type":"code","source":["# [1] Imports and helper functions\n","\n","from pyspark.sql import functions as F\n","from datetime import datetime\n","\n","def build_parquet_dir(base_files: str,\n","                      source_name: str,\n","                      run_ts: str,\n","                      table_name: str) -> str:\n","    \"\"\"\n","    Build the OneLake directory for parquet files of one table and run_ts.\n","\n","    Example result:\n","      /lakehouse/default/Files/greenhouse_sources/anva_meeus/2025/10/05/20251005T142752505/Dim_Kantoor\n","    \"\"\"\n","    if not run_ts or len(run_ts) < 8:\n","        raise ValueError(f\"run_ts '{run_ts}' is not in expected yyyymmddThhmmss format\")\n","\n","    year = run_ts[0:4]\n","    month = run_ts[4:6]\n","    day = run_ts[6:8]\n","\n","    #return f\"/lakehouse/default/Files/{base_files}/{source_name}/{year}/{month}/{day}/{run_ts}/{table_name}\"\n","    return f\"Files/{base_files}/{source_name}/{year}/{month}/{day}/{run_ts}/{table_name}\"\n","\n","def is_missing_path_error(exc: Exception) -> bool:\n","    \"\"\"\n","    Heuristic to detect 'no parquet files found' situations.\n","    \"\"\"\n","    msg = str(exc).lower()\n","    return (\n","        \"path does not exist\" in msg\n","        or \"no such file or directory\" in msg\n","        or \"file not found\" in msg\n","        or \"cannot find path\" in msg\n","    )\n","\n","\n","def is_probably_corrupt_delta(exc: Exception) -> bool:\n","    \"\"\"\n","    Heuristic to detect a broken Delta table that may need to be recreated.\n","    \"\"\"\n","    msg = str(exc).lower()\n","    return (\n","        \"is not a delta table\" in msg\n","        or \"failed to merge fields\" in msg\n","        or \"incompatible format\" in msg\n","        or (\"delta log\" in msg and \"error\" in msg)\n","    )\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"7c63cc4d-544d-42e4-bfd1-e9a430bd1c1c"},{"cell_type":"markdown","source":["## [2] Core worker: process_bronze_table\n","\n","This function:\n","\n","- Loads **one** table's parquet files for a specific `run_ts`.\n","- Writes to a bronze Delta table in the given `bronze_schema`.\n","- Does *not* log to the log table; it only returns a dict.\n","\n","Signature:\n","\n","```python\n","process_bronze_table(\n","    table_def,        # dict from DAG[\"tables\"][...]\n","    source_name,      # e.g. \"anva_meeus\"\n","    run_ts,           # e.g. \"20251005T142752505\"\n","    base_files,       # e.g. \"greenhouse_sources\"\n","    bronze_schema,    # e.g. \"bronze\"\n","    debug=False\n",") -> dict\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"dd921f11-c394-43fe-93a6-b323657633f3"},{"cell_type":"code","source":["# [2] Core worker function\n","from uuid import uuid4\n","\n","def process_bronze_table(table_def,\n","                         source_name: str,\n","                         run_ts: str,\n","                         base_files: str,\n","                         debug: bool = False) -> dict:\n","    \"\"\"\n","    Load a single table's parquet files for a given run_ts into a bronze Delta table.\n","\n","    Returns a dict with all fields needed for logging and summary.\n","    This function does not write to the log table itself; the master notebook\n","    is responsible for batch logging.\n","    \"\"\"\n","    table_name = table_def.get(\"name\")\n","    if not table_name:\n","        raise ValueError(\"table_def is missing 'name'\")\n","\n","    load_mode = (table_def.get(\"load_mode\") or \"snapshot\").lower()\n","    supported_modes = {\"snapshot\", \"window\", \"incremental\"}\n","\n","    log_id = f\"{source_name}:{table_name}:{run_ts}:{uuid4().hex[:8]}\"\n","    # if debug:\n","    #         print(f\"log_id: {log_id}\")\n","\n","    start_time = datetime.utcnow()\n","    end_time = None\n","    status = \"RUNNING\"\n","    error_message = None\n","    rows_read = None\n","    rows_written = None\n","\n","    # If load_mode is not recognised, do not even touch Spark\n","    if load_mode not in supported_modes:\n","        end_time = datetime.utcnow()\n","        duration = int((end_time - start_time).total_seconds())\n","        if debug:\n","            print(f\"[{table_name}] SKIPPED: unsupported load_mode '{load_mode}'\")\n","        return {\n","            \"log_id\": log_id,\n","            \"run_id\": RUN_ID,\n","            \"run_ts\": run_ts,\n","            \"source\": source_name,\n","            \"table_name\": table_name,\n","            \"load_mode\": load_mode,\n","            \"status\": \"SKIPPED\",\n","            \"rows_read\": None,\n","            \"rows_written\": None,\n","            \"start_time\": start_time,\n","            \"end_time\": end_time,\n","            \"duration_seconds\": duration,\n","            \"error_message\": f\"Unsupported load_mode '{load_mode}'\",\n","            \"parquet_path\": None,\n","            \"delta_table\": None,\n","        }\n","\n","    target_table = table_def.get(\"delta_table\") or table_name\n","    delta_schema = table_def.get(\"delta_schema\") or \"unk_schema\"\n","    delta_table_full =  f\"{delta_schema}.{target_table}\"\n","\n","    parquet_dir = build_parquet_dir(base_files, source_name, run_ts, table_name)\n","    parquet_glob = f\"{parquet_dir}/*.parquet\"\n","\n","    # if debug:\n","    #     print(f\"[{table_name}] start ({load_mode}) parquet={parquet_dir} → delta={delta_table_full}\")\n","    #     print(f\"glob: {parquet_glob}\")\n","\n","    # 1) Read parquet\n","    try:\n","        df = spark.read.parquet(parquet_glob)\n","        rows_read = df.count()\n","\n","        if debug:\n","            print(f\"[{table_name}] rows_read={rows_read}\")\n","\n","    except Exception as e:\n","        if is_missing_path_error(e):\n","            # Treat as empty export: no data for this table in this run_ts\n","            end_time = datetime.utcnow()\n","            duration = int((end_time - start_time).total_seconds())\n","            if debug:\n","                print(f\"[{table_name}] EMPTY: no parquet files found in folder {parquet_dir}\")\n","            return {\n","                \"log_id\": log_id,\n","                \"run_id\": RUN_ID,\n","                \"run_ts\": run_ts,\n","                \"source\": source_name,\n","                \"table_name\": table_name,\n","                \"load_mode\": load_mode,\n","                \"status\": \"SKIPPED\",\n","                \"rows_read\": 0,\n","                \"rows_written\": 0,\n","                \"start_time\": start_time,\n","                \"end_time\": end_time,\n","                \"duration_seconds\": duration,\n","                \"error_message\": f\"No parquet files found in folder {parquet_dir}.\",\n","                \"parquet_path\": parquet_dir,\n","                \"delta_table\": delta_table_full,\n","            }\n","        else:\n","            end_time = datetime.utcnow()\n","            duration = int((end_time - start_time).total_seconds())\n","            if debug:\n","                print(f\"[{table_name}] FAILED while reading parquet: {str(e)}\")\n","            return {\n","                \"log_id\": log_id,\n","                \"run_id\": RUN_ID,\n","                \"run_ts\": run_ts,\n","                \"source\": source_name,\n","                \"table_name\": table_name,\n","                \"load_mode\": load_mode,\n","                \"status\": \"FAILED\",\n","                \"rows_read\": None,\n","                \"rows_written\": None,\n","                \"start_time\": start_time,\n","                \"end_time\": end_time,\n","                \"duration_seconds\": duration,\n","                \"error_message\": f\"Read parquet failed: {str(e)}\",\n","                \"parquet_path\": parquet_dir,\n","                \"delta_table\": delta_table_full,\n","            }\n","\n","    # try:\n","    #     rows_read = df.count()\n","    #     rows_written = rows_read\n","    #     if debug:\n","    #         print(f\"[{table_name}] rows_read={rows_read}\")\n","    # except Exception as e:\n","    #     rows_read = None\n","    #     rows_written = None\n","    #     if debug:\n","    #         print(f\"[{table_name}] WARNING: failed to count rows: {str(e)[:200]}\")\n","\n","    # 2) Write to Delta\n","    try:\n","\n","        if load_mode in (\"snapshot\", \"window\"):\n","            # Overwrite the entire table\n","            writer = (df.write\n","                        .format(\"delta\")\n","                        .mode(\"overwrite\")\n","                        .option(\"overwriteSchema\", \"true\"))\n","            writer.saveAsTable(delta_table_full)\n","        elif load_mode == \"incremental\":\n","            # Append only; no pre-read health-check for performance\n","            writer = (df.write\n","                        .format(\"delta\")\n","                        .mode(\"append\"))\n","            writer.saveAsTable(delta_table_full)\n","\n","        rows_written = spark.table(delta_table_full).count()\n","\n","        # Check voor lege dataset\n","        if rows_read == 0:\n","            end_time = datetime.utcnow()\n","            duration = int((end_time - start_time).total_seconds())\n","        \n","            if debug:\n","                print(f\"[{table_name}] EMPTY: parquet exists but contains 0 rows\")\n","\n","            return {\n","                \"log_id\": log_id,\n","                \"run_id\": RUN_ID,\n","                \"run_ts\": run_ts,\n","                \"source\": source_name,\n","                \"table_name\": table_name,\n","                \"load_mode\": load_mode,\n","                \"status\": \"EMPTY\",  \n","                \"rows_read\": 0,\n","                \"rows_written\": 0,\n","                \"start_time\": start_time,\n","                \"end_time\": end_time,\n","                \"duration_seconds\": duration,\n","                \"error_message\": \"Parquet exists but contains 0 rows.\",\n","                \"parquet_path\": parquet_dir,\n","                \"delta_table\": delta_table_full,\n","            }\n","        elif rows_written != 0:\n","            status = \"SUCCESS\"\n","\n","    except Exception as e:\n","        # Try one recovery attempt if the target Delta table looks corrupt\n","        if is_probably_corrupt_delta(e):\n","            if debug:\n","                print(f\"[{table_name}] write failed, attempting drop+recreate due to probable corrupt Delta table: {str(e)}\")\n","            try:\n","                spark.sql(f\"DROP TABLE IF EXISTS {delta_table_full}\")\n","                writer = (df.write\n","                            .format(\"delta\")\n","                            .mode(\"overwrite\")\n","                            .option(\"overwriteSchema\", \"true\"))\n","                writer.saveAsTable(delta_table_full)\n","                status = \"SUCCESS\"\n","                rows_written = spark.table(delta_table_full).count()\n","                error_message = f\"Initial write failed but table was recreated. Original error: {str(e)}\"\n","            except Exception as e2:\n","                status = \"FAILED\"\n","                error_message = f\"Write failed and recovery failed: {str(e2)}\"\n","        else:\n","            status = \"FAILED\"\n","            error_message = f\"Write failed: {str(e)}\"\n","\n","    end_time = datetime.utcnow()\n","    duration = int((end_time - start_time).total_seconds())\n","\n","    if debug:\n","        print(f\"[{table_name}] {status} in {duration}s\")\n","\n","    return {\n","        \"log_id\": log_id,\n","        \"run_id\": RUN_ID,\n","        \"run_ts\": run_ts,\n","        \"source\": source_name,\n","        \"table_name\": table_name,\n","        \"load_mode\": load_mode,\n","        \"status\": status,\n","        \"rows_read\": rows_read,\n","        \"rows_written\": rows_written,\n","        \"start_time\": start_time,\n","        \"end_time\": end_time,\n","        \"duration_seconds\": duration,\n","        \"error_message\": error_message,\n","        \"parquet_path\": parquet_dir,\n","        \"delta_table\": delta_table_full,\n","    }\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"54ff31ae-70f6-4d65-a03b-e59ffc3e5ce9"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"708925f5-6775-4eaa-abcb-d10995033b8c"}],"default_lakehouse":"708925f5-6775-4eaa-abcb-d10995033b8c","default_lakehouse_name":"lh_gh_bronze","default_lakehouse_workspace_id":"f29eeacf-64cd-431a-913b-2ed71174251e"}}},"nbformat":4,"nbformat_minor":5}