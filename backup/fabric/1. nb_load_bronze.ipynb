{"cells":[{"cell_type":"markdown","source":["# Load bronze: Parquet to Delta Table\n","\n","**Purpose:** Load parquet files from greenhouse sources into Delta Tables in the bronze layer\n","\n","**Versie:** 1.0  \n","**Author:** Data Engineering Team  \n","**Last Updated:** 2025-10-27"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"87f7fa0c-63d2-4032-b49c-181f101dc0be"},{"cell_type":"markdown","source":["## Notebook Overview\n","\n","This notebook processes parquet files from the greenhouse source layer and loads them into Delta Tables in the bronze layer.\n","\n","**Key Features:**\n","- Parses DAG JSON configuration for table metadata\n","- Supports three load modes: snapshot, window, and incremental\n","- Automatic logging to `logs.bronze_processing_log` Delta Table\n","- Retry capability for failed tables\n","- Preserves Spark session for downstream processing\n","\n","**Input Parameters:**\n","- `source`: Source system name (e.g., \"anva_concern\")\n","- `run_ts`: Run timestamp identifier (e.g., \"20250923T183119772\")\n","- `dag_path`: Path to DAG JSON configuration file\n","- `retry_tables`: Optional list of specific tables to retry (default: process all)\n","\n","**Output:**\n","- Delta Tables in format: `{source}.{table_name}`\n","- Processing logs in: `logs.bronze_processing_log`"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"21c401d3-3e6e-4e0a-9212-0cdc8182210c"},{"cell_type":"markdown","source":["## Step 1: Setup Logging Infrastructure\n","\n","Create or verify the existence of the bronze processing log table. This table tracks all processing activities including success/failure status, row counts, and error messages.\n","\n","**Log Table Schema:**\n","- Captures run metadata (source, table, run_ts, load_mode)\n","- Records processing metrics (rows read/written, duration)\n","- Stores error details for troubleshooting"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"f113d925-2f52-4d44-a622-7c4f5a6316d2"},{"cell_type":"code","source":["from pyspark.sql.types import StructType, StructField, StringType, TimestampType, LongType, BooleanType\n","from pyspark.sql import functions as F\n","from datetime import datetime\n","\n","# Define logging table schema\n","log_schema = StructType([\n","    StructField(\"log_id\", StringType(), False),\n","    StructField(\"run_ts\", StringType(), False),\n","    StructField(\"source\", StringType(), False),\n","    StructField(\"table_name\", StringType(), False),\n","    StructField(\"load_mode\", StringType(), True),\n","    StructField(\"status\", StringType(), False),  # SUCCESS, FAILED, RUNNING\n","    StructField(\"rows_read\", LongType(), True),\n","    StructField(\"rows_written\", LongType(), True),\n","    StructField(\"start_time\", TimestampType(), False),\n","    StructField(\"end_time\", TimestampType(), True),\n","    StructField(\"duration_seconds\", LongType(), True),\n","    StructField(\"error_message\", StringType(), True),\n","    StructField(\"parquet_path\", StringType(), True),\n","    StructField(\"delta_table\", StringType(), True)\n","])\n","\n","# Ensure logs schema exists\n","print(\"Checking logs schema...\")\n","spark.sql(\"CREATE SCHEMA IF NOT EXISTS logs\")\n","print(\"Schema 'logs' verified\")\n","\n","# Check if log table exists\n","log_table_name = \"logs.bronze_processing_log\"\n","\n","try:\n","    existing_log = spark.table(log_table_name)\n","    print(f\"Log table exists: {log_table_name}\")\n","    print(f\"Total log records: {existing_log.count():,}\")\n","except:\n","    print(f\"Creating log table: {log_table_name}\")\n","    \n","    # Create empty DataFrame with schema\n","    empty_log = spark.createDataFrame([], log_schema) \\\n","        .withColumn(\"run_date\", F.lit(None).cast(\"date\"))\n","\n","    # Write to Delta\n","    (empty_log.write\n","        .format(\"delta\")\n","        .partitionBy(\"run_date\", \"table_name\")\n","        .mode(\"overwrite\")\n","        .saveAsTable(log_table_name)\n","    )\n","    \n","    print(f\"Log table created: {log_table_name}\")\n","\n","print(f\"Logging infrastructure ready\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":105,"statement_ids":[105],"state":"finished","livy_statement_state":"available","session_id":"7f1f5177-7640-4a28-88d7-3afc1e7d766e","normalized_state":"finished","queued_time":"2025-10-27T22:05:23.4540601Z","session_start_time":null,"execution_start_time":"2025-10-27T22:05:23.4552937Z","execution_finish_time":"2025-10-27T22:05:29.3006387Z","parent_msg_id":"85e69bb4-9fd3-468a-ba08-930c3daa91e9"},"text/plain":"StatementMeta(, 7f1f5177-7640-4a28-88d7-3afc1e7d766e, 105, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["‚öôÔ∏è  Checking logs schema...\n‚úì Schema 'logs' verified\n‚úì Log table exists: logs.bronze_processing_log\n  Total log records: 1,459\n\n‚úì Logging infrastructure ready\n"]}],"execution_count":103,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b70444c5-f234-4ad1-b037-7c4e478346e2"},{"cell_type":"markdown","source":["## Step 1b: Configure Spark for Ancient Dates\n","\n","Set Spark configuration to handle dates before 1582-10-15 correctly. This prevents calendar conversion issues when writing to Delta Tables."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"617d8bbb-e3cd-41b8-89a7-882e5a68ce9b"},{"cell_type":"code","source":["# Configure Spark to handle ancient dates\n","spark.conf.set(\"spark.sql.parquet.datetimeRebaseModeInWrite\", \"CORRECTED\")\n","spark.conf.set(\"spark.sql.parquet.int96RebaseModeInWrite\", \"CORRECTED\")\n","spark.conf.set(\"spark.sql.caseSensitive\", \"true\")\n","# Configure aggressive vacuum for bronze (no time travel needed)\n","spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\", \"false\")\n","\n","debug = True\n","print(\"Spark configuration set\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":106,"statement_ids":[106],"state":"finished","livy_statement_state":"available","session_id":"7f1f5177-7640-4a28-88d7-3afc1e7d766e","normalized_state":"finished","queued_time":"2025-10-27T22:05:23.5836835Z","session_start_time":null,"execution_start_time":"2025-10-27T22:05:29.3025481Z","execution_finish_time":"2025-10-27T22:05:29.6979422Z","parent_msg_id":"88e24d2e-3ed1-4d97-b0e5-ac866169b0c9"},"text/plain":"StatementMeta(, 7f1f5177-7640-4a28-88d7-3afc1e7d766e, 106, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["‚úì Spark configuration set\n"]}],"execution_count":104,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"3792c326-8bbb-4610-b9c5-1ea065ec2630"},{"cell_type":"markdown","source":["## Step 2: Parse Input Parameters and DAG Configuration\n","\n","Read the DAG JSON file to extract table configurations including load modes, queries, and metadata. The DAG structure contains all necessary information for processing each table.\n","\n","**Expected DAG Structure:**\n","- `source`: Source system name\n","- `run_ts`: Run timestamp\n","- `run_id`: Run identifier (e.g., \"run_20250923T183119772\")\n","- `tables[]`: Array of table configurations with load_mode, name, etc."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"2c06d9ee-4bda-454b-b41a-3f5e1f677d1f"},{"cell_type":"code","source":["if debug:\n","    # Enable Spark UI metrics\n","    spark.sparkContext.setLogLevel(\"WARN\")\n","\n","    # Track start time\n","    import time\n","    pipeline_start = time.time()\n","\n","    print(f\"Spark Configuration:\")\n","    print(f\"Executors: {spark.sparkContext._conf.get('spark.executor.instances', 'default')}\")\n","    print(f\"Executor memory: {spark.sparkContext._conf.get('spark.executor.memory', 'default')}\")\n","    print(f\"Driver memory: {spark.sparkContext._conf.get('spark.driver.memory', 'default')}\")\n","    print(f\"Cores per executor: {spark.sparkContext._conf.get('spark.executor.cores', 'default')}\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ef454960-35bd-49f9-b86e-fbc16a55b715"},{"cell_type":"code","source":["import json\n","from typing import Dict, List, Optional\n","from datetime import datetime\n","import os\n","\n","# ============================================================================\n","# INPUT PARAMETERS - Set these when running the notebook\n","# ============================================================================\n","# source = \"anva_meeus\"  # Source system name\n","# run_ts = \"20250923T183119772\"  # Run timestamp\n","# dag_path = \"/lakehouse/default/Files/config/dag_anva_meeus_week.json\"  # Path to DAG JSON\n","# retry_tables = None  # Optional: ['Dim_Agent', 'Fact_Polissen'] or None for all tables\n","# drop_existing_tables = False  # Set to True to drop and recreate all tables (fixes case issues)\n","\n","# Convert string to boolean (parameters come as strings from pipeline)\n","drop_existing_tables = drop_existing_tables.lower() == \"true\" if isinstance(drop_existing_tables, str) else drop_existing_tables\n","\n","# Convert retry_tables from string to list if needed\n","if isinstance(retry_tables, str) and retry_tables and retry_tables != \"None\":\n","    retry_tables = retry_tables.split(\",\")\n","elif retry_tables == \"None\" or not retry_tables:\n","    retry_tables = None\n","\n","print(f\"Parsing DAG configuration...\")\n","print(f\"Source: {source}\")\n","print(f\"Run TS: {run_ts}\")\n","print(f\"DAG Path: {dag_path}\")\n","print(f\"Drop existing: {drop_existing_tables}\")\n","print(f\"Retry tables: {retry_tables}\")\n","\n","# ============================================================================\n","# Parse DAG JSON\n","# ============================================================================\n","# Read DAG JSON file\n","with open(dag_path, 'r') as f:\n","    dag_config = json.load(f)\n","\n","# Verify source matches\n","if dag_config['source'] != source:\n","    print(f\"Warning: DAG source ({dag_config['source']}) doesn't match input source ({source})\")\n","\n","# Extract table configurations\n","all_tables = dag_config['tables']\n","print(f\"DAG parsed successfully\")\n","print(f\"Total tables in DAG: {len(all_tables)}\")\n","\n","# Filter tables if retry_tables is specified\n","if retry_tables:\n","    tables_to_process = [t for t in all_tables if t['name'] in retry_tables]\n","    print(f\"Retry mode: Processing {len(tables_to_process)} specific tables\")\n","else:\n","    tables_to_process = [t for t in all_tables if t.get('enabled', True)]\n","    print(f\"Processing {len(tables_to_process)} enabled tables\")\n","\n","# Display table summary\n","print(f\"Tables to process:\")\n","load_mode_counts = {}\n","for table in tables_to_process:\n","    load_mode = table.get('load_mode', 'unknown')\n","    load_mode_counts[load_mode] = load_mode_counts.get(load_mode, 0) + 1\n","\n","for mode, count in load_mode_counts.items():\n","    print(f\"  - {mode}: {count} tables\")\n","\n","# Construct base parquet path\n","base_files = dag_config.get('base_files', 'greenhouse_sources')\n","run_id = f\"{run_ts}\"\n","\n","# Extract date from run_ts (format: 20250923T183119772)\n","year = run_ts[:4]\n","month = run_ts[4:6]\n","day = run_ts[6:8]\n","\n","base_parquet_path = f\"/lakehouse/default/Files/{base_files}/{source}/{year}/{month}/{day}/{run_id}\"\n","print(f\"Base parquet path: {base_parquet_path}\")\n","\n","# Verify parquet path exists - STOP if not found\n","if not os.path.exists(base_parquet_path):\n","    error_msg = f\"Parquet path does not exist: {base_parquet_path}\"\n","    print(f\"{error_msg}\")\n","    raise FileNotFoundError(error_msg)\n","\n","# Verify table folders exist\n","table_folders = [d for d in os.listdir(base_parquet_path) if os.path.isdir(os.path.join(base_parquet_path, d))]\n","\n","if len(table_folders) == 0:\n","    error_msg = f\"No table folders found in: {base_parquet_path}\"\n","    print(f\"{error_msg}\")\n","    raise FileNotFoundError(error_msg)\n","\n","print(f\"Found {len(table_folders)} table folders\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":107,"statement_ids":[107],"state":"finished","livy_statement_state":"available","session_id":"7f1f5177-7640-4a28-88d7-3afc1e7d766e","normalized_state":"finished","queued_time":"2025-10-27T22:05:23.784678Z","session_start_time":null,"execution_start_time":"2025-10-27T22:05:29.6999397Z","execution_finish_time":"2025-10-27T22:05:30.0974089Z","parent_msg_id":"650ef9b6-bc98-4b0e-8277-16b8b9b7f3cc"},"text/plain":"StatementMeta(, 7f1f5177-7640-4a28-88d7-3afc1e7d766e, 107, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["üìñ Parsing DAG configuration...\n  Source: anva_meeus\n  Run TS: 20250923T183119772\n  DAG Path: /lakehouse/default/Files/config/dag_anva_meeus_week.json\n  Drop existing: False\n\n‚úì DAG parsed successfully\n  Total tables in DAG: 74\n  Processing 58 enabled tables\n\nüìã Tables to process:\n  - snapshot: 57 tables\n  - window: 1 tables\n\nüìÇ Base parquet path:\n  /lakehouse/default/Files/greenhouse_sources/anva_meeus/2025/09/23/run_20250923T183119772\n  ‚úì Path exists\n  ‚úì Found 58 table folders\n"]}],"execution_count":105,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"0bf8d796-f64d-4104-9117-8aaeac72e9b8"},{"cell_type":"markdown","source":["## Step 3: Define Helper Functions\n","\n","Create reusable functions for:\n","- **Logging**: Track processing status (start, success, failure)\n","- **Load Operations**: Handle different load modes (snapshot, window, incremental)\n","- **Metrics**: Calculate row counts and processing duration"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"760dc67c-afe0-4ffb-ac47-4372e641eca9"},{"cell_type":"code","source":["import uuid\n","from datetime import datetime\n","from pyspark.sql.utils import AnalysisException\n","from delta.tables import DeltaTable\n","from pyspark.sql import functions as F\n","import time\n","from delta.exceptions import ConcurrentAppendException, DeltaConcurrentModificationException\n","\n","# ============================================================================\n","# HELPER FUNCTIONS\n","# ============================================================================\n","\n","# def log_table_processing(log_id: str, run_ts: str, source: str, table_name: str, \n","#                          status: str, load_mode: str = None, rows_read: int = None, \n","#                          rows_written: int = None, start_time: datetime = None, \n","#                          end_time: datetime = None, error_message: str = None,\n","#                          parquet_path: str = None, delta_table: str = None):\n","#     \"\"\"Write a log entry to the bronze processing log table\"\"\"\n","#     duration_seconds = None\n","#     if start_time and end_time:\n","#         duration_seconds = int((end_time - start_time).total_seconds())\n","    \n","#     log_data = [(\n","#         log_id, run_ts, source, table_name, load_mode, status,\n","#         rows_read, rows_written, start_time, end_time, duration_seconds,\n","#         error_message, parquet_path, delta_table\n","#     )]\n","    \n","#     log_df = spark.createDataFrame(log_data, schema=log_schema)\n","#     log_df.write.format(\"delta\").mode(\"append\").saveAsTable(log_table_name)\n","\n","def log_table_processing(log_id: str, run_ts: str, source: str, table_name: str, \n","                         status: str, load_mode: str = None, rows_read: int = None, \n","                         rows_written: int = None, start_time: datetime = None, \n","                         end_time: datetime = None, error_message: str = None,\n","                         parquet_path: str = None, delta_table: str = None):\n","\n","    def merge_with_retry(merge_fn, max_retries=5, base_sleep=0.2):\n","        for i in range(max_retries):\n","            try:\n","                merge_fn()\n","                return\n","            except (ConcurrentAppendException, DeltaConcurrentModificationException) as e:\n","                time.sleep(base_sleep * (2 ** i))\n","        # laatste poging nog √©√©n keer laten falen\n","        merge_fn()\n","\n","    # duur berekenen\n","    duration_seconds = None\n","    if start_time and end_time:\n","        duration_seconds = int((end_time - start_time).total_seconds())\n","\n","    # enkele rij als DataFrame\n","    log_df = spark.createDataFrame(\n","        [(\n","            log_id, run_ts, source, table_name, load_mode, status,\n","            rows_read, rows_written, start_time, end_time, duration_seconds,\n","            error_message, parquet_path, delta_table\n","        )],\n","        schema=log_schema\n","    )\n","\n","    log_df = log_df.withColumn(\"run_date\", F.to_date(F.col(\"run_ts\").substr(1, 8), \"yyyyMMdd\"))\n","\n","    # Delta MERGE: update dezelfde rij op basis van sleutel (hier: log_id)\n","    dt = DeltaTable.forName(spark, log_table_name)\n","\n","    # COALESCE op matched update: laat bestaande waarden staan als nieuwe None zijn\n","    set_expr = {\n","        \"run_ts\":          F.col(\"s.run_ts\"),\n","        \"source\":          F.coalesce(F.col(\"s.source\"), F.col(\"t.source\")),\n","        \"table_name\":      F.coalesce(F.col(\"s.table_name\"), F.col(\"t.table_name\")),\n","        \"load_mode\":       F.coalesce(F.col(\"s.load_mode\"), F.col(\"t.load_mode\")),\n","        \"status\":          F.col(\"s.status\"),\n","        \"rows_read\":       F.coalesce(F.col(\"s.rows_read\"), F.col(\"t.rows_read\")),\n","        \"rows_written\":    F.coalesce(F.col(\"s.rows_written\"), F.col(\"t.rows_written\")),\n","        \"start_time\":      F.coalesce(F.col(\"s.start_time\"), F.col(\"t.start_time\")),\n","        \"end_time\":        F.coalesce(F.col(\"s.end_time\"), F.col(\"t.end_time\")),\n","        \"duration_seconds\":F.coalesce(F.col(\"s.duration_seconds\"), F.col(\"t.duration_seconds\")),\n","        \"error_message\":   F.coalesce(F.col(\"s.error_message\"), F.col(\"t.error_message\")),\n","        \"parquet_path\":    F.coalesce(F.col(\"s.parquet_path\"), F.col(\"t.parquet_path\")),\n","        \"delta_table\":     F.coalesce(F.col(\"s.delta_table\"), F.col(\"t.delta_table\")),\n","        \"run_date\":        F.col(\"s.run_date\")\n","    }\n","\n","    insert_vals = { c: F.col(f\"s.{c}\") for c in log_df.columns }\n","\n","    # (dt.alias(\"t\")\n","    #   .merge(log_df.alias(\"s\"), \"t.log_id = s.log_id AND t.table_name = s.table_name AND t.run_date = s.run_date\")           # <‚Äî jouw sleutel\n","    #   .whenMatchedUpdate(set=set_expr)                           # update dezelfde rij\n","    #   .whenNotMatchedInsert(values=insert_vals)                  # of maak ‚Äòm aan\n","    #   .execute())\n","    merge_with_retry(lambda: (\n","        dt.alias(\"t\")\n","        .merge(log_df.alias(\"s\"), \"t.log_id=s.log_id AND t.table_name=s.table_name AND t.run_date=s.run_date\")\n","        .whenMatchedUpdate(set=set_expr)\n","        .whenNotMatchedInsert(values=insert_vals)\n","        .execute()\n","    ))\n","\n","\n","def load_parquet_to_delta(table_config: dict, parquet_base_path: str, source: str, run_ts: str, drop_existing: bool = False) -> dict:\n","    \"\"\"\n","    Load parquet to Delta using file:// protocol\n","    \"\"\"\n","    table_name = table_config['name']\n","    load_mode = table_config.get('load_mode', 'snapshot')\n","    target_table = table_config.get('delta_table', table_name)\n","    delta_table_name = f\"`{source}`.`{target_table}`\"\n","    \n","    table_folder = table_name.replace('.', '_')\n","    parquet_path = f\"{parquet_base_path}/{table_folder}\"\n","    \n","    log_id = str(uuid.uuid4())\n","    start_time = datetime.now()\n","    \n","    result = {\n","        'status': 'SUCCESS', 'rows_read': 0, 'rows_written': 0,\n","        'error_message': None, 'log_id': log_id, 'start_time': start_time,\n","        'parquet_path': parquet_path, 'delta_table': f\"{source}.{target_table}\"\n","    }\n","    \n","    try:\n","        log_table_processing(\n","            log_id=log_id, run_ts=run_ts, source=source, table_name=table_name,\n","            status='RUNNING', load_mode=load_mode, start_time=start_time,\n","            parquet_path=parquet_path, delta_table=f\"{source}.{target_table}\"\n","        )\n","        \n","        spark.sql(f\"CREATE SCHEMA IF NOT EXISTS `{source}`\")\n","        \n","        if drop_existing:\n","            try:\n","                spark.sql(f\"DROP TABLE IF EXISTS {delta_table_name}\")\n","                spark.sql(f\"DROP TABLE IF EXISTS `{source}`.`{target_table.lower()}`\")\n","            except:\n","                pass\n","        \n","        # Use file:// protocol - bypasses OneLake connector issues\n","        file_protocol_path = f\"file://{parquet_path}\"\n","        df = spark.read.parquet(file_protocol_path)\n","        rows_read = df.count()\n","        result['rows_read'] = rows_read\n","        \n","        if rows_read == 0:\n","            result['error_message'] = \"Empty table - schema only\"\n","        \n","        # Write to Delta\n","        if load_mode in ['snapshot', 'window']:\n","            df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(delta_table_name)\n","        elif load_mode == 'incremental':\n","            table_exists = False\n","    \n","            try:\n","                # Check both: exists in catalog AND has readable data\n","                if spark.catalog.tableExists(f\"{source}.{target_table}\"):\n","                    # Try to read - if it fails, table is corrupt\n","                    test_df = spark.table(delta_table_name).limit(1)\n","                    test_df.count()  # Force evaluation\n","                    table_exists = True\n","                    print(f\"  ‚ÑπÔ∏è  Incremental append to existing {target_table}\")\n","            except Exception as check_error:\n","                # Table exists but is corrupt/unreadable\n","                print(f\"Table {target_table} exists but is corrupt - recreating\")\n","                print(f\"Error: {str(check_error)[:100]}\")\n","                \n","                # Drop corrupt table\n","                try:\n","                    spark.sql(f\"DROP TABLE IF EXISTS {delta_table_name}\")\n","                except:\n","                    pass\n","                \n","                table_exists = False\n","\n","            if not table_exists:\n","                # First load or recreate: create table with overwrite\n","                print(f\"Creating new incremental table {target_table}\")\n","                df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(delta_table_name)\n","            else:\n","                # Subsequent loads: append\n","                df.write.format(\"delta\").mode(\"append\").saveAsTable(delta_table_name)\n","        else:\n","            raise ValueError(f\"Unknown load_mode: {load_mode}\")\n","        \n","        result['rows_written'] = rows_read\n","        result['status'] = 'SUCCESS'\n","        \n","        # Vacuum immediately\n","        # try:\n","        #     spark.sql(f\"VACUUM {delta_table_name} RETAIN 0 HOURS\")\n","        # except AnalysisException as e:\n","        #     if \"not found\" not in str(e).lower():\n","        #         print(f\"Vacuum warning for {target_table}: {str(e)[:100]}\")\n","        \n","    except Exception as e:\n","        result['status'] = 'FAILED'\n","        import traceback\n","        error_type = type(e).__name__\n","        error_msg = str(e)\n","        result['error_message'] = f\"[{error_type}] {error_msg[:400]}\"\n","    \n","    end_time = datetime.now()\n","    log_table_processing(\n","        log_id=log_id, run_ts=run_ts, source=source, table_name=table_name,\n","        status=result['status'], load_mode=load_mode,\n","        rows_read=result['rows_read'], rows_written=result['rows_written'],\n","        start_time=start_time, end_time=end_time,\n","        error_message=result['error_message'],\n","        parquet_path=parquet_path, delta_table=f\"{source}.{target_table}\"\n","    )\n","    \n","    return result\n","\n","    Files/greenhouse_sources/anva_meeus/2025/10/05/20251005T142752505/Fact_PremieFacturen/sql_query_Boek_Datum_11042024120000_12052024000000_01_00000.parquet\n","\n","\n","print(\"Helper functions defined.\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":108,"statement_ids":[108],"state":"finished","livy_statement_state":"available","session_id":"7f1f5177-7640-4a28-88d7-3afc1e7d766e","normalized_state":"finished","queued_time":"2025-10-27T22:05:23.916969Z","session_start_time":null,"execution_start_time":"2025-10-27T22:05:30.0998353Z","execution_finish_time":"2025-10-27T22:05:30.4641335Z","parent_msg_id":"9449bfbc-4ed9-4890-92de-b46d9b7141c6"},"text/plain":"StatementMeta(, 7f1f5177-7640-4a28-88d7-3afc1e7d766e, 108, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["‚úì Helper functions defined (with drop_existing support)\n"]}],"execution_count":106,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"597f8ad7-13cd-4f2c-aefb-3821852a0683"},{"cell_type":"markdown","source":["## Step 4: Process Tables - Parquet to Delta\n","\n","Iterate through all configured tables and load them into Delta Tables based on their load mode:\n","\n","**Load Modes:**\n","- **Snapshot**: Full table overwrite with schema evolution\n","- **Window**: Full table overwrite with time-windowed data\n","- **Incremental**: Append new records to existing table\n","\n","Progress is tracked in real-time with success/failure counts."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"7e4c8a06-28ba-40fd-88f8-4f7c697b478c"},{"cell_type":"code","source":["from datetime import datetime\n","from concurrent.futures import ThreadPoolExecutor, as_completed\n","import threading\n","\n","# ============================================================================\n","# PARALLEL PROCESSING CONFIGURATION\n","# ============================================================================\n","\n","max_parallel_workers = 4\n","print(f\"‚öôÔ∏è  Parallel workers: {max_parallel_workers}\")\n","\n","# Thread-safe counters\n","lock = threading.Lock()\n","success_count = 0\n","failed_count = 0\n","empty_count = 0\n","total_rows_processed = 0\n","processed_count = 0\n","\n","def update_counters(result):\n","    \"\"\"Thread-safe counter updates\"\"\"\n","    global success_count, failed_count, empty_count, total_rows_processed, processed_count\n","    with lock:\n","        processed_count += 1\n","        if result['status'] == 'SUCCESS':\n","            success_count += 1\n","            total_rows_processed += result['rows_written']\n","            if result['rows_written'] == 0:\n","                empty_count += 1\n","        else:\n","            failed_count += 1\n","\n","# ============================================================================\n","# MAIN PARALLEL PROCESSING\n","# ============================================================================\n","\n","print(f\"Starting parallel bronze loading process...\")\n","print(f\"Tables to process: {len(tables_to_process)}\")\n","print(f\"Parallel workers: {max_parallel_workers}\")\n","print(f\"Source: {source}\")\n","print(f\"Run TS: {run_ts}\")\n","print(f\"=\" * 80)\n","\n","results = []\n","processing_start = datetime.now()\n","\n","with ThreadPoolExecutor(max_workers=max_parallel_workers) as executor:\n","    future_to_table = {\n","        executor.submit(load_parquet_to_delta, table_config, base_parquet_path, source, run_ts, drop_existing_tables): table_config\n","        for table_config in tables_to_process\n","    }\n","    \n","    for future in as_completed(future_to_table):\n","        table_config = future_to_table[future]\n","        table_name = table_config['name']\n","        load_mode = table_config.get('load_mode', 'snapshot')\n","        \n","        try:\n","            result = future.result()\n","            results.append({\n","                'table_name': table_name,\n","                'load_mode': load_mode,\n","                **result\n","            })\n","            \n","            update_counters(result)\n","            \n","            with lock:\n","                current = processed_count\n","            \n","            if result['status'] == 'SUCCESS':\n","                duration = (result.get('end_time', datetime.now()) - result['start_time']).total_seconds()\n","                if result['rows_written'] == 0:\n","                    print(f\"[{current}/{len(tables_to_process)}] ‚óã {table_name}: Empty table (schema only) in {duration:.1f}s\")\n","                else:\n","                    print(f\"[{current}/{len(tables_to_process)}] ‚úì {table_name}: {result['rows_written']:,} rows in {duration:.1f}s\")\n","            else:\n","                print(f\"[{current}/{len(tables_to_process)}] ‚úó {table_name}: {result['error_message']}\")\n","                \n","        except Exception as e:\n","            print(f\"[?/{len(tables_to_process)}] ‚úó {table_name}: Unexpected error: {str(e)}\")\n","            results.append({\n","                'table_name': table_name,\n","                'load_mode': load_mode,\n","                'status': 'FAILED',\n","                'error_message': str(e)\n","            })\n","            with lock:\n","                failed_count += 1\n","                processed_count += 1\n","\n","processing_end = datetime.now()\n","total_duration = (processing_end - processing_start).total_seconds()\n","\n","# ============================================================================\n","# SUMMARY REPORT\n","# ============================================================================\n","\n","print(f\"\\n\" + \"=\" * 80)\n","print(f\"üèÅ Processing Complete!\")\n","print(f\"=\" * 80)\n","print(f\"  Total tables: {len(tables_to_process)}\")\n","print(f\"  ‚úì Success: {success_count}\")\n","print(f\"    - With data: {success_count - empty_count}\")\n","print(f\"    - Empty (schema only): {empty_count}\")\n","print(f\"  ‚úó Failed: {failed_count}\")\n","print(f\"  Total rows: {total_rows_processed:,}\")\n","print(f\"  Duration: {total_duration:.1f} seconds ({total_duration/60:.1f} minutes)\")\n","if total_rows_processed > 0:\n","    print(f\"  Throughput: {total_rows_processed/total_duration:,.0f} rows/second\")\n","print(f\"  Average: {total_duration/len(tables_to_process):.1f} seconds/table\")\n","\n","# Show failed tables if any\n","if failed_count > 0:\n","    print(f\"\\n‚ö†Ô∏è  Failed tables ({failed_count}):\")\n","    failed_tables_list = []\n","    for result in results:\n","        if result['status'] == 'FAILED':\n","            print(f\"  - {result['table_name']}: {result.get('error_message', 'Unknown error')[:80]}\")\n","            failed_tables_list.append(result['table_name'])\n","    \n","    print(f\"\\nüí° To retry failed tables, run with:\")\n","    print(f\"  retry_tables = {failed_tables_list}\")\n","\n","print(f\"\\n‚úì All logs saved to: {log_table_name}\")\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":109,"statement_ids":[109],"state":"submitted","livy_statement_state":"running","session_id":"7f1f5177-7640-4a28-88d7-3afc1e7d766e","normalized_state":"running","queued_time":"2025-10-27T22:05:24.024Z","session_start_time":null,"execution_start_time":"2025-10-27T22:05:30.466325Z","execution_finish_time":null,"parent_msg_id":"2dc1a0a3-e2ed-44fb-89fe-e950539ac352"},"text/plain":"StatementMeta(, 7f1f5177-7640-4a28-88d7-3afc1e7d766e, 109, Submitted, Running, Running)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["‚öôÔ∏è  Parallel workers: 10\n\nüöÄ Starting parallel bronze loading process...\n  Tables to process: 58\n  Parallel workers: 10\n  Source: anva_meeus\n  Run TS: 20250923T183119772\n================================================================================\n[1/58] ‚úì Dim_Agent: 3,193 rows in 104.2s\n[2/58] ‚úì Dim_Branche: 617 rows in 104.6s\n[3/58] ‚úì Dim_DekkingCode: 2,575 rows in 107.7s\n[4/58] ‚úì Dim_FactuurSoort: 24 rows in 110.2s\n[5/58] ‚úì Dim_HoofdBranche: 23 rows in 111.8s\n[6/58] ‚úì Dim_Incassowijze: 168 rows in 117.5s\n[7/58] ‚úì Dim_Calamiteit: 14 rows in 119.1s\n[8/58] ‚úì Dim_DetailMaatschappij: 2,289 rows in 120.8s\n[9/58] ‚úì Dim_DekkingVariabel: 93,997 rows in 122.6s\n[10/58] ‚úì Dim_Collectiviteit: 10,212 rows in 123.9s\n[11/58] ‚úì Dim_Kantoor: 3 rows in 83.9s\n[12/58] ‚úì Dim_Medewerker: 9,861 rows in 81.0s\n[13/58] ‚úì Dim_Maatschappij: 2,289 rows in 85.1s\n[14/58] ‚óã Dim_PolisVariabel_VrijeLabels: Empty table (schema only) in 75.9s\n[15/58] ‚úì Dim_MeldingRDW: 7 rows in 107.0s\n[16/58] ‚úì Dim_PolisProducent: 1,473 rows in 107.4s\n[17/58] ‚úì Dim_Polisvoorwaarden: 8,792 rows in 109.5s\n[18/58] ‚óã Dim_RelatieVariabel_VrijeLabels: Empty table (schema only) in 41.5s\n[19/58] ‚úì Dim_RelatieProducent: 1,473 rows in 108.7s\n[20/58] ‚úì Dim_RelatieSoort: 16 rows in 54.8s\n[21/58] ‚úì Dim_SBICode: 956 rows in 58.8s\n[22/58] ‚úì Dim_Schade: 3,649 rows in 63.7s\n[23/58] ‚úì Dim_PolisVariabel: 480,490 rows in 167.0s\n[24/58] ‚úì Dim_SchadeCode: 17 rows in 67.8s\n[25/58] ‚úì Dim_RelatieVariabel: 554,494 rows in 101.8s\n[26/58] ‚úì Dim_SchadeSoort: 609 rows in 66.5s\n[27/58] ‚úì Dim_Relatie: 554,495 rows in 176.4s\n[28/58] ‚úì Dim_Verzekeringsvorm: 982 rows in 67.0s\n[29/58] ‚úì Dim_Tekenjaar: 27 rows in 74.0s\n"]}],"execution_count":107,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"cfa170fb-43f2-46b9-89e2-b8fcf959fc0f"},{"cell_type":"code","source":["# Return structured result for master notebook\n","result_payload = {\n","    \"success_count\": success_count,\n","    \"failed_count\": failed_count,\n","    \"total_rows\": total_rows_processed,\n","    \"duration_seconds\": total_duration,\n","    \"failed_tables\": [r['table_name'] for r in results if r['status'] == 'FAILED']\n","}\n","\n","print(f\"Result payload:\")\n","print(json.dumps(result_payload, indent=2))\n","\n","# Exit with payload for orchestration\n","mssparkutils.notebook.exit(json.dumps(result_payload))"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"2011f208-c2fa-4416-ab06-3cf3fc7e0bc3"},{"cell_type":"code","source":["# # Check Delta versies van een tabel\n","# table_name = \"anva_meeus.Dim_Agent\"\n","\n","# print(f\"üîç Delta Table History for {table_name}\")\n","# print(\"=\" * 80)\n","\n","# # Optie 1: SQL\n","# history_df = spark.sql(f\"DESCRIBE HISTORY {table_name}\")\n","# history_df.select(\"version\", \"timestamp\", \"operation\", \"operationMetrics\").show(truncate=False)\n","\n","# print(f\"\\nüìä Total versions: {history_df.count()}\")\n","\n","# # Optie 2: Python API\n","# from delta.tables import DeltaTable\n","\n","# delta_table = DeltaTable.forName(spark, table_name)\n","# history = delta_table.history()\n","\n","# print(f\"\\nüìã Version details:\")\n","# history.select(\"version\", \"timestamp\", \"operation\", \"operationMetrics.numFiles\", \"operationMetrics.numOutputRows\").show(10, truncate=False)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":-1,"statement_ids":null,"state":"waiting","livy_statement_state":null,"session_id":null,"normalized_state":"waiting","queued_time":"2025-10-27T22:05:24.109249Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":null,"parent_msg_id":"504d2e8d-3590-42c3-9183-58bca63cb08a"},"text/plain":"StatementMeta(, , -1, Waiting, , Waiting)"},"metadata":{}}],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"acea0a33-b409-4398-a2e5-0edab3f0ffe4"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"708925f5-6775-4eaa-abcb-d10995033b8c"}],"default_lakehouse":"708925f5-6775-4eaa-abcb-d10995033b8c","default_lakehouse_name":"lh_gh_bronze","default_lakehouse_workspace_id":"f29eeacf-64cd-431a-913b-2ed71174251e"}}},"nbformat":4,"nbformat_minor":5}