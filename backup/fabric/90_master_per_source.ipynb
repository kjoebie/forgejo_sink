{"cells":[{"cell_type":"markdown","source":["# 90 – Master per source (bronze load orchestrator)\n","\n","This notebook orchestrates the full bronze load for **one source** and **one run_ts**.\n","\n","Responsibilities:\n","\n","- Read the DAG.json for the given `SOURCE_NAME`.\n","- Determine `base_files` and `bronze_schema`.\n","- Sort tables (largest first, based on `size_class`).\n","- Run all tables in parallel using `ThreadPoolExecutor` on the driver.\n","- Collect per-table results (status, timing, rowcounts).\n","- Write **one batch** of log rows into `logs.bronze_processing_log`.\n","- Print a concise summary including a parallelism efficiency metric.\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"2154c02c-926b-4d11-bc98-86d8e84ca820"},{"cell_type":"markdown","source":["## [1] Parameter cell (for pipeline and manual testing)\n","\n","Adjust these values when running manually. In a pipeline, they can be\n","injected by the ForEach / pipeline parameters.\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"05e9ce6f-aad8-4fbd-bea1-e17e9571d21e"},{"cell_type":"code","source":["# [1] Parameters (for pipeline and manual testing)\n","\n","# Name of the source as used in the DAG.json\n","# Run timestamp for this batch, e.g. \"20251005T142752505\"\n","# Path to the DAG.json in OneLake\n","\n","#SOURCE_NAME = \"anva_meeus\"\n","#RUN_TS = \"20251005T142752505\"\n","#DAG_PATH = \"/lakehouse/default/Files/config/dag_anva_meeus_week.json\"\n","\n","#SOURCE_NAME = \"ods_reports\"\n","#RUN_TS = \"20250923T060123389\"\n","#DAG_PATH = \"/lakehouse/default/Files/config/dag_ods_reports_week.json\"\n","\n","SOURCE_NAME = \"insurance_data_im\"\n","RUN_TS = \"20250923T050205479\"\n","DAG_PATH = \"/lakehouse/default/Files/config/dag_insurance_data_im_week.json\"\n","\n","#SOURCE_NAME = \"anva_concern\"\n","#RUN_TS = \"20250923T190122062\"\n","#DAG_PATH = \"/lakehouse/default/Files/config/dag_anva_concern_week.json\"\n","\n","#SOURCE_NAME = \"ccs_level\"\n","#RUN_TS = \"20250923T040214235\"\n","#DAG_PATH = \"/lakehouse/default/Files/config/dag_ccs_level_week.json\"\n","\n","#SOURCE_NAME = \"vizier\"\n","#RUN_TS = \"20250923T050205479\"\n","#DAG_PATH = \"/lakehouse/default/Files/config/dag_insurance_data_im_week.json\"\n","\n","#SOURCE_NAME = \"vizier\"\n","#RUN_TS = \"20250923T050205479\"\n","#DAG_PATH = \"/lakehouse/default/Files/config/dag_insurance_data_im_week.json\"\n","\n","\n","# Enable extra debug logging\n","DEBUG = False\n","\n","# Hoeveel historische runs meenemen (1..5)\n","HISTORY_LOOKBACK_RUNS = 3\n","\n","# Min/max workers voor deze omgeving\n","MIN_WORKERS = 2\n","MAX_WORKERS_CAP = 12\n","\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":-1,"statement_ids":[],"state":"session_error","livy_statement_state":null,"session_id":null,"normalized_state":"session_error","queued_time":"2025-11-04T08:45:32.7357723Z","session_start_time":"2025-11-04T08:45:32.7367829Z","execution_start_time":null,"execution_finish_time":null,"parent_msg_id":"b1d37d74-996b-46b9-a5d9-01d68d9c5743"},"text/plain":"StatementMeta(, , -1, SessionError, , SessionError)"},"metadata":{}},{"output_type":"error","ename":"InvalidHttpRequestToLivy","evalue":"[CapacityLimitExceeded] Unable to complete the action because your organization’s Fabric compute capacity has exceeded its limits. Try again later. HTTP status code: 429.","traceback":["InvalidHttpRequestToLivy: [CapacityLimitExceeded] Unable to complete the action because your organization’s Fabric compute capacity has exceeded its limits. Try again later. HTTP status code: 429."]}],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"tags":["parameters"]},"id":"a0572d38-baa4-41cc-af69-4e1d5d7b5425"},{"cell_type":"markdown","source":["## [2] Import utility notebooks\n","\n","These `%run` statements must be **standalone** in their own cells.\n","They load:\n","\n","- `/01_utils_logging` – logging schema + helpers\n","- `/02_utils_config` – DAG and configuration utilities\n","- `/10_bronze_load` – worker function for a single table\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c939f580-272f-4362-af41-1468a19821de"},{"cell_type":"code","source":["%run \"/01_utils_logging\""],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":-1,"statement_ids":null,"state":"cancelled","livy_statement_state":null,"session_id":null,"normalized_state":"cancelled","queued_time":"2025-11-04T08:45:32.7375733Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":"2025-11-04T08:45:58.1336209Z","parent_msg_id":"c0e653d1-903e-4890-b02e-9fe09ff7d8ec"},"text/plain":"StatementMeta(, , -1, Cancelled, , Cancelled)"},"metadata":{}}],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"e028f244-ead5-4359-aea5-8dd3a98d8bb7"},{"cell_type":"code","source":["%run \"/02_utils_config\""],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":-1,"statement_ids":null,"state":"cancelled","livy_statement_state":null,"session_id":null,"normalized_state":"cancelled","queued_time":"2025-11-04T08:45:32.7390831Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":"2025-11-04T08:45:58.133927Z","parent_msg_id":"21f70f5b-60b0-4afc-9a0a-ba495fc0fe38"},"text/plain":"StatementMeta(, , -1, Cancelled, , Cancelled)"},"metadata":{}}],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"804d19e6-4615-4b18-9f67-1f25265a80b4"},{"cell_type":"code","source":["%run \"/10_bronze_load\""],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":-1,"statement_ids":null,"state":"cancelled","livy_statement_state":null,"session_id":null,"normalized_state":"cancelled","queued_time":"2025-11-04T08:45:32.7404744Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":"2025-11-04T08:45:58.1341999Z","parent_msg_id":"6f19dd1c-47f8-4b36-a023-cf2671c4117a"},"text/plain":"StatementMeta(, , -1, Cancelled, , Cancelled)"},"metadata":{}}],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"55f9c55b-948e-49cf-812b-110150720122"},{"cell_type":"markdown","source":["## [3] Load DAG, validate source and prepare table list\n","\n","This cell:\n","\n","- Validates the parameters.\n","- Reads the DAG.json from OneLake.\n","- Ensures it belongs to the requested `SOURCE_NAME`.\n","- Extracts the list of enabled tables.\n","- Determines `base_files` and `bronze_schema`.\n","- Creates the bronze schema if needed.\n","- Sorts tables by `size_class` (`L` > `M` > `S`) and name.\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"de868726-579f-4870-9151-c83c5c1bca32"},{"cell_type":"code","source":["from datetime import datetime\n","from concurrent.futures import ThreadPoolExecutor, as_completed\n","from pyspark.sql import functions as F\n","import uuid\n","\n","spark.conf.set(\"spark.sql.parquet.datetimeRebaseModeInRead\", \"CORRECTED\")\n","spark.conf.set(\"spark.sql.parquet.int96RebaseModeInRead\", \"CORRECTED\")\n","spark.conf.set(\"spark.sql.parquet.datetimeRebaseModeInWrite\", \"CORRECTED\")\n","spark.conf.set(\"spark.sql.parquet.int96RebaseModeInWrite\", \"CORRECTED\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":-1,"statement_ids":null,"state":"cancelled","livy_statement_state":null,"session_id":null,"normalized_state":"cancelled","queued_time":"2025-11-04T08:45:32.741895Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":"2025-11-04T08:45:58.1344478Z","parent_msg_id":"e6443543-9ff9-40c8-a3dd-0fddb1bf5a1a"},"text/plain":"StatementMeta(, , -1, Cancelled, , Cancelled)"},"metadata":{}}],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"06147deb-c007-422c-b373-efd62d0b890f"},{"cell_type":"code","source":["# [3] Load DAG, validate source and prepare tables\n","\n","if not SOURCE_NAME:\n","    raise ValueError(\"Parameter 'SOURCE_NAME' is required (usually provided by Pipeline ForEach).\")\n","\n","if not RUN_TS:\n","    raise ValueError(\"Parameter 'RUN_TS' is required.\")\n","\n","if not DAG_PATH:\n","    raise ValueError(\"Parameter 'DAG_PATH' is required.\")\n","\n","if DEBUG:\n","    print(f\"[MASTER] source={SOURCE_NAME} run_ts={RUN_TS} dag={DAG_PATH}\")\n","\n","# Read and validate DAG\n","dag = read_dag(DAG_PATH)\n","validate_dag_for_source(dag, SOURCE_NAME)\n","\n","#get tables\n","tables = get_tables_for_source(dag, SOURCE_NAME)\n","\n","# Determine global settings\n","base_files = get_base_files(dag)\n","\n","#raise SystemExit(\"klaar, stop cel\")\n","\n","# Ensure bronze schema exists\n","\n","# 3) Verzamel alle unieke schema's en maak ze aan (VOOR parallel processing!)\n","unique_schemas = set()\n","for t in tables:\n","    schema = t.get(\"delta_schema\")\n","    if schema:\n","        unique_schemas.add(schema)\n","\n","# Maak alle unieke schema's 1x aan\n","for schema in unique_schemas:\n","    spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {schema}\")\n","    if DEBUG:\n","        print(f\"[MASTER] Ensured schema exists: {schema}\")\n","\n","if DEBUG:\n","    print(f\"[MASTER] source={SOURCE_NAME} run_ts={RUN_TS} → {len(tables)} tables\")\n","    print(f\"[MASTER] Unique schemas: {sorted(unique_schemas)}\")\n","\n","# Get enabled tables\n","tables = get_tables_for_source(dag, SOURCE_NAME)\n","\n","if DEBUG:\n","    print(f\"[MASTER] found {len(tables)} enabled tables in DAG\")\n","\n","# 3) Sorteer op historische row counts (grootste eerst!)\n","try:\n","    size_results = spark.sql(f\"\"\"\n","        WITH latest_runs AS (\n","            SELECT \n","                table_name,\n","                rows_written,\n","                ROW_NUMBER() OVER (PARTITION BY table_name ORDER BY run_ts DESC) as rn\n","            FROM logs.bronze_processing_log\n","            WHERE source = '{SOURCE_NAME}'\n","            AND status = 'SUCCESS'\n","            AND rows_written > 0\n","        )\n","\n","        SELECT table_name, rows_written\n","        FROM latest_runs\n","        WHERE rn = 1\n","    \"\"\").collect()\n","    \n","\n","    # Bouw lookup dict\n","    size_map = {row.table_name: row.rows_written for row in size_results}\n","except Exception as e:\n","    size_map = {}\n","\n","def get_size(table_def):\n","    return size_map.get(table_def[\"name\"], 0)\n","\n","# Sorteer: grootste eerst (op basis van historische data!)\n","tables_sorted = sorted(tables, key=get_size, reverse=True)\n","\n","if DEBUG:\n","    print(\"\\nTable processing order (by historical size):\")\n","    total_estimated = 0\n","    for i, t in enumerate(tables_sorted, 1):\n","        size = size_map.get(t[\"name\"], 0)\n","        total_estimated += size\n","        print(f\"  {i:2d}. {t['name']:40s} ~{size:>12,} rows\")\n","    print(f\"\\nEstimated total: {total_estimated:,} rows\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":-1,"statement_ids":null,"state":"cancelled","livy_statement_state":null,"session_id":null,"normalized_state":"cancelled","queued_time":"2025-11-04T08:45:32.7433775Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":"2025-11-04T08:45:58.1346988Z","parent_msg_id":"db174a0f-05cb-418c-a76b-5c906833d1cb"},"text/plain":"StatementMeta(, , -1, Cancelled, , Cancelled)"},"metadata":{}}],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"0268282e-653c-415a-ae3a-8fa78bde05e3"},{"cell_type":"markdown","source":["## [4] Execute bronze loads in parallel\n","\n","This cell:\n","\n","- Starts a `ThreadPoolExecutor` with `MAX_WORKERS`.\n","- Submits one `process_bronze_table(...)` task per table.\n","- Collects results into a list of dicts (`results`).\n","- Prints progress:\n","  - With `DEBUG=True`: more detailed lines.\n","  - With `DEBUG=False`: one concise line per table.\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d124e438-1bbb-45b1-908e-ec54f1aa6af9"},{"cell_type":"code","source":["# [4] Run bronze loads in parallel\n","RUN_ID = uuid.uuid4().hex   # of str(uuid.uuid4())\n","\n","start_all = datetime.utcnow()\n","results = []\n","\n","# Maximum number of tables to process in parallel on the driver\n","# Dynamisch aantal workers bepalen op basis van historie\n","MAX_WORKERS = choose_worker_profile_from_history(\n","    source_name      = SOURCE_NAME,\n","    default_workers  = 8,                 # startpunt als er nog geen historie is\n","    min_workers      = MIN_WORKERS,\n","    max_workers_cap  = MAX_WORKERS_CAP,\n","    lookback_runs    = HISTORY_LOOKBACK_RUNS,\n",")\n","\n","MAX_WORKERS = min(MAX_WORKERS, len(tables_sorted))\n","\n","# print (MAX_WORKERS)\n","# raise SystemExit(\"klaar, stop cel\")\n","\n","if DEBUG:\n","    print(f\"[MASTER] starting ThreadPool, using MAX_WORKERS={MAX_WORKERS} (tables={len(tables_sorted)})\")\n","\n","with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n","    future_to_table = {\n","        executor.submit(\n","            process_bronze_table,\n","            t,\n","            SOURCE_NAME,\n","            RUN_TS,\n","            base_files,\n","            DEBUG\n","        ): t\n","        for t in tables_sorted\n","    }\n","\n","    for future in as_completed(future_to_table):\n","        t = future_to_table[future]\n","        t_name = t.get(\"name\")\n","        try:\n","            result = future.result()\n","        except Exception as e:\n","            # This should be rare; process_bronze_table normally catches its own errors.\n","            end_time = datetime.utcnow()\n","            result = {\n","                \"log_id\": f\"{SOURCE_NAME}:{t_name}:{RUN_TS}:ERROR\",\n","                \"run_id\": RUN_ID,\n","                \"run_ts\": RUN_TS,\n","                \"source\": SOURCE_NAME,\n","                \"table_name\": t_name,\n","                \"load_mode\": t.get(\"load_mode\"),\n","                \"status\": \"FAILED\",\n","                \"rows_read\": None,\n","                \"rows_written\": None,\n","                \"start_time\": start_all,\n","                \"end_time\": end_time,\n","                \"duration_seconds\": int((end_time - start_all).total_seconds()),\n","                \"error_message\": f\"Unexpected error in master: {str(e)}\",\n","                \"parquet_path\": None,\n","                \"delta_table\": t.get(\"delta_table\"),\n","            }\n","        results.append(result)\n","\n","        if DEBUG:\n","            print(f\"[MASTER] {result['table_name']} → {result['status']} (read:{result['rows_read']} written:{result['rows_written']} {result['duration_seconds']}s)\")\n","        else:\n","            # Minimal but still visible progress\n","            print(f\"[{result['table_name']}] {result['status']} read:{result['rows_read']} written:{result['rows_written']} ({result['duration_seconds']}s)\")\n","\n","end_all = datetime.utcnow()\n","total_duration = (end_all - start_all).total_seconds()\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":-1,"statement_ids":null,"state":"cancelled","livy_statement_state":null,"session_id":null,"normalized_state":"cancelled","queued_time":"2025-11-04T08:45:32.7447893Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":"2025-11-04T08:45:58.1349765Z","parent_msg_id":"76478d7b-7bd3-4359-b86b-2e5e04778d5f"},"text/plain":"StatementMeta(, , -1, Cancelled, , Cancelled)"},"metadata":{}}],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"fe44a511-eadc-4d0c-9372-fb275c8003e8"},{"cell_type":"markdown","source":["## [5] Batch logging and run summary\n","\n","This final cell:\n","\n","- Writes **all** per-table log records in a single batch using\n","  `log_table_processing_batch(results)`.\n","- Computes and prints:\n","  - counts of SUCCESS / EMPTY / FAILED / SKIPPED,\n","  - total rows,\n","  - total runtime,\n","  - theoretical minimum runtime (sum of per-table durations / workers),\n","  - parallelism efficiency = theoretical_min / actual_time.\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4e2937a3-a071-46c8-9c40-fc3c28cbdd39"},{"cell_type":"code","source":["# [5] Batch logging and run summary\n","\n","# 5a) Write all log records in a single append\n","log_table_processing_batch(results)\n","\n","# 5b) Compute summary stats from the in-memory results\n","total_tables = len(results)\n","success_tables = [r for r in results if r[\"status\"] == \"SUCCESS\"]\n","empty_tables   = [r for r in results if r[\"status\"] == \"EMPTY\"]\n","failed_tables  = [r for r in results if r[\"status\"] == \"FAILED\"]\n","skipped_tables = [r for r in results if r[\"status\"] == \"SKIPPED\"]\n","\n","total_success = len(success_tables)\n","total_empty   = len(empty_tables)\n","total_failed  = len(failed_tables)\n","total_skipped = len(skipped_tables)\n","\n","total_rows = sum((r.get(\"rows_written\") or 0) for r in results)\n","\n","sum_task_time = sum(\n","    (r.get(\"duration_seconds\") or 0)\n","    for r in results\n",")\n","\n","theoretical_min = sum_task_time / MAX_WORKERS if MAX_WORKERS > 0 else sum_task_time\n","efficiency = (theoretical_min / total_duration) if total_duration > 0 else 0.0\n","\n","# --- Write single run-summary record to Delta ---\n","\n","# Als je tabel anders heet, vervang dan 'logs.bronze_run_summary'\n","RUN_SUMMARY_TABLE = \"logs.bronze_run_summary\"\n","\n","# Veiligheid: bereken skipped_count als je die niet al hebt\n","tables_total   = len(tables_sorted)\n","tables_success = total_success\n","tables_empty   = total_empty\n","tables_failed  = total_failed\n","tables_skipped = max(0, tables_total - tables_success - tables_empty - tables_failed)\n","\n","\n","summary_row = {\n","    \"run_id\":              RUN_ID,\n","    \"source\":              SOURCE_NAME,\n","    \"run_ts\":              RUN_TS,\n","    \"run_start\":           start_all,\n","    \"run_end\":             end_all,\n","    \"total_tables\":        int(tables_total),\n","    \"tables_success\":      int(tables_success),\n","    \"tables_empty\":        int(tables_empty),\n","    \"tables_failed\":       int(tables_failed),\n","    \"tables_skipped\":      int(tables_skipped),\n","    \"total_rows\":          int(total_rows),\n","    \"duration_seconds\":    int(total_duration),\n","    \"workers\":             int(MAX_WORKERS),\n","    \"sum_task_seconds\":    float(sum_task_time),\n","    \"theoretical_min_sec\": float(theoretical_min),\n","    \"actual_time_sec\":     float(total_duration),\n","    \"efficiency_pct\":      float(efficiency*100),\n","}\n","\n","summary_df = spark.createDataFrame([summary_row])\n","\n","# Append precies één record voor deze run\n","summary_df.write.format(\"delta\").mode(\"append\").saveAsTable(RUN_SUMMARY_TABLE)\n","\n","if DEBUG:\n","    print(f\"[MASTER] Run summary written to {RUN_SUMMARY_TABLE}\")\n","\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":-1,"statement_ids":null,"state":"cancelled","livy_statement_state":null,"session_id":null,"normalized_state":"cancelled","queued_time":"2025-11-04T08:45:32.7461978Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":"2025-11-04T08:45:58.1352217Z","parent_msg_id":"e6b6dae9-9021-4560-91e4-a636806517c2"},"text/plain":"StatementMeta(, , -1, Cancelled, , Cancelled)"},"metadata":{}}],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"f742aac1-2f23-40a5-8dd4-9b5792d39cdc"},{"cell_type":"code","source":["print(\"\\n============================================================\")\n","print(f\"SUMMARY: {SOURCE_NAME} @ {RUN_TS}\")\n","print(\"============================================================\")\n","print(f\"Total tables:    {total_tables}\")\n","print(f\"✓ Success:       {total_success}\")\n","print(f\"  - Empty:       {total_empty}\")\n","print(f\"✗ Failed:        {total_failed}\")\n","print(f\"⏭ Skipped:       {total_skipped}\")\n","print(f\"Total rows:      {total_rows:,}\")\n","print(f\"Duration:        {total_duration:.1f}s ({total_duration/60.0:.1f}m)\")\n","print(f\"Throughput:      { (total_rows / total_duration) if total_duration > 0 else 0.0:,.0f} rows/sec\")\n","print()\n","print(\"PARALLELIZATION:\")\n","print(f\"Workers:         {MAX_WORKERS}\")\n","print(f\"Sum of tasks:    {sum_task_time:.1f}s\")\n","print(f\"Theoretical min: {theoretical_min:.1f}s\")\n","print(f\"Actual time:     {total_duration:.1f}s\")\n","print(f\"Efficiency:      {efficiency*100:.1f}%\")\n","print(\"============================================================\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":-1,"statement_ids":null,"state":"cancelled","livy_statement_state":null,"session_id":null,"normalized_state":"cancelled","queued_time":"2025-11-04T08:45:32.7475504Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":"2025-11-04T08:45:58.1357437Z","parent_msg_id":"288dd583-6cf2-4fa2-95fc-67fddbd43fa9"},"text/plain":"StatementMeta(, , -1, Cancelled, , Cancelled)"},"metadata":{}}],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"08b46142-ee1a-44ae-9208-2fdb27ec857c"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"708925f5-6775-4eaa-abcb-d10995033b8c"}],"default_lakehouse":"708925f5-6775-4eaa-abcb-d10995033b8c","default_lakehouse_name":"lh_gh_bronze","default_lakehouse_workspace_id":"f29eeacf-64cd-431a-913b-2ed71174251e"}}},"nbformat":4,"nbformat_minor":5}