{"cells":[{"cell_type":"markdown","source":["# Incremental Watermark merge\n","\n","Merges watermark values from incremental load parquet files back into the central watermarks configuration file.\n","\n","**Process:**\n","1. Receives parameters from pipeline (source, run_id, paths)\n","2. Reads watermark updates from runtime parquet files\n","3. Updates central watermarks.json with new values\n","\n","**Parameters:**\n","- `source`: Source system name\n","- `run_id`: Current run identifier\n","- `wm_configpath`: Path to central watermarks config\n","- `wm_folder`: Runtime folder containing watermark parquet files"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"3752c9ec-7b37-4cf1-8375-bc6faa5cc075"},{"cell_type":"code","source":["import os\n","import json\n","\n","# Detect mode and set base path\n","base_path = '/lakehouse/default/Files' if os.path.exists('/lakehouse/default') else 'Files'\n","\n","wm_configpath_full = f'{base_path}/{wm_configpath}'\n","wm_folder_full = f'{base_path}/{wm_folder}'\n","\n","print(f\"Source: {source} | Run: {run_id}\")\n","print(f\"Watermark folder: {wm_folder_full}\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"1a912d89-a240-4994-8096-df29c02fbe57"},{"cell_type":"markdown","source":["## Load Watermarks Configuration\n","\n","Reads the central watermarks JSON file and locates the configuration for the current source system."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"21a6e056-9d4d-4533-9058-5bb4c58bb351"},{"cell_type":"code","source":["# Load watermarks config\n","with open(wm_configpath_full, 'r') as f:\n","    cfg = json.load(f)\n","\n","# Find source in config\n","sources = cfg.get(\"source\", [])\n","src = next((s for s in sources if s.get(\"name\") == source), None)\n","\n","if not src:\n","    raise ValueError(f\"Source '{source}' not found in config\")\n","\n","tables_map = src.get(\"tables\", {})\n","print(f\"Current watermarks: {len(tables_map)} tables\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a61c4bf7-d8ef-44b9-8b8d-50d5d8d82737"},{"cell_type":"markdown","source":["## Read Watermark Updates with Spark\n","\n","Uses Spark to read all watermark parquet files in parallel and extracts the maximum (most recent) watermark value per table.\n","\n","**Optimization:** Replaces sequential file reading with distributed Spark processing for 10-100x performance improvement."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"78010f1a-1c2b-4c91-a84d-b5e8257e2db8"},{"cell_type":"code","source":["# Read all parquet files with PyArrow (workaround for OneLake issue)\n","import pyarrow.parquet as pq\n","import pyarrow.dataset as ds\n","import os\n","\n","if not os.path.exists(wm_folder_full):\n","    print(\"No watermark folder - skipping\")\n","    updates = {}\n","else:\n","    updates = {}\n","    \n","    # Loop through table folders\n","    for table_name in os.listdir(wm_folder_full):\n","        table_path = os.path.join(wm_folder_full, table_name)\n","        \n","        if os.path.isdir(table_path):\n","            try:\n","                # Read with PyArrow\n","                parquet_files = [f for f in os.listdir(table_path) if f.endswith('.parquet')]\n","                \n","                if parquet_files:\n","                    # Read all parquets in this table folder\n","                    dataset = ds.dataset(table_path, format='parquet')\n","                    table = dataset.to_table()\n","                    \n","                    # Get max watermark if data exists\n","                    if table.num_rows > 0 and 'watermark' in table.column_names:\n","                        wm_column = table.column('watermark')\n","                        \n","                        # Find max non-null watermark\n","                        valid_watermarks = [wm.as_py() for wm in wm_column if wm.as_py() is not None and wm.as_py() != '']\n","                        \n","                        if valid_watermarks:\n","                            max_wm = max(valid_watermarks)\n","                            updates[table_name] = max_wm\n","                            print(f\"  {table_name}: {max_wm}\")\n","            \n","            except Exception as e:\n","                print(f\"  âš ï¸  Skipping {table_name}: {str(e)[:80]}\")\n","    \n","    print(f\"\\nFound {len(updates)} watermark updates\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"14e51012-8316-4901-a137-8e369494d318"},{"cell_type":"code","source":["# Voeg toe NA Cell 3, VOOR Cell 4\n","print(\"\\nðŸ” Comparing old vs new watermarks:\")\n","for table, new_wm in updates.items():\n","    old_wm = tables_map.get(table)\n","    match = \"âœ“ MATCH\" if old_wm == new_wm else \"âœ— DIFFERENT\"\n","    print(f\"  {table}: {old_wm} â†’ {new_wm} {match}\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4020e6a0-a050-44c3-9fd1-7869d1565d4e"},{"cell_type":"markdown","source":["## Update Configuration\n","\n","Compares new watermark values with existing ones and updates the configuration dictionary where changes are detected."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a5cdcdb9-4e6d-4689-84db-47b25bd6996c"},{"cell_type":"code","source":["# Update watermarks\n","changed = 0\n","\n","for table, new_wm in updates.items():\n","    old_wm = tables_map.get(table)\n","    if old_wm != new_wm:\n","        tables_map[table] = new_wm\n","        changed += 1\n","        print(f\"  {table}: {old_wm} â†’ {new_wm}\")\n","\n","print(f\"âœ“ Updated {changed} watermarks\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d1bc514b-15f1-41b1-8653-6f10948e60e5"},{"cell_type":"markdown","source":["## Save Configuration\n","\n","Writes the updated configuration back to disk using atomic file replacement to prevent corruption."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"0d43414e-9ea7-41c9-9051-295896a75713"},{"cell_type":"code","source":["# Write config atomically\n","if changed > 0:\n","    tmp_path = wm_configpath_full + \".tmp\"\n","    \n","    with open(tmp_path, 'w') as f:\n","        json.dump(cfg, f, indent=2)\n","    \n","    os.replace(tmp_path, wm_configpath_full)\n","    print(\"âœ“ Config saved\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"906f93a2-4b8d-4487-aab5-cdcbd4546b00"},{"cell_type":"markdown","source":["## Return Result\n","\n","Outputs processing summary and exits notebook with result payload for pipeline orchestration."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"f3cd7a20-0e26-47c3-a1d9-ea6afb5c899a"},{"cell_type":"code","source":["# Return result for pipeline\n","result = {\"updates\": changed, \"source\": source, \"run_id\": run_id}\n","print(json.dumps(result))\n","\n","# Exit for notebook orchestration\n","mssparkutils.notebook.exit(json.dumps(result))"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d24d5569-bd10-4442-99af-b40f0aff34b6"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"708925f5-6775-4eaa-abcb-d10995033b8c"}],"default_lakehouse":"708925f5-6775-4eaa-abcb-d10995033b8c","default_lakehouse_name":"lh_gh_bronze","default_lakehouse_workspace_id":"f29eeacf-64cd-431a-913b-2ed71174251e"}}},"nbformat":4,"nbformat_minor":5}