{"cells":[{"cell_type":"markdown","source":["# 02 – Configuration and DAG utilities\n","\n","This notebook contains helper functions to:\n","\n","- Read a DAG.json from OneLake using the `file:` protocol.\n","- Validate that the DAG belongs to the expected `SOURCE_NAME`.\n","- Extract the list of enabled tables for a source.\n","- Retrieve global settings from the DAG:\n","  - `base_files` – root folder where parquet files are written.\n","  - `bronze_schema` – schema name for bronze Delta tables.\n","\n","It also configures Spark's date/time behaviour to match the original notebooks\n","(by relaxing ANSI and using legacy time parsing).\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"2b5e4051-d508-47f6-86e1-66c67235015e"},{"cell_type":"code","source":["# [1] Imports and Spark configuration\n","\n","import json\n","from notebookutils import mssparkutils\n","from pyspark.sql import functions as F\n","\n","\n","# Relaxed date/time parsing to match legacy behaviour of the original notebooks\n","spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n","spark.conf.set(\"spark.sql.ansi.enabled\", \"false\")\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"e1781f56-e44e-444f-834f-ce3358c1278c"},{"cell_type":"markdown","source":["## [2] DAG helper functions\n","\n","This cell defines:\n","\n","- `read_dag(dag_path)` – read the DAG.json via OneLake `file:` path.\n","- `validate_dag_for_source(dag, source_name)` – ensure the DAG is for the expected source.\n","- `get_tables_for_source(dag, source_name)` – list of enabled tables.\n","- `get_base_files(dag)` – returns the configured base_files folder.\n","- `get_bronze_schema(dag)` – returns the bronze schema name (default: `bronze`).\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4eb1fc69-44f6-4de9-9be9-9bf5018c475b"},{"cell_type":"code","source":["# [2] DAG utilities\n","def read_dag(dag_path: str) -> dict:\n","    \"\"\"\n","    Read the DAG.json from OneLake using the file: protocol.\n","\n","    dag_path example: \"/lakehouse/default/Files/config/dag_anva_meeus_week.json\"\n","    \"\"\"\n","    if not dag_path:\n","        raise ValueError(\"dag_path is empty\")\n","\n","    p = dag_path.strip()\n","\n","    # Strip eventuele prefixes die Fabric hier niet nodig heeft\n","    if p.startswith(\"file:\"):\n","        p = p[len(\"file:\"):]\n","    if p.startswith(\"/lakehouse/default/\"):\n","        p = p[len(\"/lakehouse/default/\"):]\n","\n","    # Nu verwachten we zoiets als: 'Files/config/dag_anva_meeus_week.json'\n","    try:\n","        txt = mssparkutils.fs.head(p)\n","    except Exception as e:\n","        raise ValueError(\n","            f\"Failed to read DAG at '{dag_path}' (normalized '{p}'): {e}\"\n","        ) from e\n","\n","    return json.loads(txt)\n","\n","def validate_dag_for_source(dag: dict, source_name: str) -> None:\n","    \"\"\"\n","    Ensure that the DAG belongs to the requested source.\n","    \"\"\"\n","    dag_source = dag.get(\"source\")\n","    if dag_source != source_name:\n","        raise ValueError(f\"DAG is for source {dag_source}, but expected {source_name}\")\n","\n","\n","def get_tables_for_source(dag: dict, source_name: str):\n","    \"\"\"\n","    Return the list of enabled tables for a given source in the DAG.\n","\n","    Currently the DAG contains a single 'source', but we keep the function\n","    flexible in case this changes in the future.\n","    \"\"\"\n","    validate_dag_for_source(dag, source_name)\n","\n","    tables = dag.get(\"tables\", [])\n","    enabled = [t for t in tables if t.get(\"enabled\", True)]\n","\n","    if not enabled:\n","        raise ValueError(f\"No enabled tables found in DAG for source '{source_name}'\")\n","\n","    return enabled\n","\n","\n","def get_base_files(dag: dict) -> str:\n","    \"\"\"\n","    Return the base_files folder used to store parquet exports.\n","    Defaults to 'greenhouse_sources' if not specified.\n","    \"\"\"\n","    return dag.get(\"base_files\", \"greenhouse_sources\")\n","\n","\n","def normalize_files_path(path: str) -> str:\n","    \"\"\"\n","    Normalize any Lakehouse path to canonical 'Files/...' form.\n","    Accepts:\n","      - 'Files/...'\n","      - 'config/dag.json'\n","      - '/lakehouse/default/Files/...'\n","      - 'file:/lakehouse/default/Files/...'\n","    Returns always: 'Files/...'\n","    \"\"\"\n","    p = (path or \"\").strip()\n","\n","    # 1) strip file: prefix\n","    if p.startswith(\"file:\"):\n","        p = p[len(\"file:\"):]\n","\n","    # 2) strip leading slashes\n","    while p.startswith(\"/\"):\n","        p = p[1:]\n","\n","    # 3) strip 'lakehouse/default/' als prefix\n","    if p.lower().startswith(\"lakehouse/default/\"):\n","        p = p[len(\"lakehouse/default/\"):]\n","\n","    # 4) zorg dat het met 'Files/' begint\n","    if not p.startswith(\"Files/\"):\n","        p = \"Files/\" + p\n","\n","    return p\n","\n","def fs_ls(path: str):\n","    \"\"\"Wrapper around mssparkutils.fs.ls using normalized Lakehouse Files path.\"\"\"\n","    return mssparkutils.fs.ls(normalize_files_path(path))\n","\n","\n","def fs_head(path: str) -> str:\n","    \"\"\"Read small text file from Lakehouse (e.g. JSON config).\"\"\"\n","    return mssparkutils.fs.head(normalize_files_path(path))\n","\n","\n","def spark_read_parquet(path: str):\n","    \"\"\"Read parquet from Lakehouse Files as Spark DataFrame.\"\"\"\n","    return spark.read.parquet(normalize_files_path(path))\n","\n","\n","def spark_read_json(path: str):\n","    \"\"\"Read JSON from Lakehouse Files as Spark DataFrame.\"\"\"\n","    return spark.read.json(normalize_files_path(path))\n","\n","\n","def spark_read_csv(path: str, **options):\n","    \"\"\"Read CSV from Lakehouse Files as Spark DataFrame.\"\"\"\n","    return spark.read.options(**options).csv(normalize_files_path(path))\n","\n","def build_files_path(*segments: str) -> str:\n","    \"\"\"\n","    Join path segments and normalize to 'Files/...'.\n","    Example:\n","      build_files_path(\"config\", \"dag.json\") -> 'Files/config/dag.json'\n","      build_files_path(\"Files\", \"config\", \"dag.json\") -> 'Files/config/dag.json'\n","    \"\"\"\n","    clean = [s.strip(\"/\\\\\") for s in segments if s]\n","    rel = \"/\".join(clean)\n","    return normalize_files_path(rel)\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"436a2ce8-50ee-4b0e-8304-16b3f27cb3fe"},{"cell_type":"markdown","source":[],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"42d0e8b5-5f2c-4aa7-9c2a-c65e71d14c2d"},{"cell_type":"code","source":["# [X] Helper: choose worker profile based on last efficiency + last workers\n","\n","def choose_worker_profile(source_name: str, debug: bool = False) -> int:\n","    \"\"\"\n","    Decide how many workers to use for this source based on the last run:\n","\n","    - Uses logs.bronze_run_summary (SUMMARY_LOG_TABLE_FULLNAME)\n","    - Looks at the latest row for this source (by run_ts desc)\n","    - Reads:\n","        - last_workers (column 'workers')\n","        - last_efficiency_pct (column 'efficiency_pct', 0..100)\n","\n","    Rules (always step of 2 workers up/down):\n","\n","    Efficiency bands:\n","      0–20%   -> -2 workers (very bad)\n","      20–40%  -> -2 workers (bad)\n","      40–60%  ->  0 workers (keep)\n","      60–80%  -> +2 workers (good)\n","      80–100% -> +2 workers (very good)\n","\n","    Hard bounds:\n","      MIN_WORKERS = 2\n","      MAX_WORKERS = 12\n","\n","    If no historical row exists or something fails:\n","      -> default 12 workers (large)\n","    \"\"\"\n","\n","    MIN_WORKERS = 2\n","    MAX_WORKERS = 12\n","    DEFAULT_WORKERS = 12\n","\n","    try:\n","        # If summary table does not exist yet: start with default\n","        if not spark.catalog.tableExists(SUMMARY_LOG_TABLE_FULLNAME):\n","            if debug:\n","                print(f\"[MASTER] No summary table yet ({SUMMARY_LOG_TABLE_FULLNAME}), using default {DEFAULT_WORKERS} workers.\")\n","            return DEFAULT_WORKERS\n","\n","        # Last row for this source with non-null efficiency and workers\n","        last_df = (\n","            spark.table(SUMMARY_LOG_TABLE_FULLNAME)\n","                 .filter(F.col(\"source\") == source_name)\n","                 .where(F.col(\"efficiency_pct\").isNotNull() & F.col(\"workers\").isNotNull())\n","                 .orderBy(F.col(\"run_ts\").desc())\n","                 .limit(1)\n","        )\n","\n","        rows = last_df.collect()\n","        if not rows:\n","            if debug:\n","                print(f\"[MASTER] No historical efficiency/workers for source={source_name}, using default {DEFAULT_WORKERS} workers.\")\n","            return DEFAULT_WORKERS\n","\n","        row = rows[0]\n","        last_eff = float(row[\"efficiency_pct\"])\n","        last_workers = int(row[\"workers\"])\n","\n","        # Clamp to sensible bounds\n","        if last_eff < 0:\n","            last_eff = 0.0\n","        if last_eff > 100:\n","            last_eff = 100.0\n","\n","        # Determine delta based on efficiency band\n","        if last_eff < 20.0:\n","            delta = -2    # very bad\n","            band = \"0-20\"\n","        elif last_eff < 40.0:\n","            delta = -2    # bad\n","            band = \"20-40\"\n","        elif last_eff < 60.0:\n","            delta = 0     # neutral\n","            band = \"40-60\"\n","        elif last_eff < 80.0:\n","            delta = +2    # good\n","            band = \"60-80\"\n","        else:\n","            delta = +2    # very good (80-100)\n","            band = \"80-100\"\n","\n","        new_workers = last_workers + delta\n","\n","        # Respect global min/max\n","        if new_workers < MIN_WORKERS:\n","            new_workers = MIN_WORKERS\n","        if new_workers > MAX_WORKERS:\n","            new_workers = MAX_WORKERS\n","\n","        if debug:\n","            print(\n","                f\"[MASTER] Historical: workers_last={last_workers}, eff_last={last_eff:.1f}% (band {band}), \"\n","                f\"delta={delta} -> new_workers={new_workers}\"\n","            )\n","\n","        return new_workers\n","\n","    except Exception as e:\n","        if debug:\n","            print(f\"[MASTER] Error reading historical profile for {source_name}: {e}. Using default {DEFAULT_WORKERS} workers.\")\n","        return DEFAULT_WORKERS\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a0384848-56eb-421b-9669-3f45be64d795"},{"cell_type":"code","source":["def choose_worker_profile_from_history(\n","    source_name: str,\n","    default_workers: int = 8,\n","    min_workers: int = 2,\n","    max_workers_cap: int = 12,\n","    lookback_runs: int = 3,\n",") -> int:\n","    \"\"\"\n","    Pick a sensible #workers for the next run of `source_name`, based on the\n","    last N runs in logs.bronze_run_summary.\n","\n","    - Uses up to `lookback_runs` (1..5) most recent runs for that source.\n","    - For each run: computes throughput = total_rows / duration_seconds.\n","    - Aggregates average throughput per workers-count.\n","    - Chooses the workers-count with best throughput (within 5% of max,\n","      prefer the smallest workers).\n","    - Moves from last_workers to target in steps of ±2 workers max.\n","    - Applies simple volume-based caps (tiny runs don’t get 12 workers).\n","    \"\"\"\n","\n","    # Clamp lookback between 1 and 5\n","    lookback_runs = max(1, min(lookback_runs, 5))\n","\n","    # If no summary table yet → just use default\n","    if not spark.catalog.tableExists(SUMMARY_LOG_TABLE_FULLNAME):\n","        return int(default_workers)\n","\n","    base_df = spark.table(SUMMARY_LOG_TABLE_FULLNAME).filter(\n","        F.col(\"source\") == source_name\n","    )\n","\n","    # If there is no data for this source → default\n","    if base_df.head(1) == []:\n","        return int(default_workers)\n","\n","    # Order by run_start if aanwezig, anders run_ts\n","    cols = base_df.columns\n","    if \"run_start\" in cols:\n","        base_df = base_df.orderBy(F.col(\"run_start\").desc())\n","    else:\n","        base_df = base_df.orderBy(F.col(\"run_ts\").desc())\n","\n","    hist_df = base_df.limit(lookback_runs).select(\n","        \"workers\",\n","        \"efficiency_pct\",\n","        \"total_rows\",\n","        \"duration_seconds\",\n","    )\n","\n","    rows = hist_df.collect()\n","    if not rows:\n","        return int(default_workers)\n","\n","    # Laatste run is eerste rij in de sortering\n","    last = rows[0]\n","    last_workers = int(last[\"workers\"]) if last[\"workers\"] is not None else default_workers\n","\n","    # Bouw lijst van “geldige” runs (met bruikbare metrics)\n","    history = []\n","    for r in rows:\n","        w = r[\"workers\"]\n","        eff = r[\"efficiency_pct\"]\n","        tot = r[\"total_rows\"]\n","        dur = r[\"duration_seconds\"]\n","\n","        if w is None or dur is None or dur <= 0 or tot is None or tot <= 0:\n","            continue\n","\n","        # Skip extreem slechte eff (<20%) als input voor throughput-analyse\n","        if eff is not None and eff < 20.0:\n","            continue\n","\n","        throughput = float(tot) / float(dur)\n","        history.append(\n","            {\n","                \"workers\": int(w),\n","                \"eff\": float(eff) if eff is not None else None,\n","                \"rows\": int(tot),\n","                \"duration\": float(dur),\n","                \"throughput\": throughput,\n","            }\n","        )\n","\n","    # Als we niets bruikbaars overhouden → houd gewoon last_workers of default\n","    if not history:\n","        return int(last_workers or default_workers)\n","\n","    # 1) Volume-profiel bepalen (mediane rows over deze runs)\n","    sorted_by_rows = sorted(history, key=lambda x: x[\"rows\"])\n","    mid = len(sorted_by_rows) // 2\n","    if len(sorted_by_rows) % 2 == 1:\n","        median_rows = sorted_by_rows[mid][\"rows\"]\n","    else:\n","        median_rows = int(\n","            (sorted_by_rows[mid - 1][\"rows\"] + sorted_by_rows[mid][\"rows\"]) / 2\n","        )\n","\n","    # 2) Gemiddelde throughput per workers berekenen\n","    from collections import defaultdict\n","\n","    thr_by_workers = defaultdict(list)\n","    for h in history:\n","        thr_by_workers[h[\"workers\"]].append(h[\"throughput\"])\n","\n","    avg_thr_by_workers = {\n","        w: sum(vals) / len(vals) for w, vals in thr_by_workers.items()\n","    }\n","\n","    # 3) Kies target_workers op basis van throughput\n","    best_thr = max(avg_thr_by_workers.values())\n","    # Workers die binnen 5% van best_thr zitten\n","    candidate_workers = [\n","        w for w, thr in avg_thr_by_workers.items() if thr >= 0.95 * best_thr\n","    ]\n","    # Kies de kleinste daarvan (resources sparen)\n","    target_workers = min(candidate_workers)\n","\n","    # 4) Volume-based caps:\n","    #    - heel kleine runs hebben weinig aan veel workers\n","    #    - pas deze grenzen gerust aan op basis van jouw landschap\n","    if median_rows < 100_000:\n","        target_workers = min(target_workers, 4)\n","    elif median_rows < 1_000_000:\n","        target_workers = min(target_workers, 8)\n","    # bij grotere volumes laten we max_workers_cap de bovengrens zijn\n","\n","    # 5) Clamp target binnen globale min/max\n","    target_workers = max(min_workers, min(max_workers_cap, target_workers))\n","\n","    # 6) Beweeg maximaal ±2 workers t.o.v. laatste run (stapjes)\n","    if target_workers > last_workers:\n","        new_workers = min(last_workers + 2, target_workers, max_workers_cap)\n","    elif target_workers < last_workers:\n","        new_workers = max(last_workers - 2, target_workers, min_workers)\n","    else:\n","        new_workers = last_workers\n","\n","    return int(new_workers)\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"17a09e06-369e-4ba3-a092-862a9b4e00e5"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"708925f5-6775-4eaa-abcb-d10995033b8c"}],"default_lakehouse":"708925f5-6775-4eaa-abcb-d10995033b8c","default_lakehouse_name":"lh_gh_bronze","default_lakehouse_workspace_id":"f29eeacf-64cd-431a-913b-2ed71174251e"}}},"nbformat":4,"nbformat_minor":5}