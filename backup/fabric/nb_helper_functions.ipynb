{"cells":[{"cell_type":"code","source":["from typing import Iterable, Optional, Set, List\n","from pathlib import Path\n","import hashlib\n","import pyarrow as pa\n","import pyarrow.parquet as pq\n","import pyarrow.compute as pc\n","from deltalake import DeltaTable, write_deltalake"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ccf01c36-1eed-41ad-9779-52d52bff03c2"},{"cell_type":"code","source":["# ---------- helpers ----------\n","def _ensure_array(col) -> pa.Array:\n","    \"\"\"Always return a pa.Array\"\"\"\n","    if isinstance(col, pa.ChunkedArray):\n","        return col.chunk(0) if col.num_chunks == 1 else col.combine_chunks()\n","    return col\n","\n","def _resolve_cols(\n","    all_cols: Set[str],\n","    include_cols: Optional[Iterable[str]],\n","    exclude_cols: Optional[Iterable[str]],\n",") -> List[str]:\n","    if include_cols is None:\n","        chosen = set(all_cols)\n","    else:\n","        include_cols = set(include_cols)\n","        missing = include_cols - all_cols\n","        if missing:\n","            raise ValueError(f\"Column(S) not found: {sorted(missing)}\")\n","        chosen = set(include_cols)\n","\n","    if exclude_cols:\n","        exclude_cols = set(exclude_cols)\n","        missing_ex = exclude_cols - all_cols\n","        if missing_ex:\n","            raise ValueError(f\"Excluded column(s) not found: {sorted(missing_ex)}\")\n","        chosen -= exclude_cols\n","\n","    cols = sorted(chosen)\n","    if not cols:\n","        raise ValueError(\"No columns available to be hashed (check include/exclude).\")\n","    return cols\n","\n","def _prepare_arrays_for_hash(tbl: pa.Table, cols: Iterable[str], null_token: str) -> List[pa.Array]:\n","    \"\"\"Normaliseer elke gekozen kolom naar Arrow string, met vaste NULL-afhandeling.\"\"\"\n","    arrays: List[pa.Array] = []\n","    for c in cols:\n","        arr = _ensure_array(tbl[c])\n","        # dictionary/binary veilig naar string\n","        if pa.types.is_dictionary(arr.type):\n","            arr = pc.cast(arr, pa.large_string())\n","        else:\n","            target = pa.large_string() if pa.types.is_binary(arr.type) else pa.string()\n","            arr = pc.cast(arr, target)\n","        arr = pc.fill_null(arr, null_token)\n","        arrays.append(arr)\n","    return arrays\n","\n","def _hash_joined(joined: pa.Array, algo: str) -> pa.Array:\n","    \"\"\"\n","    Gebruikt Python's hashlib.\n","    Retourneert pa.StringArray met hex-digests.\n","    \"\"\"\n","    name = algo.lower()\n","    # Python hashlib (batch-gewijs om geheugen te beperken)\n","    n = len(joined)\n","    batch = 200_000  # pas aan naar wens\n","    out_hex: List[str] = []\n","    if name not in (\"sha256\", \"md5\"):\n","        raise ValueError(\"Unsupported algo. Gebruik 'sha256' of 'md5'.\")\n","\n","    for start in range(0, n, batch):\n","        chunk = joined.slice(start, min(batch, n - start)).to_pylist()  # list[str]\n","        if name == \"sha256\":\n","            out_hex.extend(hashlib.sha256(s.encode(\"utf-8\")).hexdigest() for s in chunk)\n","        else:\n","            out_hex.extend(hashlib.md5(s.encode(\"utf-8\")).hexdigest() for s in chunk)\n","    return pa.array(out_hex, type=pa.string())"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b69ce004-c85c-4bd4-bd35-0cac6094049a"},{"cell_type":"code","source":["def add_row_hash_to_parquet(\n","    src_path: str,\n","    dest_path: str,\n","    include_cols: Optional[Iterable[str]] = None,  # None => hele rij\n","    exclude_cols: Optional[Iterable[str]] = None,\n","    new_col: str = \"row_hash\",\n","    algo: str = \"sha256\",      # 'sha256' of 'md5'\n","    null_token: str = \"‚àÖ\",\n","    sep: str = \"\\x1f\",\n","    parquet_write_kwargs: Optional[dict] = None,\n",") -> None:\n","    tbl: pa.Table = pq.read_table(src_path).combine_chunks()\n","    if new_col in tbl.column_names:\n","        raise ValueError(f\"Kolom '{new_col}' bestaat al in het Parquetbestand.\")\n","\n","    cols = _resolve_cols(set(tbl.column_names), include_cols, exclude_cols)\n","    arrays = _prepare_arrays_for_hash(tbl, cols, null_token)\n","\n","    # Join per rij ‚Äî belangrijk: arrays als *args en separator positioneel (PyArrow 17)\n","    joined = pc.binary_join_element_wise(*arrays, sep)\n","\n","    # Hash (Arrow-kernel of fallback)\n","    digest_hex = _hash_joined(joined, algo)\n","\n","    tbl_hashed = tbl.append_column(new_col, digest_hex)\n","    pq.write_table(tbl_hashed, dest_path, **(parquet_write_kwargs or {}))"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"179380af-2d89-44f4-a80b-3f304792a098"},{"cell_type":"code","source":["def add_row_hash_to_delta_table(\n","    table_path: str,\n","    include_cols: Optional[Iterable[str]] = None,  # None => hele rij\n","    exclude_cols: Optional[Iterable[str]] = None,\n","    new_col: str = \"row_hash\",\n","    algo: str = \"sha256\",\n","    null_token: str = \"‚àÖ\",\n","    sep: str = \"\\x1f\",\n","    mode: str = \"overwrite\",\n","    write_kwargs: Optional[dict] = None,\n",") -> int:\n","    dt = DeltaTable(table_path)\n","    tbl: pa.Table = dt.to_pyarrow_table().combine_chunks()\n","    if new_col in tbl.column_names:\n","        raise ValueError(f\"Kolom '{new_col}' bestaat al in de Delta-tabel.\")\n","\n","    cols = _resolve_cols(set(tbl.column_names), include_cols, exclude_cols)\n","    arrays = _prepare_arrays_for_hash(tbl, cols, null_token)\n","\n","    joined = pc.binary_join_element_wise(*arrays, sep)\n","    digest_hex = _hash_joined(joined, algo)\n","\n","    tbl_hashed = tbl.append_column(new_col, digest_hex)\n","    write_deltalake(table_path, tbl_hashed, mode=mode, **(write_kwargs or {}))\n","    return DeltaTable(table_path).version()"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"af7d1cdc-f9bc-4f0a-a679-f8d5d73146f5"},{"cell_type":"code","source":["# # Gebruik\n","# src = \"/lakehouse/default/Files/test/ods_reports/2025/10/03/run_20251003T060110071/AOV_PRODUCT_OVERZICHT/AOV_PRODUCT_OVERZICHT_00000.parquet\"\n","# dest = \"/lakehouse/default/Files/test/ods_reports/2025/10/03/run_20251003T060110071/AOV_PRODUCT_OVERZICHT/AOV_PRODUCT_OVERZICHT_00000_hashed.parquet\"\n","\n","# add_row_hash_to_parquet(src, dest, include_cols=None, exclude_cols=None)\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"0a4cc82b-0f73-42d7-8a42-e80b79901829"},{"cell_type":"code","source":["import os\n","from concurrent.futures import ThreadPoolExecutor, as_completed\n","from collections import defaultdict\n","\n","def calculate_size_by_level(base_path, aggregation_level, level_filters=None, max_workers=20):\n","    \"\"\"\n","    Bereken storage per folder op een specifiek niveau met optionele filters.\n","    Geoptimaliseerd: filtert tijdens os.walk() voor maximale snelheid.\n","    \"\"\"\n","    \n","    if level_filters is None:\n","        level_filters = {}\n","    \n","    size_by_path = defaultdict(lambda: {'size': 0, 'files': 0})\n","    \n","    def get_aggregation_key(full_path, base, level):\n","        \"\"\"Bepaal de aggregatie key op basis van het niveau\"\"\"\n","        relative = os.path.relpath(full_path, base)\n","        parts = relative.split(os.sep)\n","        return os.sep.join(parts[:level]) if len(parts) >= level else relative\n","    \n","    def process_folder(folder_path):\n","        \"\"\"Verwerk √©√©n folder\"\"\"\n","        local_sizes = defaultdict(lambda: {'size': 0, 'files': 0})\n","        \n","        try:\n","            for item in os.listdir(folder_path):\n","                item_path = os.path.join(folder_path, item)\n","                if os.path.isfile(item_path):\n","                    try:\n","                        size = os.path.getsize(item_path)\n","                        key = get_aggregation_key(folder_path, base_path, aggregation_level)\n","                        local_sizes[key]['size'] += size\n","                        local_sizes[key]['files'] += 1\n","                    except:\n","                        pass\n","        except:\n","            pass\n","        \n","        return local_sizes\n","    \n","    # Slim verzamelen van folders met filtering tijdens walk\n","    all_folders = []\n","    \n","    for root, dirs, files in os.walk(base_path):\n","        # Bepaal huidige niveau\n","        relative = os.path.relpath(root, base_path)\n","        if relative == '.':\n","            current_parts = []\n","        else:\n","            current_parts = relative.split(os.sep)\n","        \n","        current_level = len(current_parts)\n","        \n","        # Filter subdirectories op basis van filters\n","        if current_level + 1 in level_filters:\n","            filter_value = level_filters[current_level + 1]\n","            # Behoud alleen directories die matchen met de filter\n","            dirs[:] = [d for d in dirs if d == filter_value]\n","        \n","        # Check of huidige folder aan alle filters voldoet\n","        matches = True\n","        for level, filter_value in level_filters.items():\n","            if level <= len(current_parts):\n","                if current_parts[level - 1] != filter_value:\n","                    matches = False\n","                    break\n","        \n","        if matches:\n","            all_folders.append(root)\n","    \n","    print(f\"üîç Relevante folders na filtering: {len(all_folders):,}\")\n","    print(f\"üìä Aggregatie niveau: {aggregation_level}\")\n","    if level_filters:\n","        print(f\"üîé Filters actief:\")\n","        level_names = {1: \"source\", 2: \"year\", 3: \"month\", 4: \"day\", 5: \"run_ts\", 6: \"table\"}\n","        for level, value in sorted(level_filters.items()):\n","            print(f\"   - Niveau {level} ({level_names.get(level, 'unknown')}): {value}\")\n","    print(\"‚öôÔ∏è  Berekenen...\\n\")\n","    \n","    # Parallel processing\n","    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n","        futures = [executor.submit(process_folder, folder) for folder in all_folders]\n","        \n","        for future in as_completed(futures):\n","            local_result = future.result()\n","            for key, data in local_result.items():\n","                size_by_path[key]['size'] += data['size']\n","                size_by_path[key]['files'] += data['files']\n","    \n","    return dict(size_by_path)\n","\n","# ============================================================================\n","# CONFIGURATIE\n","# ============================================================================\n","\n","base_path = \"/lakehouse/default/Files/greenhouse_sources\"\n","\n","# Stel aggregatie niveau in\n","aggregation_level = 3  # 1=source, 2=year, 3=month, 4=day, 5=run_ts, 6=table\n","\n","# Stel filters in (None voor geen filter, of dict met level: waarde)\n","# Voorbeelden:\n","# level_filters = None                              # Geen filters\n","# level_filters = {1: \"anva_concern\"}              # Alleen anva_concern\n","# level_filters = {1: \"anva_concern\", 2: \"2025\"}   # anva_concern in 2025\n","# level_filters = {2: \"2025\", 3: \"10\"}             # Alle sources in 2025/10\n","#level_filters = None\n","level_filters = {2: \"2025\", 3: \"10\"}\n","\n","# Bereken\n","results = calculate_size_by_level(base_path, aggregation_level, level_filters)\n","\n","# Sorteer en toon resultaten\n","sorted_results = sorted(results.items(), key=lambda x: x[1]['size'], reverse=True)\n","\n","print(f\"\\n{'='*93}\")\n","print(f\"üìà Resultaten (top 20 - gesorteerd op grootte)\")\n","print(f\"{'='*93}\")\n","print(f\"{'Path':<50} {'Files':>10} {'Size (MB)':>15} {'Size (GB)':>15}\")\n","print(f\"{'-'*93}\")\n","\n","for path, data in sorted_results[:20]:\n","    size_mb = data['size'] / (1024**2)\n","    size_gb = data['size'] / (1024**3)\n","    print(f\"{path:<50} {data['files']:>10,} {size_mb:>15.2f} {size_gb:>15.2f}\")\n","\n","# Totalen\n","total_size = sum(d['size'] for d in results.values())\n","total_files = sum(d['files'] for d in results.values())\n","print(f\"{'-'*93}\")\n","print(f\"{'TOTAAL':<50} {total_files:>10,} {total_size/(1024**2):>15.2f} {total_size/(1024**3):>15.2f}\")\n","print(f\"\\n‚úì Aantal unieke paden op niveau {aggregation_level}: {len(results):,}\")\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":3,"statement_ids":[3],"state":"finished","livy_statement_state":"available","session_id":"20c1d4c1-3390-4047-a06c-d08d6d296c91","normalized_state":"finished","queued_time":"2025-10-30T14:00:00.7657034Z","session_start_time":null,"execution_start_time":"2025-10-30T14:00:08.7732678Z","execution_finish_time":"2025-10-30T14:00:14.5057729Z","parent_msg_id":"0ea858d1-2071-411f-a42f-336633f421ea"},"text/plain":"StatementMeta(, 20c1d4c1-3390-4047-a06c-d08d6d296c91, 3, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["üîç Relevante folders na filtering: 96\nüìä Aggregatie niveau: 3\nüîé Filters actief:\n   - Niveau 2 (year): 2025\n   - Niveau 3 (month): 10\n‚öôÔ∏è  Berekenen...\n\n\n=============================================================================================\nüìà Resultaten (top 20 - gesorteerd op grootte)\n=============================================================================================\nPath                                                    Files       Size (MB)       Size (GB)\n---------------------------------------------------------------------------------------------\nvizier/2025/10                                             53         2328.62            2.27\nanva_meeus/2025/10                                         70          131.60            0.13\n---------------------------------------------------------------------------------------------\nTOTAAL                                                    123         2460.21            2.40\n\n‚úì Aantal unieke paden op niveau 3: 2\n"]}],"execution_count":1,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c0de6b40-b060-4849-be74-a6fbfb04b921"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"708925f5-6775-4eaa-abcb-d10995033b8c"}],"default_lakehouse":"708925f5-6775-4eaa-abcb-d10995033b8c","default_lakehouse_name":"lh_gh_bronze","default_lakehouse_workspace_id":"f29eeacf-64cd-431a-913b-2ed71174251e"}}},"nbformat":4,"nbformat_minor":5}