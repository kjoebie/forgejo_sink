{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters (deze worden overschreven door Papermill)\n",
    "source = \"default_source\"\n",
    "run_ts = \"2024-01-01T00:00:00\"\n",
    "process_type = \"bronze\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Voeg project root toe aan path\n",
    "project_root = Path.cwd().parent if Path.cwd().name == 'notebooks' else Path.cwd()\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Python path: {sys.path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialiseer Spark session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Gebruik getOrCreate om bestaande sessie te hergebruiken\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(f\"Process_{source}_{process_type}\") \\\n",
    "    .master(\"local[2]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"âœ… Spark session created: {spark.sparkContext.appName}\")\n",
    "print(f\"ðŸ“Š Spark version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toon parameters\n",
    "print(\"=\" * 70)\n",
    "print(\"NOTEBOOK PARAMETERS\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Source: {source}\")\n",
    "print(f\"Run timestamp: {run_ts}\")\n",
    "print(f\"Process type: {process_type}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark data processing\n",
    "from pyspark.sql import functions as F\n",
    "from datetime import datetime\n",
    "\n",
    "print(f\"\\nðŸ”„ Processing {source} data with Spark...\")\n",
    "print(f\"ðŸ“… Timestamp: {run_ts}\")\n",
    "print(f\"âš™ï¸  Type: {process_type}\")\n",
    "\n",
    "# Maak sample data\n",
    "data = [\n",
    "    (1, \"Product A\", 100, run_ts),\n",
    "    (2, \"Product B\", 200, run_ts),\n",
    "    (3, \"Product C\", 150, run_ts),\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"id\", \"product\", \"amount\", \"timestamp\"])\n",
    "\n",
    "# Voeg derived column toe\n",
    "df = df.withColumn(\"category\", \n",
    "    F.when(F.col(\"amount\") > 150, \"high\")\n",
    "     .when(F.col(\"amount\") > 100, \"medium\")\n",
    "     .otherwise(\"low\")\n",
    ")\n",
    "\n",
    "print(f\"\\nðŸ“Š Data processed:\")\n",
    "df.show()\n",
    "\n",
    "record_count = df.count()\n",
    "print(f\"âœ… Processing complete! Records: {record_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "spark.stop()\n",
    "print(\"ðŸ›‘ Spark session stopped\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
